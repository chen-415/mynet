================ Training Loss (Mon Nov 25 16:35:21 2024) ================
import os
import datetime
import argparse
import numpy as np

import torch
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import DataLoader

from ImagePairPrefixFolder import ImagePairPrefixFolder, var_custom_collate
from utils import MovingAvg
from tf_visualizer import TFVisualizer

parser = argparse.ArgumentParser()
parser.add_argument('--network', default='GCANet')
parser.add_argument('--name', default='default_exp')
parser.add_argument('--gpu_ids', default='5')
parser.add_argument('--epochs', type=int, default=100)
parser.add_argument('--lr', type=float, default=0.001)
parser.add_argument('--lr_step', type=int, default=40)
parser.add_argument('--lr_gamma', type=float, default=0.1)
parser.add_argument('--weight_decay', type=float, default=0.0005)
parser.add_argument('--checkpoints_dir', default='checkpoint')
parser.add_argument('--logDir', default='tblogdir')
parser.add_argument('--resume_dir', default='')
parser.add_argument('--resume_epoch', type=int, default=0)
parser.add_argument('--save_epoch', type=int, default=5)
parser.add_argument('--save_latest_freq', type=int, default=5000)
parser.add_argument('--test_epoch', type=int, default=5)
parser.add_argument('--test_max_size', type=int, default=1080)
parser.add_argument('--size_unit', type=int,  default=8)
parser.add_argument('--print_iter', type=int,  default=100)
parser.add_argument('--input_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy/')
parser.add_argument('--gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/test/clear/')
parser.add_argument('--test_input_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy/')
parser.add_argument('--test_gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear/')
parser.add_argument('--num_workers', type=int, default=16)
parser.add_argument('--batch_size', type=int, default=2)
parser.add_argument('--only_residual', action='store_true', help='regress residual rather than image')
parser.add_argument('--loss_func', default='l2', help='l2|l1')
parser.add_argument('--inc', type=int, default=3)
parser.add_argument('--outc', type=int, default=3)
parser.add_argument('--force_rgb', action='store_true')
parser.add_argument('--no_edge', action='store_true')

opt = parser.parse_args()

opt.input_folder = os.path.expanduser(opt.input_folder)
opt.gt_folder = os.path.expanduser(opt.gt_folder)
opt.test_input_folder = os.path.expanduser(opt.test_input_folder)
opt.test_gt_folder = os.path.expanduser(opt.test_gt_folder)


if not os.path.exists(os.path.join(opt.checkpoints_dir, opt.name)):
    os.makedirs(os.path.join(opt.checkpoints_dir, opt.name))
opt.resume_dir = opt.resume_dir if opt.resume_dir != '' else os.path.join(opt.checkpoints_dir, opt.name)

visualizer = TFVisualizer(opt)
### Log out
with open(os.path.realpath(__file__), 'r') as fid:
    visualizer.print_logs(fid.read())

## print argument
for key, val in vars(opt).items():
    visualizer.print_logs('%s: %s' % (key, val))

opt.gpu_ids = [int(x) for x in opt.gpu_ids.split(',')]
assert all(0 <= x <= torch.cuda.device_count() for x in opt.gpu_ids), 'gpu id should ' \
                                                      'be 0~{0}'.format(torch.cuda.device_count())
torch.cuda.set_device(opt.gpu_ids[0])


train_dataset = ImagePairPrefixFolder(opt.input_folder, opt.gt_folder, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
train_dataloader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True,
                              collate_fn=var_custom_collate, pin_memory=True,
                              num_workers=opt.num_workers)

opt.do_test = opt.test_gt_folder != ''
if opt.do_test:
    test_dataset = ImagePairPrefixFolder(opt.test_input_folder, opt.test_gt_folder,
                                         max_img_size=opt.test_max_size, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,
                                 collate_fn=var_custom_collate, pin_memory=True,
                                 num_workers=1)

total_inc = opt.inc if opt.no_edge else opt.inc + 1
if opt.network == 'GCANet':
    from GCANet import GCANet
    net = GCANet(in_c=total_inc, out_c=3, only_residual=opt.only_residual)
else:
    print('network structure %s not supported' % opt.network)
    raise ValueError

if opt.loss_func == 'l2':
    loss_crit = torch.nn.MSELoss()
elif opt.loss_func == 'l1':
    loss_crit = torch.nn.SmoothL1Loss()
else:
    print('loss_func %s not supported' % opt.loss_func)
    raise ValueError
pnsr_crit = torch.nn.MSELoss()

if len(opt.gpu_ids) > 0:
    net.cuda()
    if len(opt.gpu_ids) > 1:
        net = torch.nn.DataParallel(net)
    loss_crit = loss_crit.cuda()
    pnsr_crit = pnsr_crit.cuda()

optimizer = optim.Adam(net.parameters(), lr=opt.lr)
step_optim_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.lr_step, gamma=opt.lr_gamma)
loss_avg = MovingAvg(pool_size=50)

start_epoch = 0
total_iter = 0

if os.path.exists(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth')):
    print('resuming from latest.pth')
    latest_info = torch.load(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))
    start_epoch = latest_info['epoch']
    total_iter = latest_info['total_iter']
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(latest_info['net_state'])
    else:
        net.load_state_dict(latest_info['net_state'])
    optimizer.load_state_dict(latest_info['optim_state'])

if opt.resume_epoch > 0:
    start_epoch = opt.resume_epoch
    total_iter = opt.resume_epoch * len(train_dataloader)
    resume_path = os.path.join(opt.resume_epoch, 'net_epoch_%d.pth') % opt.resume_epoch
    print('resume from : %s' % resume_path)
    assert os.path.exists(resume_path), 'cannot find the resume model: %s ' % resume_path
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(torch.load(resume_path))
    else:
        net.load_state_dict(torch.load(resume_path))

for epoch in range(start_epoch, opt.epochs):
    visualizer.print_logs("Start to train epoch %d" % epoch)
    net.train()
    for iter, data in enumerate(train_dataloader):
        total_iter += 1
        optimizer.zero_grad()
        step_optim_scheduler.step(epoch)

        batch_input_img, batch_input_edge,  batch_gt = data
        if len(opt.gpu_ids) > 0:
            batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

        if opt.no_edge:
            batch_input = batch_input_img
        else:
            batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
        batch_input_v = Variable(batch_input)
        if opt.only_residual:
            batch_gt_v = Variable(batch_gt - (batch_input_img+128))
        else:
            batch_gt_v = Variable(batch_gt)

        pred = net(batch_input_v)

        loss = loss_crit(pred, batch_gt_v)
        avg_loss = loss_avg.set_curr_val(loss.data)

        loss.backward()
        optimizer.step()

        if iter % opt.print_iter == 0:
            visualizer.plot_current_losses(total_iter, { 'loss': loss})
            visualizer.print_logs('%s Step[%d/%d], lr: %f, mv_avg_loss: %f, loss: %f' %
                                    (str(datetime.datetime.now()).split(' ')[1], iter, len(train_dataloader),
                                     step_optim_scheduler.get_lr()[0], avg_loss, loss))

        if total_iter % opt.save_latest_freq == 0:
            latest_info = {'total_iter': total_iter,
                           'epoch': epoch,
                           'optim_state': optimizer.state_dict()}
            if len(opt.gpu_ids) > 1:
                latest_info['net_state'] = net.module.state_dict()
            else:
                latest_info['net_state'] = net.state_dict()
            print('save lastest model.')
            torch.save(latest_info, os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))

    if (epoch+1) % opt.save_epoch == 0 :
        visualizer.print_logs('saving model for epoch %d' % epoch)
        if len(opt.gpu_ids) > 1:
            torch.save(net.module.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch+1)))
        else:
            torch.save(net.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch + 1)))

    if opt.do_test:        
        avg_psnr = 0
        task_cnt = 0
        net.eval()
        with torch.no_grad():
            for iter, data in enumerate(test_dataloader):
                batch_input_img, batch_input_edge,  batch_gt = data
                if len(opt.gpu_ids) > 0:
                    batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

                if opt.no_edge:
                    batch_input = batch_input_img
                else:
                    batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
                batch_input_v = Variable(batch_input)
                batch_gt_v = Variable(batch_gt)


                pred = net(batch_input_v)

                if opt.only_residual:
                    loss = pnsr_crit(pred+Variable(batch_input_img+128), batch_gt_v)
                else:
                    loss = pnsr_crit(pred, batch_gt_v)
                avg_psnr += 10 * np.log10(255 * 255 / loss.item())
                task_cnt += 1

        visualizer.print_logs('Testing for epoch: %d' % epoch)
        visualizer.print_logs('Average test PNSR is %f for %d images' % (avg_psnr/task_cnt, task_cnt))







network: GCANet
name: default_exp
gpu_ids: 5
epochs: 100
lr: 0.001
lr_step: 40
lr_gamma: 0.1
weight_decay: 0.0005
checkpoints_dir: checkpoint
logDir: tblogdir
resume_dir: checkpoint/default_exp
resume_epoch: 0
save_epoch: 5
save_latest_freq: 5000
test_epoch: 5
test_max_size: 1080
size_unit: 8
print_iter: 100
input_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy/
gt_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/test/clear/
test_input_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy/
test_gt_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear/
num_workers: 16
batch_size: 2
only_residual: False
loss_func: l2
inc: 3
outc: 3
force_rgb: False
no_edge: False
================ Training Loss (Mon Nov 25 16:35:41 2024) ================
import os
import datetime
import argparse
import numpy as np

import torch
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import DataLoader

from ImagePairPrefixFolder import ImagePairPrefixFolder, var_custom_collate
from utils import MovingAvg
from tf_visualizer import TFVisualizer

parser = argparse.ArgumentParser()
parser.add_argument('--network', default='GCANet')
parser.add_argument('--name', default='default_exp')
parser.add_argument('--gpu_ids', default='5')
parser.add_argument('--epochs', type=int, default=100)
parser.add_argument('--lr', type=float, default=0.001)
parser.add_argument('--lr_step', type=int, default=40)
parser.add_argument('--lr_gamma', type=float, default=0.1)
parser.add_argument('--weight_decay', type=float, default=0.0005)
parser.add_argument('--checkpoints_dir', default='checkpoint')
parser.add_argument('--logDir', default='tblogdir')
parser.add_argument('--resume_dir', default='')
parser.add_argument('--resume_epoch', type=int, default=0)
parser.add_argument('--save_epoch', type=int, default=5)
parser.add_argument('--save_latest_freq', type=int, default=5000)
parser.add_argument('--test_epoch', type=int, default=5)
parser.add_argument('--test_max_size', type=int, default=1080)
parser.add_argument('--size_unit', type=int,  default=8)
parser.add_argument('--print_iter', type=int,  default=100)
parser.add_argument('--input_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy')
parser.add_argument('--gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/test/clear')
parser.add_argument('--test_input_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy')
parser.add_argument('--test_gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear')
parser.add_argument('--num_workers', type=int, default=16)
parser.add_argument('--batch_size', type=int, default=2)
parser.add_argument('--only_residual', action='store_true', help='regress residual rather than image')
parser.add_argument('--loss_func', default='l2', help='l2|l1')
parser.add_argument('--inc', type=int, default=3)
parser.add_argument('--outc', type=int, default=3)
parser.add_argument('--force_rgb', action='store_true')
parser.add_argument('--no_edge', action='store_true')

opt = parser.parse_args()

opt.input_folder = os.path.expanduser(opt.input_folder)
opt.gt_folder = os.path.expanduser(opt.gt_folder)
opt.test_input_folder = os.path.expanduser(opt.test_input_folder)
opt.test_gt_folder = os.path.expanduser(opt.test_gt_folder)


if not os.path.exists(os.path.join(opt.checkpoints_dir, opt.name)):
    os.makedirs(os.path.join(opt.checkpoints_dir, opt.name))
opt.resume_dir = opt.resume_dir if opt.resume_dir != '' else os.path.join(opt.checkpoints_dir, opt.name)

visualizer = TFVisualizer(opt)
### Log out
with open(os.path.realpath(__file__), 'r') as fid:
    visualizer.print_logs(fid.read())

## print argument
for key, val in vars(opt).items():
    visualizer.print_logs('%s: %s' % (key, val))

opt.gpu_ids = [int(x) for x in opt.gpu_ids.split(',')]
assert all(0 <= x <= torch.cuda.device_count() for x in opt.gpu_ids), 'gpu id should ' \
                                                      'be 0~{0}'.format(torch.cuda.device_count())
torch.cuda.set_device(opt.gpu_ids[0])


train_dataset = ImagePairPrefixFolder(opt.input_folder, opt.gt_folder, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
train_dataloader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True,
                              collate_fn=var_custom_collate, pin_memory=True,
                              num_workers=opt.num_workers)

opt.do_test = opt.test_gt_folder != ''
if opt.do_test:
    test_dataset = ImagePairPrefixFolder(opt.test_input_folder, opt.test_gt_folder,
                                         max_img_size=opt.test_max_size, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,
                                 collate_fn=var_custom_collate, pin_memory=True,
                                 num_workers=1)

total_inc = opt.inc if opt.no_edge else opt.inc + 1
if opt.network == 'GCANet':
    from GCANet import GCANet
    net = GCANet(in_c=total_inc, out_c=3, only_residual=opt.only_residual)
else:
    print('network structure %s not supported' % opt.network)
    raise ValueError

if opt.loss_func == 'l2':
    loss_crit = torch.nn.MSELoss()
elif opt.loss_func == 'l1':
    loss_crit = torch.nn.SmoothL1Loss()
else:
    print('loss_func %s not supported' % opt.loss_func)
    raise ValueError
pnsr_crit = torch.nn.MSELoss()

if len(opt.gpu_ids) > 0:
    net.cuda()
    if len(opt.gpu_ids) > 1:
        net = torch.nn.DataParallel(net)
    loss_crit = loss_crit.cuda()
    pnsr_crit = pnsr_crit.cuda()

optimizer = optim.Adam(net.parameters(), lr=opt.lr)
step_optim_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.lr_step, gamma=opt.lr_gamma)
loss_avg = MovingAvg(pool_size=50)

start_epoch = 0
total_iter = 0

if os.path.exists(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth')):
    print('resuming from latest.pth')
    latest_info = torch.load(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))
    start_epoch = latest_info['epoch']
    total_iter = latest_info['total_iter']
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(latest_info['net_state'])
    else:
        net.load_state_dict(latest_info['net_state'])
    optimizer.load_state_dict(latest_info['optim_state'])

if opt.resume_epoch > 0:
    start_epoch = opt.resume_epoch
    total_iter = opt.resume_epoch * len(train_dataloader)
    resume_path = os.path.join(opt.resume_epoch, 'net_epoch_%d.pth') % opt.resume_epoch
    print('resume from : %s' % resume_path)
    assert os.path.exists(resume_path), 'cannot find the resume model: %s ' % resume_path
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(torch.load(resume_path))
    else:
        net.load_state_dict(torch.load(resume_path))

for epoch in range(start_epoch, opt.epochs):
    visualizer.print_logs("Start to train epoch %d" % epoch)
    net.train()
    for iter, data in enumerate(train_dataloader):
        total_iter += 1
        optimizer.zero_grad()
        step_optim_scheduler.step(epoch)

        batch_input_img, batch_input_edge,  batch_gt = data
        if len(opt.gpu_ids) > 0:
            batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

        if opt.no_edge:
            batch_input = batch_input_img
        else:
            batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
        batch_input_v = Variable(batch_input)
        if opt.only_residual:
            batch_gt_v = Variable(batch_gt - (batch_input_img+128))
        else:
            batch_gt_v = Variable(batch_gt)

        pred = net(batch_input_v)

        loss = loss_crit(pred, batch_gt_v)
        avg_loss = loss_avg.set_curr_val(loss.data)

        loss.backward()
        optimizer.step()

        if iter % opt.print_iter == 0:
            visualizer.plot_current_losses(total_iter, { 'loss': loss})
            visualizer.print_logs('%s Step[%d/%d], lr: %f, mv_avg_loss: %f, loss: %f' %
                                    (str(datetime.datetime.now()).split(' ')[1], iter, len(train_dataloader),
                                     step_optim_scheduler.get_lr()[0], avg_loss, loss))

        if total_iter % opt.save_latest_freq == 0:
            latest_info = {'total_iter': total_iter,
                           'epoch': epoch,
                           'optim_state': optimizer.state_dict()}
            if len(opt.gpu_ids) > 1:
                latest_info['net_state'] = net.module.state_dict()
            else:
                latest_info['net_state'] = net.state_dict()
            print('save lastest model.')
            torch.save(latest_info, os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))

    if (epoch+1) % opt.save_epoch == 0 :
        visualizer.print_logs('saving model for epoch %d' % epoch)
        if len(opt.gpu_ids) > 1:
            torch.save(net.module.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch+1)))
        else:
            torch.save(net.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch + 1)))

    if opt.do_test:        
        avg_psnr = 0
        task_cnt = 0
        net.eval()
        with torch.no_grad():
            for iter, data in enumerate(test_dataloader):
                batch_input_img, batch_input_edge,  batch_gt = data
                if len(opt.gpu_ids) > 0:
                    batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

                if opt.no_edge:
                    batch_input = batch_input_img
                else:
                    batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
                batch_input_v = Variable(batch_input)
                batch_gt_v = Variable(batch_gt)


                pred = net(batch_input_v)

                if opt.only_residual:
                    loss = pnsr_crit(pred+Variable(batch_input_img+128), batch_gt_v)
                else:
                    loss = pnsr_crit(pred, batch_gt_v)
                avg_psnr += 10 * np.log10(255 * 255 / loss.item())
                task_cnt += 1

        visualizer.print_logs('Testing for epoch: %d' % epoch)
        visualizer.print_logs('Average test PNSR is %f for %d images' % (avg_psnr/task_cnt, task_cnt))







network: GCANet
name: default_exp
gpu_ids: 5
epochs: 100
lr: 0.001
lr_step: 40
lr_gamma: 0.1
weight_decay: 0.0005
checkpoints_dir: checkpoint
logDir: tblogdir
resume_dir: checkpoint/default_exp
resume_epoch: 0
save_epoch: 5
save_latest_freq: 5000
test_epoch: 5
test_max_size: 1080
size_unit: 8
print_iter: 100
input_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy
gt_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/test/clear
test_input_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy
test_gt_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear
num_workers: 16
batch_size: 2
only_residual: False
loss_func: l2
inc: 3
outc: 3
force_rgb: False
no_edge: False
================ Training Loss (Mon Nov 25 16:36:02 2024) ================
import os
import datetime
import argparse
import numpy as np

import torch
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import DataLoader

from ImagePairPrefixFolder import ImagePairPrefixFolder, var_custom_collate
from utils import MovingAvg
from tf_visualizer import TFVisualizer

parser = argparse.ArgumentParser()
parser.add_argument('--network', default='GCANet')
parser.add_argument('--name', default='default_exp')
parser.add_argument('--gpu_ids', default='5')
parser.add_argument('--epochs', type=int, default=100)
parser.add_argument('--lr', type=float, default=0.001)
parser.add_argument('--lr_step', type=int, default=40)
parser.add_argument('--lr_gamma', type=float, default=0.1)
parser.add_argument('--weight_decay', type=float, default=0.0005)
parser.add_argument('--checkpoints_dir', default='checkpoint')
parser.add_argument('--logDir', default='tblogdir')
parser.add_argument('--resume_dir', default='')
parser.add_argument('--resume_epoch', type=int, default=0)
parser.add_argument('--save_epoch', type=int, default=5)
parser.add_argument('--save_latest_freq', type=int, default=5000)
parser.add_argument('--test_epoch', type=int, default=5)
parser.add_argument('--test_max_size', type=int, default=1080)
parser.add_argument('--size_unit', type=int,  default=8)
parser.add_argument('--print_iter', type=int,  default=100)
parser.add_argument('--input_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy')
parser.add_argument('--gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/clear')
parser.add_argument('--test_input_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy')
parser.add_argument('--test_gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear')
parser.add_argument('--num_workers', type=int, default=16)
parser.add_argument('--batch_size', type=int, default=2)
parser.add_argument('--only_residual', action='store_true', help='regress residual rather than image')
parser.add_argument('--loss_func', default='l2', help='l2|l1')
parser.add_argument('--inc', type=int, default=3)
parser.add_argument('--outc', type=int, default=3)
parser.add_argument('--force_rgb', action='store_true')
parser.add_argument('--no_edge', action='store_true')

opt = parser.parse_args()

opt.input_folder = os.path.expanduser(opt.input_folder)
opt.gt_folder = os.path.expanduser(opt.gt_folder)
opt.test_input_folder = os.path.expanduser(opt.test_input_folder)
opt.test_gt_folder = os.path.expanduser(opt.test_gt_folder)


if not os.path.exists(os.path.join(opt.checkpoints_dir, opt.name)):
    os.makedirs(os.path.join(opt.checkpoints_dir, opt.name))
opt.resume_dir = opt.resume_dir if opt.resume_dir != '' else os.path.join(opt.checkpoints_dir, opt.name)

visualizer = TFVisualizer(opt)
### Log out
with open(os.path.realpath(__file__), 'r') as fid:
    visualizer.print_logs(fid.read())

## print argument
for key, val in vars(opt).items():
    visualizer.print_logs('%s: %s' % (key, val))

opt.gpu_ids = [int(x) for x in opt.gpu_ids.split(',')]
assert all(0 <= x <= torch.cuda.device_count() for x in opt.gpu_ids), 'gpu id should ' \
                                                      'be 0~{0}'.format(torch.cuda.device_count())
torch.cuda.set_device(opt.gpu_ids[0])


train_dataset = ImagePairPrefixFolder(opt.input_folder, opt.gt_folder, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
train_dataloader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True,
                              collate_fn=var_custom_collate, pin_memory=True,
                              num_workers=opt.num_workers)

opt.do_test = opt.test_gt_folder != ''
if opt.do_test:
    test_dataset = ImagePairPrefixFolder(opt.test_input_folder, opt.test_gt_folder,
                                         max_img_size=opt.test_max_size, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,
                                 collate_fn=var_custom_collate, pin_memory=True,
                                 num_workers=1)

total_inc = opt.inc if opt.no_edge else opt.inc + 1
if opt.network == 'GCANet':
    from GCANet import GCANet
    net = GCANet(in_c=total_inc, out_c=3, only_residual=opt.only_residual)
else:
    print('network structure %s not supported' % opt.network)
    raise ValueError

if opt.loss_func == 'l2':
    loss_crit = torch.nn.MSELoss()
elif opt.loss_func == 'l1':
    loss_crit = torch.nn.SmoothL1Loss()
else:
    print('loss_func %s not supported' % opt.loss_func)
    raise ValueError
pnsr_crit = torch.nn.MSELoss()

if len(opt.gpu_ids) > 0:
    net.cuda()
    if len(opt.gpu_ids) > 1:
        net = torch.nn.DataParallel(net)
    loss_crit = loss_crit.cuda()
    pnsr_crit = pnsr_crit.cuda()

optimizer = optim.Adam(net.parameters(), lr=opt.lr)
step_optim_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.lr_step, gamma=opt.lr_gamma)
loss_avg = MovingAvg(pool_size=50)

start_epoch = 0
total_iter = 0

if os.path.exists(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth')):
    print('resuming from latest.pth')
    latest_info = torch.load(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))
    start_epoch = latest_info['epoch']
    total_iter = latest_info['total_iter']
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(latest_info['net_state'])
    else:
        net.load_state_dict(latest_info['net_state'])
    optimizer.load_state_dict(latest_info['optim_state'])

if opt.resume_epoch > 0:
    start_epoch = opt.resume_epoch
    total_iter = opt.resume_epoch * len(train_dataloader)
    resume_path = os.path.join(opt.resume_epoch, 'net_epoch_%d.pth') % opt.resume_epoch
    print('resume from : %s' % resume_path)
    assert os.path.exists(resume_path), 'cannot find the resume model: %s ' % resume_path
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(torch.load(resume_path))
    else:
        net.load_state_dict(torch.load(resume_path))

for epoch in range(start_epoch, opt.epochs):
    visualizer.print_logs("Start to train epoch %d" % epoch)
    net.train()
    for iter, data in enumerate(train_dataloader):
        total_iter += 1
        optimizer.zero_grad()
        step_optim_scheduler.step(epoch)

        batch_input_img, batch_input_edge,  batch_gt = data
        if len(opt.gpu_ids) > 0:
            batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

        if opt.no_edge:
            batch_input = batch_input_img
        else:
            batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
        batch_input_v = Variable(batch_input)
        if opt.only_residual:
            batch_gt_v = Variable(batch_gt - (batch_input_img+128))
        else:
            batch_gt_v = Variable(batch_gt)

        pred = net(batch_input_v)

        loss = loss_crit(pred, batch_gt_v)
        avg_loss = loss_avg.set_curr_val(loss.data)

        loss.backward()
        optimizer.step()

        if iter % opt.print_iter == 0:
            visualizer.plot_current_losses(total_iter, { 'loss': loss})
            visualizer.print_logs('%s Step[%d/%d], lr: %f, mv_avg_loss: %f, loss: %f' %
                                    (str(datetime.datetime.now()).split(' ')[1], iter, len(train_dataloader),
                                     step_optim_scheduler.get_lr()[0], avg_loss, loss))

        if total_iter % opt.save_latest_freq == 0:
            latest_info = {'total_iter': total_iter,
                           'epoch': epoch,
                           'optim_state': optimizer.state_dict()}
            if len(opt.gpu_ids) > 1:
                latest_info['net_state'] = net.module.state_dict()
            else:
                latest_info['net_state'] = net.state_dict()
            print('save lastest model.')
            torch.save(latest_info, os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))

    if (epoch+1) % opt.save_epoch == 0 :
        visualizer.print_logs('saving model for epoch %d' % epoch)
        if len(opt.gpu_ids) > 1:
            torch.save(net.module.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch+1)))
        else:
            torch.save(net.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch + 1)))

    if opt.do_test:        
        avg_psnr = 0
        task_cnt = 0
        net.eval()
        with torch.no_grad():
            for iter, data in enumerate(test_dataloader):
                batch_input_img, batch_input_edge,  batch_gt = data
                if len(opt.gpu_ids) > 0:
                    batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

                if opt.no_edge:
                    batch_input = batch_input_img
                else:
                    batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
                batch_input_v = Variable(batch_input)
                batch_gt_v = Variable(batch_gt)


                pred = net(batch_input_v)

                if opt.only_residual:
                    loss = pnsr_crit(pred+Variable(batch_input_img+128), batch_gt_v)
                else:
                    loss = pnsr_crit(pred, batch_gt_v)
                avg_psnr += 10 * np.log10(255 * 255 / loss.item())
                task_cnt += 1

        visualizer.print_logs('Testing for epoch: %d' % epoch)
        visualizer.print_logs('Average test PNSR is %f for %d images' % (avg_psnr/task_cnt, task_cnt))







network: GCANet
name: default_exp
gpu_ids: 5
epochs: 100
lr: 0.001
lr_step: 40
lr_gamma: 0.1
weight_decay: 0.0005
checkpoints_dir: checkpoint
logDir: tblogdir
resume_dir: checkpoint/default_exp
resume_epoch: 0
save_epoch: 5
save_latest_freq: 5000
test_epoch: 5
test_max_size: 1080
size_unit: 8
print_iter: 100
input_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy
gt_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/clear
test_input_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy
test_gt_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear
num_workers: 16
batch_size: 2
only_residual: False
loss_func: l2
inc: 3
outc: 3
force_rgb: False
no_edge: False
Start to train epoch 0
================ Training Loss (Mon Nov 25 16:38:08 2024) ================
import os
import datetime
import argparse
import numpy as np

import torch
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import DataLoader

from ImagePairPrefixFolder import ImagePairPrefixFolder, var_custom_collate
from utils import MovingAvg
from tf_visualizer import TFVisualizer

parser = argparse.ArgumentParser()
parser.add_argument('--network', default='GCANet')
parser.add_argument('--name', default='default_exp')
parser.add_argument('--gpu_ids', default='5')
parser.add_argument('--epochs', type=int, default=100)
parser.add_argument('--lr', type=float, default=0.001)
parser.add_argument('--lr_step', type=int, default=40)
parser.add_argument('--lr_gamma', type=float, default=0.1)
parser.add_argument('--weight_decay', type=float, default=0.0005)
parser.add_argument('--checkpoints_dir', default='checkpoint')
parser.add_argument('--logDir', default='tblogdir')
parser.add_argument('--resume_dir', default='')
parser.add_argument('--resume_epoch', type=int, default=0)
parser.add_argument('--save_epoch', type=int, default=5)
parser.add_argument('--save_latest_freq', type=int, default=5000)
parser.add_argument('--test_epoch', type=int, default=5)
parser.add_argument('--test_max_size', type=int, default=1080)
parser.add_argument('--size_unit', type=int,  default=8)
parser.add_argument('--print_iter', type=int,  default=100)
parser.add_argument('--input_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy')
parser.add_argument('--gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/clear')
parser.add_argument('--test_input_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy')
parser.add_argument('--test_gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear')
parser.add_argument('--num_workers', type=int, default=16)
parser.add_argument('--batch_size', type=int, default=2)
parser.add_argument('--only_residual', action='store_true', help='regress residual rather than image')
parser.add_argument('--loss_func', default='l2', help='l2|l1')
parser.add_argument('--inc', type=int, default=3)
parser.add_argument('--outc', type=int, default=3)
parser.add_argument('--force_rgb', action='store_true')
parser.add_argument('--no_edge', action='store_true')

opt = parser.parse_args()

opt.input_folder = os.path.expanduser(opt.input_folder)
opt.gt_folder = os.path.expanduser(opt.gt_folder)
opt.test_input_folder = os.path.expanduser(opt.test_input_folder)
opt.test_gt_folder = os.path.expanduser(opt.test_gt_folder)


if not os.path.exists(os.path.join(opt.checkpoints_dir, opt.name)):
    os.makedirs(os.path.join(opt.checkpoints_dir, opt.name))
opt.resume_dir = opt.resume_dir if opt.resume_dir != '' else os.path.join(opt.checkpoints_dir, opt.name)

visualizer = TFVisualizer(opt)
### Log out
with open(os.path.realpath(__file__), 'r') as fid:
    visualizer.print_logs(fid.read())

## print argument
for key, val in vars(opt).items():
    visualizer.print_logs('%s: %s' % (key, val))

opt.gpu_ids = [int(x) for x in opt.gpu_ids.split(',')]
assert all(0 <= x <= torch.cuda.device_count() for x in opt.gpu_ids), 'gpu id should ' \
                                                      'be 0~{0}'.format(torch.cuda.device_count())
torch.cuda.set_device(opt.gpu_ids[0])


train_dataset = ImagePairPrefixFolder(opt.input_folder, opt.gt_folder, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
train_dataloader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True,
                              collate_fn=var_custom_collate, pin_memory=True,
                              num_workers=opt.num_workers)

opt.do_test = opt.test_gt_folder != ''
if opt.do_test:
    test_dataset = ImagePairPrefixFolder(opt.test_input_folder, opt.test_gt_folder,
                                         max_img_size=opt.test_max_size, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,
                                 collate_fn=var_custom_collate, pin_memory=True,
                                 num_workers=1)

total_inc = opt.inc if opt.no_edge else opt.inc + 1
if opt.network == 'GCANet':
    from GCANet import GCANet
    net = GCANet(in_c=total_inc, out_c=3, only_residual=opt.only_residual)
else:
    print('network structure %s not supported' % opt.network)
    raise ValueError

if opt.loss_func == 'l2':
    loss_crit = torch.nn.MSELoss()
elif opt.loss_func == 'l1':
    loss_crit = torch.nn.SmoothL1Loss()
else:
    print('loss_func %s not supported' % opt.loss_func)
    raise ValueError
pnsr_crit = torch.nn.MSELoss()

if len(opt.gpu_ids) > 0:
    net.cuda()
    if len(opt.gpu_ids) > 1:
        net = torch.nn.DataParallel(net)
    loss_crit = loss_crit.cuda()
    pnsr_crit = pnsr_crit.cuda()

optimizer = optim.Adam(net.parameters(), lr=opt.lr)
step_optim_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.lr_step, gamma=opt.lr_gamma)
loss_avg = MovingAvg(pool_size=50)

start_epoch = 0
total_iter = 0

if os.path.exists(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth')):
    print('resuming from latest.pth')
    latest_info = torch.load(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))
    start_epoch = latest_info['epoch']
    total_iter = latest_info['total_iter']
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(latest_info['net_state'])
    else:
        net.load_state_dict(latest_info['net_state'])
    optimizer.load_state_dict(latest_info['optim_state'])

if opt.resume_epoch > 0:
    start_epoch = opt.resume_epoch
    total_iter = opt.resume_epoch * len(train_dataloader)
    resume_path = os.path.join(opt.resume_epoch, 'net_epoch_%d.pth') % opt.resume_epoch
    print('resume from : %s' % resume_path)
    assert os.path.exists(resume_path), 'cannot find the resume model: %s ' % resume_path
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(torch.load(resume_path))
    else:
        net.load_state_dict(torch.load(resume_path))

for epoch in range(start_epoch, opt.epochs):
    visualizer.print_logs("Start to train epoch %d" % epoch)
    net.train()
    for iter, data in enumerate(train_dataloader):
        total_iter += 1
        optimizer.zero_grad()
        step_optim_scheduler.step(epoch)

        batch_input_img, batch_input_edge,  batch_gt = data
        if len(opt.gpu_ids) > 0:
            batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

        if opt.no_edge:
            batch_input = batch_input_img
        else:
            batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
        batch_input_v = Variable(batch_input)
        if opt.only_residual:
            batch_gt_v = Variable(batch_gt - (batch_input_img+128))
        else:
            batch_gt_v = Variable(batch_gt)

        pred = net(batch_input_v)

        loss = loss_crit(pred, batch_gt_v)
        avg_loss = loss_avg.set_curr_val(loss.data)

        loss.backward()
        optimizer.step()

        if iter % opt.print_iter == 0:
            visualizer.plot_current_losses(total_iter, { 'loss': loss})
            visualizer.print_logs('%s Step[%d/%d], lr: %f, mv_avg_loss: %f, loss: %f' %
                                    (str(datetime.datetime.now()).split(' ')[1], iter, len(train_dataloader),
                                     step_optim_scheduler.get_lr()[0], avg_loss, loss))

        if total_iter % opt.save_latest_freq == 0:
            latest_info = {'total_iter': total_iter,
                           'epoch': epoch,
                           'optim_state': optimizer.state_dict()}
            if len(opt.gpu_ids) > 1:
                latest_info['net_state'] = net.module.state_dict()
            else:
                latest_info['net_state'] = net.state_dict()
            print('save lastest model.')
            torch.save(latest_info, os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))

    if (epoch+1) % opt.save_epoch == 0 :
        visualizer.print_logs('saving model for epoch %d' % epoch)
        if len(opt.gpu_ids) > 1:
            torch.save(net.module.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch+1)))
        else:
            torch.save(net.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch + 1)))

    if opt.do_test:        
        avg_psnr = 0
        task_cnt = 0
        net.eval()
        with torch.no_grad():
            for iter, data in enumerate(test_dataloader):
                batch_input_img, batch_input_edge,  batch_gt = data
                if len(opt.gpu_ids) > 0:
                    batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

                if opt.no_edge:
                    batch_input = batch_input_img
                else:
                    batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
                batch_input_v = Variable(batch_input)
                batch_gt_v = Variable(batch_gt)


                pred = net(batch_input_v)

                if opt.only_residual:
                    loss = pnsr_crit(pred+Variable(batch_input_img+128), batch_gt_v)
                else:
                    loss = pnsr_crit(pred, batch_gt_v)
                avg_psnr += 10 * np.log10(255 * 255 / loss.item())
                task_cnt += 1

        visualizer.print_logs('Testing for epoch: %d' % epoch)
        visualizer.print_logs('Average test PNSR is %f for %d images' % (avg_psnr/task_cnt, task_cnt))







network: GCANet
name: default_exp
gpu_ids: 5
epochs: 100
lr: 0.001
lr_step: 40
lr_gamma: 0.1
weight_decay: 0.0005
checkpoints_dir: checkpoint
logDir: tblogdir
resume_dir: checkpoint/default_exp
resume_epoch: 0
save_epoch: 5
save_latest_freq: 5000
test_epoch: 5
test_max_size: 1080
size_unit: 8
print_iter: 100
input_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy
gt_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/clear
test_input_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy
test_gt_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear
num_workers: 16
batch_size: 2
only_residual: False
loss_func: l2
inc: 3
outc: 3
force_rgb: False
no_edge: False
Start to train epoch 0
================ Training Loss (Mon Nov 25 16:48:00 2024) ================
import os
import datetime
import argparse
import numpy as np

import torch
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import DataLoader

from ImagePairPrefixFolder import ImagePairPrefixFolder, var_custom_collate
from utils import MovingAvg
from tf_visualizer import TFVisualizer

parser = argparse.ArgumentParser()
parser.add_argument('--network', default='GCANet')
parser.add_argument('--name', default='default_exp')
parser.add_argument('--gpu_ids', default='5')
parser.add_argument('--epochs', type=int, default=100)
parser.add_argument('--lr', type=float, default=0.001)
parser.add_argument('--lr_step', type=int, default=40)
parser.add_argument('--lr_gamma', type=float, default=0.1)
parser.add_argument('--weight_decay', type=float, default=0.0005)
parser.add_argument('--checkpoints_dir', default='checkpoint')
parser.add_argument('--logDir', default='tblogdir')
parser.add_argument('--resume_dir', default='')
parser.add_argument('--resume_epoch', type=int, default=0)
parser.add_argument('--save_epoch', type=int, default=5)
parser.add_argument('--save_latest_freq', type=int, default=5000)
parser.add_argument('--test_epoch', type=int, default=5)
parser.add_argument('--test_max_size', type=int, default=1080)
parser.add_argument('--size_unit', type=int,  default=8)
parser.add_argument('--print_iter', type=int,  default=100)
parser.add_argument('--input_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy')
parser.add_argument('--gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/clear')
parser.add_argument('--test_input_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy')
parser.add_argument('--test_gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear')
parser.add_argument('--num_workers', type=int, default=16)
parser.add_argument('--batch_size', type=int, default=2)
parser.add_argument('--only_residual', action='store_true', help='regress residual rather than image')
parser.add_argument('--loss_func', default='l2', help='l2|l1')
parser.add_argument('--inc', type=int, default=3)
parser.add_argument('--outc', type=int, default=3)
parser.add_argument('--force_rgb', action='store_true')
parser.add_argument('--no_edge', action='store_true')

opt = parser.parse_args()

opt.input_folder = os.path.expanduser(opt.input_folder)
opt.gt_folder = os.path.expanduser(opt.gt_folder)
opt.test_input_folder = os.path.expanduser(opt.test_input_folder)
opt.test_gt_folder = os.path.expanduser(opt.test_gt_folder)


if not os.path.exists(os.path.join(opt.checkpoints_dir, opt.name)):
    os.makedirs(os.path.join(opt.checkpoints_dir, opt.name))
opt.resume_dir = opt.resume_dir if opt.resume_dir != '' else os.path.join(opt.checkpoints_dir, opt.name)

visualizer = TFVisualizer(opt)
### Log out
with open(os.path.realpath(__file__), 'r') as fid:
    visualizer.print_logs(fid.read())

## print argument
for key, val in vars(opt).items():
    visualizer.print_logs('%s: %s' % (key, val))

opt.gpu_ids = [int(x) for x in opt.gpu_ids.split(',')]
assert all(0 <= x <= torch.cuda.device_count() for x in opt.gpu_ids), 'gpu id should ' \
                                                      'be 0~{0}'.format(torch.cuda.device_count())
torch.cuda.set_device(opt.gpu_ids[0])


train_dataset = ImagePairPrefixFolder(opt.input_folder, opt.gt_folder, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
train_dataloader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True,
                              collate_fn=var_custom_collate, pin_memory=True,
                              num_workers=opt.num_workers)

opt.do_test = opt.test_gt_folder != ''
if opt.do_test:
    test_dataset = ImagePairPrefixFolder(opt.test_input_folder, opt.test_gt_folder,
                                         max_img_size=opt.test_max_size, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,
                                 collate_fn=var_custom_collate, pin_memory=True,
                                 num_workers=1)

total_inc = opt.inc if opt.no_edge else opt.inc + 1
if opt.network == 'GCANet':
    from GCANet import GCANet
    net = GCANet(in_c=total_inc, out_c=3, only_residual=opt.only_residual)
else:
    print('network structure %s not supported' % opt.network)
    raise ValueError

if opt.loss_func == 'l2':
    loss_crit = torch.nn.MSELoss()
elif opt.loss_func == 'l1':
    loss_crit = torch.nn.SmoothL1Loss()
else:
    print('loss_func %s not supported' % opt.loss_func)
    raise ValueError
pnsr_crit = torch.nn.MSELoss()

if len(opt.gpu_ids) > 0:
    net.cuda()
    if len(opt.gpu_ids) > 1:
        net = torch.nn.DataParallel(net)
    loss_crit = loss_crit.cuda()
    pnsr_crit = pnsr_crit.cuda()

optimizer = optim.Adam(net.parameters(), lr=opt.lr)
step_optim_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.lr_step, gamma=opt.lr_gamma)
loss_avg = MovingAvg(pool_size=50)

start_epoch = 0
total_iter = 0

if os.path.exists(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth')):
    print('resuming from latest.pth')
    latest_info = torch.load(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))
    start_epoch = latest_info['epoch']
    total_iter = latest_info['total_iter']
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(latest_info['net_state'])
    else:
        net.load_state_dict(latest_info['net_state'])
    optimizer.load_state_dict(latest_info['optim_state'])

if opt.resume_epoch > 0:
    start_epoch = opt.resume_epoch
    total_iter = opt.resume_epoch * len(train_dataloader)
    resume_path = os.path.join(opt.resume_epoch, 'net_epoch_%d.pth') % opt.resume_epoch
    print('resume from : %s' % resume_path)
    assert os.path.exists(resume_path), 'cannot find the resume model: %s ' % resume_path
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(torch.load(resume_path))
    else:
        net.load_state_dict(torch.load(resume_path))

for epoch in range(start_epoch, opt.epochs):
    visualizer.print_logs("Start to train epoch %d" % epoch)
    net.train()
    for iter, data in enumerate(train_dataloader):
        total_iter += 1
        optimizer.zero_grad()
        step_optim_scheduler.step(epoch)

        batch_input_img, batch_input_edge,  batch_gt = data
        if len(opt.gpu_ids) > 0:
            batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

        if opt.no_edge:
            batch_input = batch_input_img
        else:
            batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
        batch_input_v = Variable(batch_input)
        if opt.only_residual:
            batch_gt_v = Variable(batch_gt - (batch_input_img+128))
        else:
            batch_gt_v = Variable(batch_gt)

        pred = net(batch_input_v)

        loss = loss_crit(pred, batch_gt_v)
        avg_loss = loss_avg.set_curr_val(loss.data)

        loss.backward()
        optimizer.step()

        if iter % opt.print_iter == 0:
            visualizer.plot_current_losses(total_iter, { 'loss': loss})
            visualizer.print_logs('%s Step[%d/%d], lr: %f, mv_avg_loss: %f, loss: %f' %
                                    (str(datetime.datetime.now()).split(' ')[1], iter, len(train_dataloader),
                                     step_optim_scheduler.get_lr()[0], avg_loss, loss))

        if total_iter % opt.save_latest_freq == 0:
            latest_info = {'total_iter': total_iter,
                           'epoch': epoch,
                           'optim_state': optimizer.state_dict()}
            if len(opt.gpu_ids) > 1:
                latest_info['net_state'] = net.module.state_dict()
            else:
                latest_info['net_state'] = net.state_dict()
            print('save lastest model.')
            torch.save(latest_info, os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))

    if (epoch+1) % opt.save_epoch == 0 :
        visualizer.print_logs('saving model for epoch %d' % epoch)
        if len(opt.gpu_ids) > 1:
            torch.save(net.module.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch+1)))
        else:
            torch.save(net.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch + 1)))

    if opt.do_test:        
        avg_psnr = 0
        task_cnt = 0
        net.eval()
        with torch.no_grad():
            for iter, data in enumerate(test_dataloader):
                batch_input_img, batch_input_edge,  batch_gt = data
                if len(opt.gpu_ids) > 0:
                    batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

                if opt.no_edge:
                    batch_input = batch_input_img
                else:
                    batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
                batch_input_v = Variable(batch_input)
                batch_gt_v = Variable(batch_gt)


                pred = net(batch_input_v)

                if opt.only_residual:
                    loss = pnsr_crit(pred+Variable(batch_input_img+128), batch_gt_v)
                else:
                    loss = pnsr_crit(pred, batch_gt_v)
                avg_psnr += 10 * np.log10(255 * 255 / loss.item())
                task_cnt += 1

        visualizer.print_logs('Testing for epoch: %d' % epoch)
        visualizer.print_logs('Average test PNSR is %f for %d images' % (avg_psnr/task_cnt, task_cnt))







network: GCANet
name: default_exp
gpu_ids: 5
epochs: 100
lr: 0.001
lr_step: 40
lr_gamma: 0.1
weight_decay: 0.0005
checkpoints_dir: checkpoint
logDir: tblogdir
resume_dir: checkpoint/default_exp
resume_epoch: 0
save_epoch: 5
save_latest_freq: 5000
test_epoch: 5
test_max_size: 1080
size_unit: 8
print_iter: 100
input_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy
gt_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/clear
test_input_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy
test_gt_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear
num_workers: 16
batch_size: 2
only_residual: False
loss_func: l2
inc: 3
outc: 3
force_rgb: False
no_edge: False
Start to train epoch 0
================ Training Loss (Mon Nov 25 16:51:16 2024) ================
import os
import datetime
import argparse
import numpy as np

import torch
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import DataLoader

from ImagePairPrefixFolder import ImagePairPrefixFolder, var_custom_collate
from utils import MovingAvg
from tf_visualizer import TFVisualizer

parser = argparse.ArgumentParser()
parser.add_argument('--network', default='GCANet')
parser.add_argument('--name', default='default_exp')
parser.add_argument('--gpu_ids', default='5')
parser.add_argument('--epochs', type=int, default=100)
parser.add_argument('--lr', type=float, default=0.001)
parser.add_argument('--lr_step', type=int, default=40)
parser.add_argument('--lr_gamma', type=float, default=0.1)
parser.add_argument('--weight_decay', type=float, default=0.0005)
parser.add_argument('--checkpoints_dir', default='checkpoint')
parser.add_argument('--logDir', default='tblogdir')
parser.add_argument('--resume_dir', default='')
parser.add_argument('--resume_epoch', type=int, default=0)
parser.add_argument('--save_epoch', type=int, default=5)
parser.add_argument('--save_latest_freq', type=int, default=5000)
parser.add_argument('--test_epoch', type=int, default=5)
parser.add_argument('--test_max_size', type=int, default=1080)
parser.add_argument('--size_unit', type=int,  default=8)
parser.add_argument('--print_iter', type=int,  default=100)
parser.add_argument('--input_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy')
parser.add_argument('--gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/clear')
parser.add_argument('--test_input_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy')
parser.add_argument('--test_gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear')
parser.add_argument('--num_workers', type=int, default=16)
parser.add_argument('--batch_size', type=int, default=2)
parser.add_argument('--only_residual', action='store_true', help='regress residual rather than image')
parser.add_argument('--loss_func', default='l2', help='l2|l1')
parser.add_argument('--inc', type=int, default=3)
parser.add_argument('--outc', type=int, default=3)
parser.add_argument('--force_rgb', action='store_true')
parser.add_argument('--no_edge', action='store_true')

opt = parser.parse_args()

opt.input_folder = os.path.expanduser(opt.input_folder)
opt.gt_folder = os.path.expanduser(opt.gt_folder)
opt.test_input_folder = os.path.expanduser(opt.test_input_folder)
opt.test_gt_folder = os.path.expanduser(opt.test_gt_folder)


if not os.path.exists(os.path.join(opt.checkpoints_dir, opt.name)):
    os.makedirs(os.path.join(opt.checkpoints_dir, opt.name))
opt.resume_dir = opt.resume_dir if opt.resume_dir != '' else os.path.join(opt.checkpoints_dir, opt.name)

visualizer = TFVisualizer(opt)
### Log out
with open(os.path.realpath(__file__), 'r') as fid:
    visualizer.print_logs(fid.read())

## print argument
for key, val in vars(opt).items():
    visualizer.print_logs('%s: %s' % (key, val))

opt.gpu_ids = [int(x) for x in opt.gpu_ids.split(',')]
assert all(0 <= x <= torch.cuda.device_count() for x in opt.gpu_ids), 'gpu id should ' \
                                                      'be 0~{0}'.format(torch.cuda.device_count())
torch.cuda.set_device(opt.gpu_ids[0])


train_dataset = ImagePairPrefixFolder(opt.input_folder, opt.gt_folder, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
train_dataloader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True,
                              collate_fn=var_custom_collate, pin_memory=True,
                              num_workers=opt.num_workers)

opt.do_test = opt.test_gt_folder != ''
if opt.do_test:
    test_dataset = ImagePairPrefixFolder(opt.test_input_folder, opt.test_gt_folder,
                                         max_img_size=opt.test_max_size, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,
                                 collate_fn=var_custom_collate, pin_memory=True,
                                 num_workers=1)

total_inc = opt.inc if opt.no_edge else opt.inc + 1
if opt.network == 'GCANet':
    from GCANet import GCANet
    net = GCANet(in_c=total_inc, out_c=3, only_residual=opt.only_residual)
else:
    print('network structure %s not supported' % opt.network)
    raise ValueError

if opt.loss_func == 'l2':
    loss_crit = torch.nn.MSELoss()
elif opt.loss_func == 'l1':
    loss_crit = torch.nn.SmoothL1Loss()
else:
    print('loss_func %s not supported' % opt.loss_func)
    raise ValueError
pnsr_crit = torch.nn.MSELoss()

if len(opt.gpu_ids) > 0:
    net.cuda()
    if len(opt.gpu_ids) > 1:
        net = torch.nn.DataParallel(net)
    loss_crit = loss_crit.cuda()
    pnsr_crit = pnsr_crit.cuda()

optimizer = optim.Adam(net.parameters(), lr=opt.lr)
step_optim_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.lr_step, gamma=opt.lr_gamma)
loss_avg = MovingAvg(pool_size=50)

start_epoch = 0
total_iter = 0

if os.path.exists(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth')):
    print('resuming from latest.pth')
    latest_info = torch.load(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))
    start_epoch = latest_info['epoch']
    total_iter = latest_info['total_iter']
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(latest_info['net_state'])
    else:
        net.load_state_dict(latest_info['net_state'])
    optimizer.load_state_dict(latest_info['optim_state'])

if opt.resume_epoch > 0:
    start_epoch = opt.resume_epoch
    total_iter = opt.resume_epoch * len(train_dataloader)
    resume_path = os.path.join(opt.resume_epoch, 'net_epoch_%d.pth') % opt.resume_epoch
    print('resume from : %s' % resume_path)
    assert os.path.exists(resume_path), 'cannot find the resume model: %s ' % resume_path
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(torch.load(resume_path))
    else:
        net.load_state_dict(torch.load(resume_path))

for epoch in range(start_epoch, opt.epochs):
    visualizer.print_logs("Start to train epoch %d" % epoch)
    net.train()
    for iter, data in enumerate(train_dataloader):
        total_iter += 1
        optimizer.zero_grad()
        step_optim_scheduler.step(epoch)

        batch_input_img, batch_input_edge,  batch_gt = data
        if len(opt.gpu_ids) > 0:
            batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

        if opt.no_edge:
            batch_input = batch_input_img
        else:
            batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
        batch_input_v = Variable(batch_input)
        if opt.only_residual:
            batch_gt_v = Variable(batch_gt - (batch_input_img+128))
        else:
            batch_gt_v = Variable(batch_gt)

        pred = net(batch_input_v)

        loss = loss_crit(pred, batch_gt_v)
        avg_loss = loss_avg.set_curr_val(loss.data)

        loss.backward()
        optimizer.step()

        if iter % opt.print_iter == 0:
            visualizer.plot_current_losses(total_iter, { 'loss': loss})
            visualizer.print_logs('%s Step[%d/%d], lr: %f, mv_avg_loss: %f, loss: %f' %
                                    (str(datetime.datetime.now()).split(' ')[1], iter, len(train_dataloader),
                                     step_optim_scheduler.get_lr()[0], avg_loss, loss))

        if total_iter % opt.save_latest_freq == 0:
            latest_info = {'total_iter': total_iter,
                           'epoch': epoch,
                           'optim_state': optimizer.state_dict()}
            if len(opt.gpu_ids) > 1:
                latest_info['net_state'] = net.module.state_dict()
            else:
                latest_info['net_state'] = net.state_dict()
            print('save lastest model.')
            torch.save(latest_info, os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))

    if (epoch+1) % opt.save_epoch == 0 :
        visualizer.print_logs('saving model for epoch %d' % epoch)
        if len(opt.gpu_ids) > 1:
            torch.save(net.module.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch+1)))
        else:
            torch.save(net.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch + 1)))

    if opt.do_test:        
        avg_psnr = 0
        task_cnt = 0
        net.eval()
        with torch.no_grad():
            for iter, data in enumerate(test_dataloader):
                batch_input_img, batch_input_edge,  batch_gt = data
                if len(opt.gpu_ids) > 0:
                    batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

                if opt.no_edge:
                    batch_input = batch_input_img
                else:
                    batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
                batch_input_v = Variable(batch_input)
                batch_gt_v = Variable(batch_gt)


                pred = net(batch_input_v)

                if opt.only_residual:
                    loss = pnsr_crit(pred+Variable(batch_input_img+128), batch_gt_v)
                else:
                    loss = pnsr_crit(pred, batch_gt_v)
                avg_psnr += 10 * np.log10(255 * 255 / loss.item())
                task_cnt += 1

        visualizer.print_logs('Testing for epoch: %d' % epoch)
        visualizer.print_logs('Average test PNSR is %f for %d images' % (avg_psnr/task_cnt, task_cnt))







network: GCANet
name: default_exp
gpu_ids: 5
epochs: 100
lr: 0.001
lr_step: 40
lr_gamma: 0.1
weight_decay: 0.0005
checkpoints_dir: checkpoint
logDir: tblogdir
resume_dir: checkpoint/default_exp
resume_epoch: 0
save_epoch: 5
save_latest_freq: 5000
test_epoch: 5
test_max_size: 1080
size_unit: 8
print_iter: 100
input_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy
gt_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/clear
test_input_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy
test_gt_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear
num_workers: 16
batch_size: 2
only_residual: False
loss_func: l2
inc: 3
outc: 3
force_rgb: False
no_edge: False
Start to train epoch 0
16:51:23.358671 Step[0/6995], lr: 0.001000, mv_avg_loss: 14736.213867, loss: 14736.213867
16:51:27.854237 Step[100/6995], lr: 0.001000, mv_avg_loss: 15218.478516, loss: 11715.419922
16:51:32.331571 Step[200/6995], lr: 0.001000, mv_avg_loss: 13428.748047, loss: 10776.640625
16:51:36.831246 Step[300/6995], lr: 0.001000, mv_avg_loss: 11902.912109, loss: 14650.273438
16:51:41.349153 Step[400/6995], lr: 0.001000, mv_avg_loss: 10336.538086, loss: 12989.794922
16:51:45.894256 Step[500/6995], lr: 0.001000, mv_avg_loss: 6763.268555, loss: 8834.320312
16:51:50.427804 Step[600/6995], lr: 0.001000, mv_avg_loss: 5814.761719, loss: 2040.860107
16:51:54.999253 Step[700/6995], lr: 0.001000, mv_avg_loss: 4222.622070, loss: 3059.210449
16:51:59.575548 Step[800/6995], lr: 0.001000, mv_avg_loss: 3458.502441, loss: 2569.081543
16:52:04.175231 Step[900/6995], lr: 0.001000, mv_avg_loss: 2581.826172, loss: 6019.588867
16:52:08.781443 Step[1000/6995], lr: 0.001000, mv_avg_loss: 2091.352295, loss: 2638.411621
16:52:13.386358 Step[1100/6995], lr: 0.001000, mv_avg_loss: 1562.025635, loss: 1293.888184
16:52:18.003494 Step[1200/6995], lr: 0.001000, mv_avg_loss: 1450.164795, loss: 1111.653198
16:52:22.642116 Step[1300/6995], lr: 0.001000, mv_avg_loss: 1709.813110, loss: 2197.128906
16:52:27.291252 Step[1400/6995], lr: 0.001000, mv_avg_loss: 1529.866943, loss: 1411.158936
16:52:31.943037 Step[1500/6995], lr: 0.001000, mv_avg_loss: 1478.726685, loss: 516.958374
16:52:36.616526 Step[1600/6995], lr: 0.001000, mv_avg_loss: 1368.194824, loss: 1994.950439
16:52:41.298322 Step[1700/6995], lr: 0.001000, mv_avg_loss: 1184.930420, loss: 1193.083252
16:52:45.985647 Step[1800/6995], lr: 0.001000, mv_avg_loss: 1321.781494, loss: 1472.407959
16:52:50.667278 Step[1900/6995], lr: 0.001000, mv_avg_loss: 1356.633911, loss: 567.388428
16:52:55.358200 Step[2000/6995], lr: 0.001000, mv_avg_loss: 1188.292603, loss: 1293.645508
16:53:00.047735 Step[2100/6995], lr: 0.001000, mv_avg_loss: 1390.732788, loss: 738.725586
16:53:04.749623 Step[2200/6995], lr: 0.001000, mv_avg_loss: 1144.771973, loss: 2467.315918
16:53:09.452824 Step[2300/6995], lr: 0.001000, mv_avg_loss: 1369.108887, loss: 510.250244
16:53:14.182252 Step[2400/6995], lr: 0.001000, mv_avg_loss: 1245.450439, loss: 2474.176270
16:53:18.891096 Step[2500/6995], lr: 0.001000, mv_avg_loss: 1104.524170, loss: 381.822021
16:53:23.612106 Step[2600/6995], lr: 0.001000, mv_avg_loss: 1122.210938, loss: 701.959656
16:53:28.334336 Step[2700/6995], lr: 0.001000, mv_avg_loss: 1097.079468, loss: 3567.273193
16:53:33.059789 Step[2800/6995], lr: 0.001000, mv_avg_loss: 1026.754395, loss: 501.336029
16:53:37.786454 Step[2900/6995], lr: 0.001000, mv_avg_loss: 991.611511, loss: 878.354980
16:53:42.540197 Step[3000/6995], lr: 0.001000, mv_avg_loss: 983.534912, loss: 548.329285
16:53:47.263150 Step[3100/6995], lr: 0.001000, mv_avg_loss: 944.353882, loss: 570.068359
16:53:52.003589 Step[3200/6995], lr: 0.001000, mv_avg_loss: 731.315247, loss: 350.786743
16:53:56.748808 Step[3300/6995], lr: 0.001000, mv_avg_loss: 903.886230, loss: 2281.497314
16:54:01.490034 Step[3400/6995], lr: 0.001000, mv_avg_loss: 831.359131, loss: 675.812256
16:54:06.230067 Step[3500/6995], lr: 0.001000, mv_avg_loss: 948.938110, loss: 542.520569
16:54:10.976662 Step[3600/6995], lr: 0.001000, mv_avg_loss: 882.496948, loss: 776.690613
16:54:15.719554 Step[3700/6995], lr: 0.001000, mv_avg_loss: 869.230591, loss: 855.609131
16:54:20.466524 Step[3800/6995], lr: 0.001000, mv_avg_loss: 737.020630, loss: 455.455536
16:54:25.219622 Step[3900/6995], lr: 0.001000, mv_avg_loss: 809.621155, loss: 682.438599
16:54:29.973791 Step[4000/6995], lr: 0.001000, mv_avg_loss: 783.372559, loss: 435.579102
16:54:34.731104 Step[4100/6995], lr: 0.001000, mv_avg_loss: 792.945679, loss: 1119.630249
16:54:39.492069 Step[4200/6995], lr: 0.001000, mv_avg_loss: 737.887146, loss: 1480.336914
16:54:44.252805 Step[4300/6995], lr: 0.001000, mv_avg_loss: 643.125122, loss: 650.083801
16:54:49.007121 Step[4400/6995], lr: 0.001000, mv_avg_loss: 731.148315, loss: 352.987061
16:54:53.859324 Step[4500/6995], lr: 0.001000, mv_avg_loss: 613.785400, loss: 406.767517
16:54:58.619684 Step[4600/6995], lr: 0.001000, mv_avg_loss: 616.208008, loss: 339.875092
16:55:03.390091 Step[4700/6995], lr: 0.001000, mv_avg_loss: 793.833252, loss: 290.231689
16:55:08.177623 Step[4800/6995], lr: 0.001000, mv_avg_loss: 551.191956, loss: 792.010620
16:55:12.947214 Step[4900/6995], lr: 0.001000, mv_avg_loss: 707.980347, loss: 369.361572
16:55:17.745421 Step[5000/6995], lr: 0.001000, mv_avg_loss: 616.366516, loss: 285.651550
16:55:22.527514 Step[5100/6995], lr: 0.001000, mv_avg_loss: 582.645935, loss: 250.341965
16:55:27.336754 Step[5200/6995], lr: 0.001000, mv_avg_loss: 529.222595, loss: 682.890320
16:55:32.144845 Step[5300/6995], lr: 0.001000, mv_avg_loss: 569.283875, loss: 539.612793
16:55:36.913132 Step[5400/6995], lr: 0.001000, mv_avg_loss: 577.879089, loss: 557.431763
16:55:41.680818 Step[5500/6995], lr: 0.001000, mv_avg_loss: 481.955658, loss: 1258.759521
16:55:46.456946 Step[5600/6995], lr: 0.001000, mv_avg_loss: 543.533691, loss: 574.200317
16:55:51.224406 Step[5700/6995], lr: 0.001000, mv_avg_loss: 420.093872, loss: 220.667664
16:55:56.000188 Step[5800/6995], lr: 0.001000, mv_avg_loss: 436.466431, loss: 457.257172
16:56:00.762229 Step[5900/6995], lr: 0.001000, mv_avg_loss: 485.752136, loss: 465.036987
16:56:05.514151 Step[6000/6995], lr: 0.001000, mv_avg_loss: 510.714233, loss: 764.429565
16:56:10.279549 Step[6100/6995], lr: 0.001000, mv_avg_loss: 467.202484, loss: 790.796875
16:56:15.040621 Step[6200/6995], lr: 0.001000, mv_avg_loss: 417.386108, loss: 324.441162
16:56:19.804048 Step[6300/6995], lr: 0.001000, mv_avg_loss: 453.506439, loss: 618.561157
16:56:24.560902 Step[6400/6995], lr: 0.001000, mv_avg_loss: 457.959717, loss: 338.803345
16:56:29.320826 Step[6500/6995], lr: 0.001000, mv_avg_loss: 475.766937, loss: 540.926514
16:56:34.090044 Step[6600/6995], lr: 0.001000, mv_avg_loss: 584.615051, loss: 486.371429
16:56:38.855260 Step[6700/6995], lr: 0.001000, mv_avg_loss: 537.430542, loss: 785.814636
16:56:43.628296 Step[6800/6995], lr: 0.001000, mv_avg_loss: 488.258514, loss: 405.816742
16:56:48.394299 Step[6900/6995], lr: 0.001000, mv_avg_loss: 411.793304, loss: 328.969330
Testing for epoch: 0
Average test PNSR is 22.218134 for 500 images
Start to train epoch 1
16:57:14.519871 Step[0/6995], lr: 0.001000, mv_avg_loss: 461.073090, loss: 294.754852
16:57:19.281254 Step[100/6995], lr: 0.001000, mv_avg_loss: 459.781464, loss: 274.227692
16:57:24.039846 Step[200/6995], lr: 0.001000, mv_avg_loss: 484.681702, loss: 256.939331
16:57:28.802187 Step[300/6995], lr: 0.001000, mv_avg_loss: 427.030182, loss: 1266.108154
16:57:33.566294 Step[400/6995], lr: 0.001000, mv_avg_loss: 444.423706, loss: 469.818298
16:57:38.343836 Step[500/6995], lr: 0.001000, mv_avg_loss: 424.745331, loss: 282.050171
16:57:43.107731 Step[600/6995], lr: 0.001000, mv_avg_loss: 448.774445, loss: 442.766235
16:57:47.876824 Step[700/6995], lr: 0.001000, mv_avg_loss: 449.481232, loss: 157.556976
16:57:52.655849 Step[800/6995], lr: 0.001000, mv_avg_loss: 394.006165, loss: 295.192261
16:57:57.428147 Step[900/6995], lr: 0.001000, mv_avg_loss: 395.261871, loss: 360.308441
16:58:02.200275 Step[1000/6995], lr: 0.001000, mv_avg_loss: 384.012695, loss: 521.850037
16:58:06.969076 Step[1100/6995], lr: 0.001000, mv_avg_loss: 417.662170, loss: 700.077026
16:58:11.737477 Step[1200/6995], lr: 0.001000, mv_avg_loss: 397.800415, loss: 258.356049
16:58:16.519219 Step[1300/6995], lr: 0.001000, mv_avg_loss: 512.202209, loss: 554.492126
16:58:21.298335 Step[1400/6995], lr: 0.001000, mv_avg_loss: 437.590668, loss: 262.922272
16:58:26.064458 Step[1500/6995], lr: 0.001000, mv_avg_loss: 374.502563, loss: 215.179047
16:58:30.840810 Step[1600/6995], lr: 0.001000, mv_avg_loss: 417.473907, loss: 420.798279
16:58:35.624377 Step[1700/6995], lr: 0.001000, mv_avg_loss: 410.257111, loss: 730.050964
16:58:40.403551 Step[1800/6995], lr: 0.001000, mv_avg_loss: 364.934570, loss: 169.429062
16:58:45.183355 Step[1900/6995], lr: 0.001000, mv_avg_loss: 420.270996, loss: 405.519226
16:58:49.974930 Step[2000/6995], lr: 0.001000, mv_avg_loss: 374.374908, loss: 280.948547
16:58:54.765859 Step[2100/6995], lr: 0.001000, mv_avg_loss: 318.639679, loss: 171.242432
16:58:59.539872 Step[2200/6995], lr: 0.001000, mv_avg_loss: 458.821625, loss: 844.453491
16:59:04.317372 Step[2300/6995], lr: 0.001000, mv_avg_loss: 399.647766, loss: 293.348511
16:59:09.095184 Step[2400/6995], lr: 0.001000, mv_avg_loss: 373.172974, loss: 194.584641
16:59:13.872668 Step[2500/6995], lr: 0.001000, mv_avg_loss: 386.407837, loss: 214.933350
16:59:18.656321 Step[2600/6995], lr: 0.001000, mv_avg_loss: 336.445984, loss: 187.287964
16:59:23.436978 Step[2700/6995], lr: 0.001000, mv_avg_loss: 357.604095, loss: 245.758057
16:59:28.219192 Step[2800/6995], lr: 0.001000, mv_avg_loss: 303.085083, loss: 145.777466
16:59:33.013920 Step[2900/6995], lr: 0.001000, mv_avg_loss: 375.745178, loss: 180.397858
16:59:37.796332 Step[3000/6995], lr: 0.001000, mv_avg_loss: 346.153442, loss: 393.408447
16:59:42.610623 Step[3100/6995], lr: 0.001000, mv_avg_loss: 330.667755, loss: 244.100571
16:59:47.398520 Step[3200/6995], lr: 0.001000, mv_avg_loss: 322.506287, loss: 299.254364
16:59:52.179249 Step[3300/6995], lr: 0.001000, mv_avg_loss: 344.395538, loss: 312.277222
16:59:56.997459 Step[3400/6995], lr: 0.001000, mv_avg_loss: 368.043854, loss: 945.015808
17:00:01.813836 Step[3500/6995], lr: 0.001000, mv_avg_loss: 288.387665, loss: 271.512512
17:00:06.592828 Step[3600/6995], lr: 0.001000, mv_avg_loss: 342.109131, loss: 221.467087
17:00:11.387737 Step[3700/6995], lr: 0.001000, mv_avg_loss: 334.284302, loss: 320.154114
17:00:16.183210 Step[3800/6995], lr: 0.001000, mv_avg_loss: 377.623077, loss: 235.832321
17:00:20.972052 Step[3900/6995], lr: 0.001000, mv_avg_loss: 370.755585, loss: 352.643311
17:00:25.769283 Step[4000/6995], lr: 0.001000, mv_avg_loss: 326.326385, loss: 351.974854
17:00:30.560101 Step[4100/6995], lr: 0.001000, mv_avg_loss: 315.041962, loss: 376.410583
17:00:35.343265 Step[4200/6995], lr: 0.001000, mv_avg_loss: 373.724548, loss: 248.524994
17:00:40.127075 Step[4300/6995], lr: 0.001000, mv_avg_loss: 316.694763, loss: 162.924835
17:00:44.898192 Step[4400/6995], lr: 0.001000, mv_avg_loss: 309.132599, loss: 365.938965
17:00:49.686458 Step[4500/6995], lr: 0.001000, mv_avg_loss: 309.672333, loss: 470.446411
17:00:54.472750 Step[4600/6995], lr: 0.001000, mv_avg_loss: 311.765137, loss: 173.844299
17:00:59.263787 Step[4700/6995], lr: 0.001000, mv_avg_loss: 343.419861, loss: 222.626099
17:01:04.048544 Step[4800/6995], lr: 0.001000, mv_avg_loss: 280.176971, loss: 254.492111
17:01:08.837921 Step[4900/6995], lr: 0.001000, mv_avg_loss: 299.829010, loss: 538.385132
17:01:13.636508 Step[5000/6995], lr: 0.001000, mv_avg_loss: 354.862854, loss: 330.859833
17:01:18.415131 Step[5100/6995], lr: 0.001000, mv_avg_loss: 333.552612, loss: 175.616425
17:01:23.214347 Step[5200/6995], lr: 0.001000, mv_avg_loss: 292.587280, loss: 175.543991
17:01:28.012786 Step[5300/6995], lr: 0.001000, mv_avg_loss: 263.868317, loss: 221.806564
17:01:32.795548 Step[5400/6995], lr: 0.001000, mv_avg_loss: 295.840302, loss: 160.987747
17:01:37.592589 Step[5500/6995], lr: 0.001000, mv_avg_loss: 253.839722, loss: 332.485168
17:01:42.395964 Step[5600/6995], lr: 0.001000, mv_avg_loss: 305.459137, loss: 222.352997
17:01:47.192088 Step[5700/6995], lr: 0.001000, mv_avg_loss: 272.073944, loss: 218.772186
17:01:51.977035 Step[5800/6995], lr: 0.001000, mv_avg_loss: 302.358521, loss: 295.377533
17:01:56.771929 Step[5900/6995], lr: 0.001000, mv_avg_loss: 267.688110, loss: 263.752869
17:02:01.566459 Step[6000/6995], lr: 0.001000, mv_avg_loss: 265.384552, loss: 486.180054
17:02:06.356049 Step[6100/6995], lr: 0.001000, mv_avg_loss: 316.077972, loss: 167.837158
17:02:11.153471 Step[6200/6995], lr: 0.001000, mv_avg_loss: 263.393280, loss: 175.452362
17:02:15.964643 Step[6300/6995], lr: 0.001000, mv_avg_loss: 299.728241, loss: 146.564423
17:02:20.776145 Step[6400/6995], lr: 0.001000, mv_avg_loss: 277.957489, loss: 540.505249
17:02:25.580825 Step[6500/6995], lr: 0.001000, mv_avg_loss: 253.786407, loss: 282.433197
17:02:30.381279 Step[6600/6995], lr: 0.001000, mv_avg_loss: 254.577881, loss: 120.491661
17:02:35.198744 Step[6700/6995], lr: 0.001000, mv_avg_loss: 262.537048, loss: 458.166260
17:02:40.015526 Step[6800/6995], lr: 0.001000, mv_avg_loss: 310.066864, loss: 583.314331
17:02:44.829989 Step[6900/6995], lr: 0.001000, mv_avg_loss: 322.544708, loss: 137.906250
Testing for epoch: 1
Average test PNSR is 22.780094 for 500 images
Start to train epoch 2
17:03:11.239747 Step[0/6995], lr: 0.001000, mv_avg_loss: 281.663574, loss: 168.699875
17:03:16.037373 Step[100/6995], lr: 0.001000, mv_avg_loss: 302.176056, loss: 236.238495
17:03:20.837890 Step[200/6995], lr: 0.001000, mv_avg_loss: 263.528229, loss: 430.135315
17:03:25.632711 Step[300/6995], lr: 0.001000, mv_avg_loss: 296.888214, loss: 339.209412
17:03:30.428630 Step[400/6995], lr: 0.001000, mv_avg_loss: 262.234131, loss: 208.159714
17:03:35.222035 Step[500/6995], lr: 0.001000, mv_avg_loss: 250.467697, loss: 148.701462
17:03:40.022674 Step[600/6995], lr: 0.001000, mv_avg_loss: 249.432999, loss: 298.576233
17:03:44.821214 Step[700/6995], lr: 0.001000, mv_avg_loss: 265.539398, loss: 197.940887
17:03:49.633453 Step[800/6995], lr: 0.001000, mv_avg_loss: 225.760147, loss: 153.489487
17:03:54.451938 Step[900/6995], lr: 0.001000, mv_avg_loss: 304.003845, loss: 581.779175
17:03:59.256771 Step[1000/6995], lr: 0.001000, mv_avg_loss: 249.947296, loss: 156.298477
17:04:04.078454 Step[1100/6995], lr: 0.001000, mv_avg_loss: 396.614136, loss: 447.196503
17:04:08.875238 Step[1200/6995], lr: 0.001000, mv_avg_loss: 257.540070, loss: 156.035950
17:04:13.660540 Step[1300/6995], lr: 0.001000, mv_avg_loss: 253.716385, loss: 232.669769
17:04:18.453404 Step[1400/6995], lr: 0.001000, mv_avg_loss: 280.603790, loss: 253.107147
17:04:23.252444 Step[1500/6995], lr: 0.001000, mv_avg_loss: 288.243927, loss: 368.764038
17:04:28.056565 Step[1600/6995], lr: 0.001000, mv_avg_loss: 231.807343, loss: 411.486786
17:04:32.861627 Step[1700/6995], lr: 0.001000, mv_avg_loss: 232.697296, loss: 178.163422
17:04:37.658653 Step[1800/6995], lr: 0.001000, mv_avg_loss: 211.019119, loss: 98.138229
17:04:42.457833 Step[1900/6995], lr: 0.001000, mv_avg_loss: 216.459900, loss: 164.872147
17:04:47.256219 Step[2000/6995], lr: 0.001000, mv_avg_loss: 277.847382, loss: 245.195404
17:04:52.052145 Step[2100/6995], lr: 0.001000, mv_avg_loss: 230.399414, loss: 216.008423
17:04:56.851377 Step[2200/6995], lr: 0.001000, mv_avg_loss: 229.525665, loss: 307.863220
17:05:01.653459 Step[2300/6995], lr: 0.001000, mv_avg_loss: 217.699142, loss: 391.547791
17:05:06.457351 Step[2400/6995], lr: 0.001000, mv_avg_loss: 225.288528, loss: 180.274048
17:05:11.254573 Step[2500/6995], lr: 0.001000, mv_avg_loss: 237.627319, loss: 386.019012
17:05:16.053519 Step[2600/6995], lr: 0.001000, mv_avg_loss: 215.101654, loss: 224.371811
17:05:20.852980 Step[2700/6995], lr: 0.001000, mv_avg_loss: 231.532013, loss: 166.245560
17:05:25.649432 Step[2800/6995], lr: 0.001000, mv_avg_loss: 224.308517, loss: 192.563843
17:05:30.456972 Step[2900/6995], lr: 0.001000, mv_avg_loss: 260.629181, loss: 257.368896
17:05:35.259303 Step[3000/6995], lr: 0.001000, mv_avg_loss: 222.179443, loss: 112.483734
17:05:40.058494 Step[3100/6995], lr: 0.001000, mv_avg_loss: 255.844955, loss: 379.251404
17:05:44.850247 Step[3200/6995], lr: 0.001000, mv_avg_loss: 247.136658, loss: 166.649277
17:05:49.645426 Step[3300/6995], lr: 0.001000, mv_avg_loss: 239.224747, loss: 146.447006
17:05:54.437177 Step[3400/6995], lr: 0.001000, mv_avg_loss: 224.563858, loss: 189.473862
17:05:59.230979 Step[3500/6995], lr: 0.001000, mv_avg_loss: 250.782028, loss: 126.980942
17:06:04.017759 Step[3600/6995], lr: 0.001000, mv_avg_loss: 245.143906, loss: 399.736023
17:06:08.817102 Step[3700/6995], lr: 0.001000, mv_avg_loss: 222.504333, loss: 140.262878
17:06:13.620392 Step[3800/6995], lr: 0.001000, mv_avg_loss: 211.616135, loss: 256.097290
17:06:18.432124 Step[3900/6995], lr: 0.001000, mv_avg_loss: 213.767960, loss: 166.711121
17:06:23.222931 Step[4000/6995], lr: 0.001000, mv_avg_loss: 221.025085, loss: 162.391754
17:06:28.009340 Step[4100/6995], lr: 0.001000, mv_avg_loss: 227.435226, loss: 119.497910
17:06:32.807653 Step[4200/6995], lr: 0.001000, mv_avg_loss: 220.283569, loss: 347.968689
17:06:37.603991 Step[4300/6995], lr: 0.001000, mv_avg_loss: 238.509155, loss: 185.641083
17:06:42.402871 Step[4400/6995], lr: 0.001000, mv_avg_loss: 234.451035, loss: 157.938843
17:06:47.196586 Step[4500/6995], lr: 0.001000, mv_avg_loss: 204.117325, loss: 318.413696
17:06:51.997748 Step[4600/6995], lr: 0.001000, mv_avg_loss: 191.984192, loss: 251.565552
17:06:56.792963 Step[4700/6995], lr: 0.001000, mv_avg_loss: 196.166107, loss: 231.376511
17:07:01.581878 Step[4800/6995], lr: 0.001000, mv_avg_loss: 236.904541, loss: 139.457169
17:07:06.381283 Step[4900/6995], lr: 0.001000, mv_avg_loss: 211.189804, loss: 99.336906
17:07:11.172006 Step[5000/6995], lr: 0.001000, mv_avg_loss: 226.300720, loss: 197.282639
17:07:15.963503 Step[5100/6995], lr: 0.001000, mv_avg_loss: 233.962708, loss: 152.149704
17:07:20.756343 Step[5200/6995], lr: 0.001000, mv_avg_loss: 218.640137, loss: 176.684296
17:07:25.558689 Step[5300/6995], lr: 0.001000, mv_avg_loss: 214.064743, loss: 150.112366
17:07:30.354924 Step[5400/6995], lr: 0.001000, mv_avg_loss: 193.681076, loss: 173.777954
17:07:35.159286 Step[5500/6995], lr: 0.001000, mv_avg_loss: 209.811676, loss: 290.106750
17:07:39.951293 Step[5600/6995], lr: 0.001000, mv_avg_loss: 224.651031, loss: 73.878647
17:07:44.727756 Step[5700/6995], lr: 0.001000, mv_avg_loss: 237.643860, loss: 141.117584
17:07:49.518549 Step[5800/6995], lr: 0.001000, mv_avg_loss: 205.437241, loss: 226.608002
17:07:54.293134 Step[5900/6995], lr: 0.001000, mv_avg_loss: 202.708908, loss: 229.557373
17:07:59.076056 Step[6000/6995], lr: 0.001000, mv_avg_loss: 179.246109, loss: 130.306747
17:08:03.892514 Step[6100/6995], lr: 0.001000, mv_avg_loss: 186.701309, loss: 162.188202
17:08:08.677856 Step[6200/6995], lr: 0.001000, mv_avg_loss: 190.783569, loss: 163.688034
17:08:13.468207 Step[6300/6995], lr: 0.001000, mv_avg_loss: 189.466751, loss: 117.063499
17:08:18.256822 Step[6400/6995], lr: 0.001000, mv_avg_loss: 186.231186, loss: 157.561371
17:08:23.040716 Step[6500/6995], lr: 0.001000, mv_avg_loss: 255.423798, loss: 501.028778
17:08:27.828220 Step[6600/6995], lr: 0.001000, mv_avg_loss: 216.580780, loss: 160.948944
17:08:32.612095 Step[6700/6995], lr: 0.001000, mv_avg_loss: 182.862518, loss: 188.005295
17:08:37.399142 Step[6800/6995], lr: 0.001000, mv_avg_loss: 189.939835, loss: 156.568909
17:08:42.186871 Step[6900/6995], lr: 0.001000, mv_avg_loss: 185.255173, loss: 141.943878
Testing for epoch: 2
Average test PNSR is 24.406830 for 500 images
Start to train epoch 3
17:09:08.329512 Step[0/6995], lr: 0.001000, mv_avg_loss: 176.761749, loss: 270.289185
17:09:13.094401 Step[100/6995], lr: 0.001000, mv_avg_loss: 175.970795, loss: 317.554535
17:09:17.865487 Step[200/6995], lr: 0.001000, mv_avg_loss: 182.729904, loss: 146.933197
17:09:22.632995 Step[300/6995], lr: 0.001000, mv_avg_loss: 186.037476, loss: 248.598801
17:09:27.391349 Step[400/6995], lr: 0.001000, mv_avg_loss: 190.343857, loss: 215.603546
17:09:32.155099 Step[500/6995], lr: 0.001000, mv_avg_loss: 160.621979, loss: 105.186813
17:09:36.910136 Step[600/6995], lr: 0.001000, mv_avg_loss: 161.338837, loss: 173.327637
17:09:41.667093 Step[700/6995], lr: 0.001000, mv_avg_loss: 186.523514, loss: 144.778519
17:09:46.423904 Step[800/6995], lr: 0.001000, mv_avg_loss: 176.405579, loss: 267.900208
17:09:51.192614 Step[900/6995], lr: 0.001000, mv_avg_loss: 206.470352, loss: 313.004639
17:09:55.959061 Step[1000/6995], lr: 0.001000, mv_avg_loss: 170.495819, loss: 181.149231
17:10:00.739479 Step[1100/6995], lr: 0.001000, mv_avg_loss: 173.155899, loss: 255.034241
17:10:05.508098 Step[1200/6995], lr: 0.001000, mv_avg_loss: 172.897659, loss: 184.012070
17:10:10.278836 Step[1300/6995], lr: 0.001000, mv_avg_loss: 212.639832, loss: 176.754486
17:10:15.044434 Step[1400/6995], lr: 0.001000, mv_avg_loss: 162.543976, loss: 156.228714
17:10:19.808279 Step[1500/6995], lr: 0.001000, mv_avg_loss: 163.870880, loss: 146.754059
17:10:24.573332 Step[1600/6995], lr: 0.001000, mv_avg_loss: 241.670639, loss: 217.173386
17:10:29.337610 Step[1700/6995], lr: 0.001000, mv_avg_loss: 197.292343, loss: 215.139084
17:10:34.126489 Step[1800/6995], lr: 0.001000, mv_avg_loss: 183.015091, loss: 109.285545
17:10:38.889924 Step[1900/6995], lr: 0.001000, mv_avg_loss: 196.353607, loss: 293.206390
17:10:43.660701 Step[2000/6995], lr: 0.001000, mv_avg_loss: 167.496185, loss: 201.594589
17:10:48.432992 Step[2100/6995], lr: 0.001000, mv_avg_loss: 173.557922, loss: 194.662445
17:10:53.206409 Step[2200/6995], lr: 0.001000, mv_avg_loss: 154.052246, loss: 167.317291
17:10:57.979941 Step[2300/6995], lr: 0.001000, mv_avg_loss: 175.216949, loss: 92.611778
17:11:02.750433 Step[2400/6995], lr: 0.001000, mv_avg_loss: 193.606644, loss: 78.220619
17:11:07.526629 Step[2500/6995], lr: 0.001000, mv_avg_loss: 180.428162, loss: 308.276123
17:11:12.301102 Step[2600/6995], lr: 0.001000, mv_avg_loss: 174.715546, loss: 367.850433
17:11:17.073337 Step[2700/6995], lr: 0.001000, mv_avg_loss: 173.439056, loss: 120.062767
17:11:21.846903 Step[2800/6995], lr: 0.001000, mv_avg_loss: 172.136230, loss: 138.480347
17:11:26.614259 Step[2900/6995], lr: 0.001000, mv_avg_loss: 158.398560, loss: 163.657562
17:11:31.379388 Step[3000/6995], lr: 0.001000, mv_avg_loss: 177.400543, loss: 163.575760
17:11:36.149761 Step[3100/6995], lr: 0.001000, mv_avg_loss: 188.365448, loss: 182.935516
17:11:40.930337 Step[3200/6995], lr: 0.001000, mv_avg_loss: 144.970398, loss: 127.414001
17:11:45.706583 Step[3300/6995], lr: 0.001000, mv_avg_loss: 140.335159, loss: 180.508286
17:11:50.482832 Step[3400/6995], lr: 0.001000, mv_avg_loss: 159.922699, loss: 149.532486
17:11:55.250812 Step[3500/6995], lr: 0.001000, mv_avg_loss: 158.452362, loss: 194.905884
17:12:00.023134 Step[3600/6995], lr: 0.001000, mv_avg_loss: 157.982864, loss: 160.335846
17:12:04.795892 Step[3700/6995], lr: 0.001000, mv_avg_loss: 160.128555, loss: 136.117584
17:12:09.565661 Step[3800/6995], lr: 0.001000, mv_avg_loss: 183.463959, loss: 313.293549
17:12:14.340536 Step[3900/6995], lr: 0.001000, mv_avg_loss: 151.749146, loss: 250.925629
17:12:19.106661 Step[4000/6995], lr: 0.001000, mv_avg_loss: 158.247345, loss: 243.111252
17:12:23.904836 Step[4100/6995], lr: 0.001000, mv_avg_loss: 160.025146, loss: 172.411240
17:12:28.664786 Step[4200/6995], lr: 0.001000, mv_avg_loss: 162.067551, loss: 220.427856
17:12:33.437166 Step[4300/6995], lr: 0.001000, mv_avg_loss: 170.858688, loss: 108.579422
17:12:38.199549 Step[4400/6995], lr: 0.001000, mv_avg_loss: 154.793213, loss: 109.198654
17:12:42.975723 Step[4500/6995], lr: 0.001000, mv_avg_loss: 158.127914, loss: 168.273010
17:12:47.731562 Step[4600/6995], lr: 0.001000, mv_avg_loss: 153.217438, loss: 147.463043
17:12:52.483372 Step[4700/6995], lr: 0.001000, mv_avg_loss: 171.727051, loss: 125.636948
17:12:57.255471 Step[4800/6995], lr: 0.001000, mv_avg_loss: 163.525009, loss: 303.866302
17:13:02.012654 Step[4900/6995], lr: 0.001000, mv_avg_loss: 156.028503, loss: 174.709152
17:13:06.753653 Step[5000/6995], lr: 0.001000, mv_avg_loss: 160.746323, loss: 194.165939
17:13:11.503681 Step[5100/6995], lr: 0.001000, mv_avg_loss: 146.947754, loss: 222.197067
17:13:16.247077 Step[5200/6995], lr: 0.001000, mv_avg_loss: 152.694931, loss: 125.592911
17:13:20.984909 Step[5300/6995], lr: 0.001000, mv_avg_loss: 152.767578, loss: 113.004181
17:13:25.733603 Step[5400/6995], lr: 0.001000, mv_avg_loss: 143.718155, loss: 100.157310
17:13:30.507276 Step[5500/6995], lr: 0.001000, mv_avg_loss: 170.820251, loss: 375.996399
17:13:35.257438 Step[5600/6995], lr: 0.001000, mv_avg_loss: 156.895340, loss: 282.840759
17:13:40.014745 Step[5700/6995], lr: 0.001000, mv_avg_loss: 144.729660, loss: 117.309616
17:13:44.761955 Step[5800/6995], lr: 0.001000, mv_avg_loss: 180.032166, loss: 234.284302
17:13:49.525989 Step[5900/6995], lr: 0.001000, mv_avg_loss: 241.019958, loss: 245.031433
17:13:54.277852 Step[6000/6995], lr: 0.001000, mv_avg_loss: 163.793350, loss: 152.324890
17:13:59.051193 Step[6100/6995], lr: 0.001000, mv_avg_loss: 145.212357, loss: 147.846512
17:14:03.798040 Step[6200/6995], lr: 0.001000, mv_avg_loss: 143.752716, loss: 166.716034
17:14:08.540310 Step[6300/6995], lr: 0.001000, mv_avg_loss: 153.297501, loss: 125.949020
17:14:13.285964 Step[6400/6995], lr: 0.001000, mv_avg_loss: 141.586868, loss: 150.505310
17:14:18.058586 Step[6500/6995], lr: 0.001000, mv_avg_loss: 151.883682, loss: 284.979401
17:14:22.799224 Step[6600/6995], lr: 0.001000, mv_avg_loss: 139.131180, loss: 114.715378
17:14:27.557800 Step[6700/6995], lr: 0.001000, mv_avg_loss: 143.347168, loss: 111.183090
17:14:32.305805 Step[6800/6995], lr: 0.001000, mv_avg_loss: 131.522324, loss: 184.770721
17:14:37.055491 Step[6900/6995], lr: 0.001000, mv_avg_loss: 138.249146, loss: 80.899734
Testing for epoch: 3
Average test PNSR is 24.716574 for 500 images
Start to train epoch 4
17:15:03.606255 Step[0/6995], lr: 0.001000, mv_avg_loss: 147.179932, loss: 164.978363
17:15:08.353887 Step[100/6995], lr: 0.001000, mv_avg_loss: 194.749191, loss: 122.171005
17:15:13.107464 Step[200/6995], lr: 0.001000, mv_avg_loss: 139.489853, loss: 84.221313
17:15:17.851195 Step[300/6995], lr: 0.001000, mv_avg_loss: 163.013733, loss: 302.507538
17:15:22.596006 Step[400/6995], lr: 0.001000, mv_avg_loss: 128.554306, loss: 121.490623
17:15:27.352143 Step[500/6995], lr: 0.001000, mv_avg_loss: 153.381531, loss: 148.692856
17:15:32.085807 Step[600/6995], lr: 0.001000, mv_avg_loss: 144.135208, loss: 67.476334
17:15:36.824135 Step[700/6995], lr: 0.001000, mv_avg_loss: 142.936127, loss: 91.761459
17:15:41.580411 Step[800/6995], lr: 0.001000, mv_avg_loss: 149.341736, loss: 94.046227
17:15:46.343482 Step[900/6995], lr: 0.001000, mv_avg_loss: 154.759247, loss: 225.435883
17:15:51.088468 Step[1000/6995], lr: 0.001000, mv_avg_loss: 132.156418, loss: 91.754440
17:15:55.838098 Step[1100/6995], lr: 0.001000, mv_avg_loss: 155.621643, loss: 123.196579
17:16:00.603261 Step[1200/6995], lr: 0.001000, mv_avg_loss: 145.235626, loss: 103.160934
17:16:05.376602 Step[1300/6995], lr: 0.001000, mv_avg_loss: 139.821320, loss: 99.073059
17:16:10.125953 Step[1400/6995], lr: 0.001000, mv_avg_loss: 124.334839, loss: 111.725304
17:16:14.892623 Step[1500/6995], lr: 0.001000, mv_avg_loss: 114.992165, loss: 115.486938
17:16:19.651106 Step[1600/6995], lr: 0.001000, mv_avg_loss: 127.040550, loss: 132.971664
17:16:24.404094 Step[1700/6995], lr: 0.001000, mv_avg_loss: 122.350868, loss: 98.109573
17:16:29.159936 Step[1800/6995], lr: 0.001000, mv_avg_loss: 136.382355, loss: 127.986938
17:16:33.937331 Step[1900/6995], lr: 0.001000, mv_avg_loss: 119.785057, loss: 138.257874
17:16:38.696922 Step[2000/6995], lr: 0.001000, mv_avg_loss: 118.818565, loss: 73.287872
17:16:43.500669 Step[2100/6995], lr: 0.001000, mv_avg_loss: 132.318878, loss: 205.664291
17:16:48.268443 Step[2200/6995], lr: 0.001000, mv_avg_loss: 144.326584, loss: 225.893661
17:16:53.044785 Step[2300/6995], lr: 0.001000, mv_avg_loss: 131.010223, loss: 100.007690
17:16:57.814594 Step[2400/6995], lr: 0.001000, mv_avg_loss: 167.183319, loss: 180.796387
17:17:02.588825 Step[2500/6995], lr: 0.001000, mv_avg_loss: 139.220215, loss: 154.992828
17:17:07.360547 Step[2600/6995], lr: 0.001000, mv_avg_loss: 105.657860, loss: 89.680145
17:17:12.142820 Step[2700/6995], lr: 0.001000, mv_avg_loss: 162.730270, loss: 143.090240
17:17:16.922763 Step[2800/6995], lr: 0.001000, mv_avg_loss: 133.907379, loss: 181.361938
17:17:21.697080 Step[2900/6995], lr: 0.001000, mv_avg_loss: 121.410713, loss: 72.073074
17:17:26.485312 Step[3000/6995], lr: 0.001000, mv_avg_loss: 151.590775, loss: 99.799820
17:17:31.264290 Step[3100/6995], lr: 0.001000, mv_avg_loss: 142.619858, loss: 109.835434
17:17:36.038886 Step[3200/6995], lr: 0.001000, mv_avg_loss: 125.988945, loss: 85.078979
17:17:40.808302 Step[3300/6995], lr: 0.001000, mv_avg_loss: 125.429291, loss: 132.058823
17:17:45.572473 Step[3400/6995], lr: 0.001000, mv_avg_loss: 115.980026, loss: 80.655083
17:17:50.356451 Step[3500/6995], lr: 0.001000, mv_avg_loss: 132.401306, loss: 117.864426
17:17:55.140688 Step[3600/6995], lr: 0.001000, mv_avg_loss: 128.264221, loss: 89.106903
17:17:59.916596 Step[3700/6995], lr: 0.001000, mv_avg_loss: 108.625328, loss: 77.612259
17:18:04.702667 Step[3800/6995], lr: 0.001000, mv_avg_loss: 110.282547, loss: 161.600510
17:18:09.469533 Step[3900/6995], lr: 0.001000, mv_avg_loss: 136.926712, loss: 155.757736
17:18:14.272545 Step[4000/6995], lr: 0.001000, mv_avg_loss: 129.625656, loss: 108.275864
17:18:19.047427 Step[4100/6995], lr: 0.001000, mv_avg_loss: 133.030762, loss: 132.667297
17:18:23.841767 Step[4200/6995], lr: 0.001000, mv_avg_loss: 125.315079, loss: 156.531616
17:18:28.623277 Step[4300/6995], lr: 0.001000, mv_avg_loss: 126.845482, loss: 92.551453
17:18:33.414919 Step[4400/6995], lr: 0.001000, mv_avg_loss: 137.438538, loss: 126.026642
17:18:38.210531 Step[4500/6995], lr: 0.001000, mv_avg_loss: 198.589417, loss: 258.196930
17:18:42.993698 Step[4600/6995], lr: 0.001000, mv_avg_loss: 133.954391, loss: 89.844101
17:18:47.770919 Step[4700/6995], lr: 0.001000, mv_avg_loss: 132.857101, loss: 136.714233
17:18:52.546069 Step[4800/6995], lr: 0.001000, mv_avg_loss: 147.578842, loss: 251.286072
17:18:57.324953 Step[4900/6995], lr: 0.001000, mv_avg_loss: 126.402466, loss: 140.884125
17:19:02.110708 Step[5000/6995], lr: 0.001000, mv_avg_loss: 140.357712, loss: 137.902573
17:19:06.885730 Step[5100/6995], lr: 0.001000, mv_avg_loss: 123.043076, loss: 102.839378
17:19:11.665976 Step[5200/6995], lr: 0.001000, mv_avg_loss: 115.016983, loss: 72.488510
17:19:16.444721 Step[5300/6995], lr: 0.001000, mv_avg_loss: 119.958321, loss: 131.553757
17:19:21.206310 Step[5400/6995], lr: 0.001000, mv_avg_loss: 118.874313, loss: 89.910851
17:19:25.983068 Step[5500/6995], lr: 0.001000, mv_avg_loss: 125.977608, loss: 79.446884
17:19:30.767456 Step[5600/6995], lr: 0.001000, mv_avg_loss: 107.124619, loss: 121.810104
17:19:35.542158 Step[5700/6995], lr: 0.001000, mv_avg_loss: 126.845795, loss: 62.366425
17:19:40.307169 Step[5800/6995], lr: 0.001000, mv_avg_loss: 118.120201, loss: 82.128502
17:19:45.097151 Step[5900/6995], lr: 0.001000, mv_avg_loss: 125.670174, loss: 97.009689
17:19:49.874781 Step[6000/6995], lr: 0.001000, mv_avg_loss: 116.422066, loss: 246.481171
17:19:54.647347 Step[6100/6995], lr: 0.001000, mv_avg_loss: 129.394287, loss: 331.613983
17:19:59.420546 Step[6200/6995], lr: 0.001000, mv_avg_loss: 125.499603, loss: 97.780624
17:20:04.188189 Step[6300/6995], lr: 0.001000, mv_avg_loss: 126.099266, loss: 85.071976
17:20:08.958042 Step[6400/6995], lr: 0.001000, mv_avg_loss: 119.886169, loss: 110.403458
17:20:13.725046 Step[6500/6995], lr: 0.001000, mv_avg_loss: 108.743301, loss: 118.931244
17:20:18.499927 Step[6600/6995], lr: 0.001000, mv_avg_loss: 125.974236, loss: 217.123337
17:20:23.258847 Step[6700/6995], lr: 0.001000, mv_avg_loss: 113.036804, loss: 76.385544
17:20:28.038910 Step[6800/6995], lr: 0.001000, mv_avg_loss: 107.584991, loss: 136.915146
17:20:32.803255 Step[6900/6995], lr: 0.001000, mv_avg_loss: 134.653717, loss: 116.557358
saving model for epoch 4
Testing for epoch: 4
Average test PNSR is 24.245236 for 500 images
Start to train epoch 5
17:20:58.545592 Step[0/6995], lr: 0.001000, mv_avg_loss: 132.501068, loss: 126.555870
17:21:03.311899 Step[100/6995], lr: 0.001000, mv_avg_loss: 106.423256, loss: 124.428032
17:21:08.072247 Step[200/6995], lr: 0.001000, mv_avg_loss: 109.371269, loss: 145.613174
17:21:12.838265 Step[300/6995], lr: 0.001000, mv_avg_loss: 108.642517, loss: 118.443359
17:21:17.592764 Step[400/6995], lr: 0.001000, mv_avg_loss: 110.663719, loss: 100.705841
17:21:22.349703 Step[500/6995], lr: 0.001000, mv_avg_loss: 106.720795, loss: 92.561523
17:21:27.105444 Step[600/6995], lr: 0.001000, mv_avg_loss: 118.257866, loss: 108.613525
17:21:31.865841 Step[700/6995], lr: 0.001000, mv_avg_loss: 115.237953, loss: 75.652893
17:21:36.618852 Step[800/6995], lr: 0.001000, mv_avg_loss: 119.731293, loss: 88.422150
17:21:41.377993 Step[900/6995], lr: 0.001000, mv_avg_loss: 108.336823, loss: 160.976074
17:21:46.124496 Step[1000/6995], lr: 0.001000, mv_avg_loss: 95.901627, loss: 113.989098
17:21:50.873241 Step[1100/6995], lr: 0.001000, mv_avg_loss: 122.346481, loss: 250.782578
17:21:55.611500 Step[1200/6995], lr: 0.001000, mv_avg_loss: 119.920418, loss: 85.969017
17:22:00.371509 Step[1300/6995], lr: 0.001000, mv_avg_loss: 122.713760, loss: 68.797348
17:22:05.115565 Step[1400/6995], lr: 0.001000, mv_avg_loss: 134.140671, loss: 81.697983
17:22:09.860830 Step[1500/6995], lr: 0.001000, mv_avg_loss: 116.723793, loss: 154.639008
17:22:14.607991 Step[1600/6995], lr: 0.001000, mv_avg_loss: 129.373535, loss: 60.638317
17:22:19.358616 Step[1700/6995], lr: 0.001000, mv_avg_loss: 130.870743, loss: 71.302094
17:22:24.103089 Step[1800/6995], lr: 0.001000, mv_avg_loss: 127.193077, loss: 95.721619
17:22:28.838350 Step[1900/6995], lr: 0.001000, mv_avg_loss: 111.448425, loss: 93.831406
17:22:33.583873 Step[2000/6995], lr: 0.001000, mv_avg_loss: 107.742653, loss: 111.160484
17:22:38.337580 Step[2100/6995], lr: 0.001000, mv_avg_loss: 135.387054, loss: 117.645134
17:22:43.095484 Step[2200/6995], lr: 0.001000, mv_avg_loss: 111.301407, loss: 74.240646
17:22:47.850032 Step[2300/6995], lr: 0.001000, mv_avg_loss: 113.564949, loss: 76.049652
17:22:52.606123 Step[2400/6995], lr: 0.001000, mv_avg_loss: 108.150391, loss: 177.680222
17:22:57.344672 Step[2500/6995], lr: 0.001000, mv_avg_loss: 108.197334, loss: 163.627747
17:23:02.096014 Step[2600/6995], lr: 0.001000, mv_avg_loss: 122.549110, loss: 79.574409
17:23:06.847121 Step[2700/6995], lr: 0.001000, mv_avg_loss: 128.363235, loss: 117.372070
17:23:11.599504 Step[2800/6995], lr: 0.001000, mv_avg_loss: 120.974403, loss: 184.376877
17:23:16.343553 Step[2900/6995], lr: 0.001000, mv_avg_loss: 114.049004, loss: 169.233551
17:23:21.080037 Step[3000/6995], lr: 0.001000, mv_avg_loss: 100.581726, loss: 100.176048
17:23:25.820715 Step[3100/6995], lr: 0.001000, mv_avg_loss: 101.596573, loss: 99.882675
17:23:30.566157 Step[3200/6995], lr: 0.001000, mv_avg_loss: 105.192627, loss: 142.923752
17:23:35.339846 Step[3300/6995], lr: 0.001000, mv_avg_loss: 119.203514, loss: 157.598755
17:23:40.107860 Step[3400/6995], lr: 0.001000, mv_avg_loss: 108.168419, loss: 116.804321
17:23:44.844526 Step[3500/6995], lr: 0.001000, mv_avg_loss: 123.331566, loss: 125.056686
17:23:49.593786 Step[3600/6995], lr: 0.001000, mv_avg_loss: 108.986092, loss: 101.773277
17:23:54.354308 Step[3700/6995], lr: 0.001000, mv_avg_loss: 107.428017, loss: 82.495079
17:23:59.095705 Step[3800/6995], lr: 0.001000, mv_avg_loss: 105.562233, loss: 117.450302
17:24:03.848970 Step[3900/6995], lr: 0.001000, mv_avg_loss: 92.232750, loss: 106.592407
17:24:08.600029 Step[4000/6995], lr: 0.001000, mv_avg_loss: 91.172409, loss: 114.097496
17:24:13.343099 Step[4100/6995], lr: 0.001000, mv_avg_loss: 99.076042, loss: 95.563133
17:24:18.088176 Step[4200/6995], lr: 0.001000, mv_avg_loss: 111.144638, loss: 126.455040
17:24:22.821757 Step[4300/6995], lr: 0.001000, mv_avg_loss: 102.197182, loss: 85.330711
17:24:27.555938 Step[4400/6995], lr: 0.001000, mv_avg_loss: 110.669624, loss: 109.982811
17:24:32.302028 Step[4500/6995], lr: 0.001000, mv_avg_loss: 104.393974, loss: 95.552940
17:24:37.040427 Step[4600/6995], lr: 0.001000, mv_avg_loss: 104.230713, loss: 119.787376
17:24:41.777345 Step[4700/6995], lr: 0.001000, mv_avg_loss: 117.959831, loss: 91.477013
17:24:46.516911 Step[4800/6995], lr: 0.001000, mv_avg_loss: 104.049644, loss: 93.670578
17:24:51.253003 Step[4900/6995], lr: 0.001000, mv_avg_loss: 115.183441, loss: 68.167809
17:24:55.985507 Step[5000/6995], lr: 0.001000, mv_avg_loss: 107.118866, loss: 83.797623
17:25:00.766871 Step[5100/6995], lr: 0.001000, mv_avg_loss: 117.388901, loss: 173.664032
17:25:05.514328 Step[5200/6995], lr: 0.001000, mv_avg_loss: 117.022041, loss: 80.322586
17:25:10.269781 Step[5300/6995], lr: 0.001000, mv_avg_loss: 111.802895, loss: 97.268974
17:25:15.005330 Step[5400/6995], lr: 0.001000, mv_avg_loss: 117.366325, loss: 71.312363
17:25:19.751158 Step[5500/6995], lr: 0.001000, mv_avg_loss: 99.425537, loss: 80.010780
17:25:24.496223 Step[5600/6995], lr: 0.001000, mv_avg_loss: 114.223091, loss: 107.617661
17:25:29.236354 Step[5700/6995], lr: 0.001000, mv_avg_loss: 113.627312, loss: 125.026459
17:25:33.979495 Step[5800/6995], lr: 0.001000, mv_avg_loss: 118.167915, loss: 84.158234
17:25:38.722002 Step[5900/6995], lr: 0.001000, mv_avg_loss: 99.391968, loss: 131.112946
17:25:43.468561 Step[6000/6995], lr: 0.001000, mv_avg_loss: 105.682487, loss: 84.610863
17:25:48.230091 Step[6100/6995], lr: 0.001000, mv_avg_loss: 87.943497, loss: 148.355087
17:25:52.976902 Step[6200/6995], lr: 0.001000, mv_avg_loss: 112.265915, loss: 315.276031
17:25:57.725777 Step[6300/6995], lr: 0.001000, mv_avg_loss: 105.657906, loss: 71.911499
17:26:02.469334 Step[6400/6995], lr: 0.001000, mv_avg_loss: 102.842186, loss: 68.921158
17:26:07.219707 Step[6500/6995], lr: 0.001000, mv_avg_loss: 89.819321, loss: 54.805481
17:26:11.960575 Step[6600/6995], lr: 0.001000, mv_avg_loss: 85.839386, loss: 104.012802
17:26:16.707582 Step[6700/6995], lr: 0.001000, mv_avg_loss: 119.936676, loss: 97.015976
17:26:21.455234 Step[6800/6995], lr: 0.001000, mv_avg_loss: 112.900108, loss: 193.287201
17:26:26.189442 Step[6900/6995], lr: 0.001000, mv_avg_loss: 108.253120, loss: 79.293205
Testing for epoch: 5
Average test PNSR is 24.067987 for 500 images
Start to train epoch 6
17:26:51.734829 Step[0/6995], lr: 0.001000, mv_avg_loss: 112.691872, loss: 134.308334
17:26:56.457825 Step[100/6995], lr: 0.001000, mv_avg_loss: 100.056831, loss: 147.211670
17:27:01.211781 Step[200/6995], lr: 0.001000, mv_avg_loss: 102.211357, loss: 52.528053
17:27:05.943307 Step[300/6995], lr: 0.001000, mv_avg_loss: 97.380203, loss: 139.881699
17:27:10.774356 Step[400/6995], lr: 0.001000, mv_avg_loss: 89.412285, loss: 60.710983
17:27:15.544165 Step[500/6995], lr: 0.001000, mv_avg_loss: 97.511597, loss: 88.938766
17:27:20.292339 Step[600/6995], lr: 0.001000, mv_avg_loss: 96.179810, loss: 138.090393
17:27:25.048154 Step[700/6995], lr: 0.001000, mv_avg_loss: 101.040443, loss: 70.459518
17:27:29.800077 Step[800/6995], lr: 0.001000, mv_avg_loss: 88.574043, loss: 49.281902
17:27:34.554021 Step[900/6995], lr: 0.001000, mv_avg_loss: 88.600533, loss: 102.608582
17:27:39.312216 Step[1000/6995], lr: 0.001000, mv_avg_loss: 86.036484, loss: 54.711945
17:27:44.065959 Step[1100/6995], lr: 0.001000, mv_avg_loss: 112.945869, loss: 79.460175
17:27:48.836430 Step[1200/6995], lr: 0.001000, mv_avg_loss: 91.904282, loss: 58.877846
17:27:53.600657 Step[1300/6995], lr: 0.001000, mv_avg_loss: 93.053696, loss: 76.470642
17:27:58.366642 Step[1400/6995], lr: 0.001000, mv_avg_loss: 100.407509, loss: 51.353821
17:28:03.119070 Step[1500/6995], lr: 0.001000, mv_avg_loss: 100.944862, loss: 81.374382
17:28:07.902132 Step[1600/6995], lr: 0.001000, mv_avg_loss: 107.643974, loss: 143.588608
17:28:12.639408 Step[1700/6995], lr: 0.001000, mv_avg_loss: 84.875252, loss: 106.248863
17:28:17.400791 Step[1800/6995], lr: 0.001000, mv_avg_loss: 98.374413, loss: 104.310326
17:28:22.166976 Step[1900/6995], lr: 0.001000, mv_avg_loss: 100.440773, loss: 97.758415
17:28:26.903261 Step[2000/6995], lr: 0.001000, mv_avg_loss: 93.245811, loss: 79.685753
17:28:31.649068 Step[2100/6995], lr: 0.001000, mv_avg_loss: 107.465782, loss: 82.327568
17:28:36.411117 Step[2200/6995], lr: 0.001000, mv_avg_loss: 125.043053, loss: 120.903801
17:28:41.157137 Step[2300/6995], lr: 0.001000, mv_avg_loss: 88.266541, loss: 102.226952
17:28:45.899049 Step[2400/6995], lr: 0.001000, mv_avg_loss: 97.299667, loss: 98.151581
17:28:50.641411 Step[2500/6995], lr: 0.001000, mv_avg_loss: 90.166229, loss: 117.533035
17:28:55.381567 Step[2600/6995], lr: 0.001000, mv_avg_loss: 84.683502, loss: 84.039551
17:29:00.135662 Step[2700/6995], lr: 0.001000, mv_avg_loss: 94.708862, loss: 146.451599
17:29:04.889691 Step[2800/6995], lr: 0.001000, mv_avg_loss: 97.485977, loss: 145.625885
17:29:09.638470 Step[2900/6995], lr: 0.001000, mv_avg_loss: 91.392120, loss: 54.479214
17:29:14.404822 Step[3000/6995], lr: 0.001000, mv_avg_loss: 106.476768, loss: 55.200562
17:29:19.169797 Step[3100/6995], lr: 0.001000, mv_avg_loss: 118.206230, loss: 96.274277
17:29:23.910558 Step[3200/6995], lr: 0.001000, mv_avg_loss: 103.899704, loss: 138.921738
17:29:28.658226 Step[3300/6995], lr: 0.001000, mv_avg_loss: 84.146622, loss: 53.469971
17:29:33.407147 Step[3400/6995], lr: 0.001000, mv_avg_loss: 94.542465, loss: 86.177856
17:29:38.167788 Step[3500/6995], lr: 0.001000, mv_avg_loss: 93.049789, loss: 66.228973
17:29:42.924360 Step[3600/6995], lr: 0.001000, mv_avg_loss: 80.749130, loss: 85.752426
17:29:47.665603 Step[3700/6995], lr: 0.001000, mv_avg_loss: 92.669907, loss: 68.721489
17:29:52.418761 Step[3800/6995], lr: 0.001000, mv_avg_loss: 116.581451, loss: 67.298714
17:29:57.190068 Step[3900/6995], lr: 0.001000, mv_avg_loss: 110.441002, loss: 99.111145
17:30:01.947739 Step[4000/6995], lr: 0.001000, mv_avg_loss: 98.727081, loss: 85.244957
17:30:06.702549 Step[4100/6995], lr: 0.001000, mv_avg_loss: 98.281082, loss: 65.763741
17:30:11.475995 Step[4200/6995], lr: 0.001000, mv_avg_loss: 94.898972, loss: 113.354271
17:30:16.257955 Step[4300/6995], lr: 0.001000, mv_avg_loss: 98.901474, loss: 133.007126
17:30:21.036379 Step[4400/6995], lr: 0.001000, mv_avg_loss: 90.945168, loss: 206.234497
17:30:25.797538 Step[4500/6995], lr: 0.001000, mv_avg_loss: 89.142822, loss: 72.667007
17:30:30.577072 Step[4600/6995], lr: 0.001000, mv_avg_loss: 95.061844, loss: 82.209961
17:30:35.354627 Step[4700/6995], lr: 0.001000, mv_avg_loss: 96.258133, loss: 73.854630
17:30:40.116015 Step[4800/6995], lr: 0.001000, mv_avg_loss: 89.152695, loss: 183.527710
17:30:44.875857 Step[4900/6995], lr: 0.001000, mv_avg_loss: 97.554169, loss: 52.860847
17:30:49.640591 Step[5000/6995], lr: 0.001000, mv_avg_loss: 90.908096, loss: 86.366013
17:30:54.405925 Step[5100/6995], lr: 0.001000, mv_avg_loss: 98.715759, loss: 103.872269
17:30:59.171065 Step[5200/6995], lr: 0.001000, mv_avg_loss: 90.180000, loss: 56.534222
17:31:03.946960 Step[5300/6995], lr: 0.001000, mv_avg_loss: 91.006187, loss: 63.668884
17:31:08.734077 Step[5400/6995], lr: 0.001000, mv_avg_loss: 89.630402, loss: 97.758690
17:31:13.503022 Step[5500/6995], lr: 0.001000, mv_avg_loss: 103.161591, loss: 116.063461
17:31:18.280804 Step[5600/6995], lr: 0.001000, mv_avg_loss: 101.391953, loss: 46.428082
17:31:23.061281 Step[5700/6995], lr: 0.001000, mv_avg_loss: 90.508690, loss: 66.024780
17:31:27.830638 Step[5800/6995], lr: 0.001000, mv_avg_loss: 92.333885, loss: 75.086761
17:31:32.608756 Step[5900/6995], lr: 0.001000, mv_avg_loss: 98.085037, loss: 73.188721
17:31:37.375113 Step[6000/6995], lr: 0.001000, mv_avg_loss: 85.421661, loss: 83.448715
17:31:42.195576 Step[6100/6995], lr: 0.001000, mv_avg_loss: 81.221855, loss: 85.176147
17:31:46.997440 Step[6200/6995], lr: 0.001000, mv_avg_loss: 94.123940, loss: 132.031754
17:31:51.777508 Step[6300/6995], lr: 0.001000, mv_avg_loss: 93.515800, loss: 198.511337
17:31:56.555644 Step[6400/6995], lr: 0.001000, mv_avg_loss: 98.602829, loss: 89.104637
17:32:01.320639 Step[6500/6995], lr: 0.001000, mv_avg_loss: 93.388435, loss: 82.394440
17:32:06.084399 Step[6600/6995], lr: 0.001000, mv_avg_loss: 88.157555, loss: 128.224854
17:32:10.860008 Step[6700/6995], lr: 0.001000, mv_avg_loss: 84.794044, loss: 93.338562
17:32:15.644350 Step[6800/6995], lr: 0.001000, mv_avg_loss: 89.880913, loss: 85.222534
17:32:20.421842 Step[6900/6995], lr: 0.001000, mv_avg_loss: 91.688332, loss: 99.699089
Testing for epoch: 6
Average test PNSR is 24.822686 for 500 images
Start to train epoch 7
17:32:46.735102 Step[0/6995], lr: 0.001000, mv_avg_loss: 80.747391, loss: 54.643234
17:32:51.497975 Step[100/6995], lr: 0.001000, mv_avg_loss: 93.303886, loss: 110.513916
17:32:56.268368 Step[200/6995], lr: 0.001000, mv_avg_loss: 88.600700, loss: 58.178417
17:33:01.068070 Step[300/6995], lr: 0.001000, mv_avg_loss: 81.987320, loss: 82.810440
17:33:05.871621 Step[400/6995], lr: 0.001000, mv_avg_loss: 86.563614, loss: 73.308914
17:33:10.638269 Step[500/6995], lr: 0.001000, mv_avg_loss: 93.521698, loss: 70.595184
17:33:15.430197 Step[600/6995], lr: 0.001000, mv_avg_loss: 78.646080, loss: 132.296997
17:33:20.214155 Step[700/6995], lr: 0.001000, mv_avg_loss: 86.458969, loss: 69.296043
17:33:24.999513 Step[800/6995], lr: 0.001000, mv_avg_loss: 78.062447, loss: 73.781479
17:33:29.764581 Step[900/6995], lr: 0.001000, mv_avg_loss: 95.027069, loss: 96.188721
17:33:34.530573 Step[1000/6995], lr: 0.001000, mv_avg_loss: 88.100372, loss: 50.007675
17:33:39.331285 Step[1100/6995], lr: 0.001000, mv_avg_loss: 81.734512, loss: 56.637352
17:33:44.121553 Step[1200/6995], lr: 0.001000, mv_avg_loss: 84.722488, loss: 121.176682
17:33:48.884205 Step[1300/6995], lr: 0.001000, mv_avg_loss: 88.829247, loss: 95.810837
17:33:53.671919 Step[1400/6995], lr: 0.001000, mv_avg_loss: 93.126831, loss: 74.561562
17:33:58.458659 Step[1500/6995], lr: 0.001000, mv_avg_loss: 84.383904, loss: 96.052757
17:34:03.236982 Step[1600/6995], lr: 0.001000, mv_avg_loss: 68.325035, loss: 71.149200
17:34:08.001393 Step[1700/6995], lr: 0.001000, mv_avg_loss: 94.582260, loss: 66.448639
17:34:12.756126 Step[1800/6995], lr: 0.001000, mv_avg_loss: 91.851631, loss: 75.029953
17:34:17.516621 Step[1900/6995], lr: 0.001000, mv_avg_loss: 107.590378, loss: 51.075470
17:34:22.283886 Step[2000/6995], lr: 0.001000, mv_avg_loss: 83.265099, loss: 98.613312
17:34:27.043015 Step[2100/6995], lr: 0.001000, mv_avg_loss: 77.386932, loss: 118.261284
17:34:31.821366 Step[2200/6995], lr: 0.001000, mv_avg_loss: 82.723351, loss: 137.052383
17:34:36.578251 Step[2300/6995], lr: 0.001000, mv_avg_loss: 91.780067, loss: 68.993332
17:34:41.343452 Step[2400/6995], lr: 0.001000, mv_avg_loss: 89.561264, loss: 148.661255
17:34:46.096574 Step[2500/6995], lr: 0.001000, mv_avg_loss: 73.102173, loss: 60.237560
17:34:50.847695 Step[2600/6995], lr: 0.001000, mv_avg_loss: 80.079643, loss: 92.767212
17:34:55.606273 Step[2700/6995], lr: 0.001000, mv_avg_loss: 82.774208, loss: 77.645988
17:35:00.367151 Step[2800/6995], lr: 0.001000, mv_avg_loss: 86.036598, loss: 77.543022
17:35:05.143160 Step[2900/6995], lr: 0.001000, mv_avg_loss: 99.929039, loss: 61.476665
17:35:09.911719 Step[3000/6995], lr: 0.001000, mv_avg_loss: 89.964630, loss: 68.295158
17:35:14.675837 Step[3100/6995], lr: 0.001000, mv_avg_loss: 79.443176, loss: 57.570168
17:35:19.439994 Step[3200/6995], lr: 0.001000, mv_avg_loss: 77.198677, loss: 67.049408
17:35:24.184013 Step[3300/6995], lr: 0.001000, mv_avg_loss: 89.420464, loss: 57.268944
17:35:28.935990 Step[3400/6995], lr: 0.001000, mv_avg_loss: 80.206032, loss: 80.612541
17:35:33.707254 Step[3500/6995], lr: 0.001000, mv_avg_loss: 81.863785, loss: 109.425232
17:35:38.471764 Step[3600/6995], lr: 0.001000, mv_avg_loss: 71.748207, loss: 86.996140
17:35:43.232955 Step[3700/6995], lr: 0.001000, mv_avg_loss: 81.490005, loss: 62.020767
17:35:48.005498 Step[3800/6995], lr: 0.001000, mv_avg_loss: 93.607681, loss: 142.065842
17:35:52.771423 Step[3900/6995], lr: 0.001000, mv_avg_loss: 81.612206, loss: 69.560532
17:35:57.530213 Step[4000/6995], lr: 0.001000, mv_avg_loss: 85.590210, loss: 156.994202
17:36:02.306509 Step[4100/6995], lr: 0.001000, mv_avg_loss: 76.592262, loss: 56.547985
17:36:07.062389 Step[4200/6995], lr: 0.001000, mv_avg_loss: 82.074181, loss: 91.209412
17:36:11.831320 Step[4300/6995], lr: 0.001000, mv_avg_loss: 102.771149, loss: 139.916016
17:36:16.587229 Step[4400/6995], lr: 0.001000, mv_avg_loss: 84.221954, loss: 72.661743
17:36:21.355267 Step[4500/6995], lr: 0.001000, mv_avg_loss: 99.960075, loss: 118.771637
17:36:26.133035 Step[4600/6995], lr: 0.001000, mv_avg_loss: 87.578926, loss: 74.728287
17:36:30.906943 Step[4700/6995], lr: 0.001000, mv_avg_loss: 96.120827, loss: 98.148735
17:36:35.661308 Step[4800/6995], lr: 0.001000, mv_avg_loss: 85.550407, loss: 63.945244
17:36:40.428379 Step[4900/6995], lr: 0.001000, mv_avg_loss: 73.456619, loss: 72.270134
17:36:45.200025 Step[5000/6995], lr: 0.001000, mv_avg_loss: 71.866493, loss: 57.711498
17:36:49.956370 Step[5100/6995], lr: 0.001000, mv_avg_loss: 90.875397, loss: 61.752514
17:36:54.713836 Step[5200/6995], lr: 0.001000, mv_avg_loss: 74.250725, loss: 80.587463
17:36:59.481188 Step[5300/6995], lr: 0.001000, mv_avg_loss: 73.840317, loss: 44.656506
17:37:04.239364 Step[5400/6995], lr: 0.001000, mv_avg_loss: 78.785919, loss: 106.329865
17:37:08.981800 Step[5500/6995], lr: 0.001000, mv_avg_loss: 77.283813, loss: 38.684151
17:37:13.730032 Step[5600/6995], lr: 0.001000, mv_avg_loss: 83.111984, loss: 94.646507
17:37:18.482787 Step[5700/6995], lr: 0.001000, mv_avg_loss: 76.532051, loss: 72.443085
17:37:23.225307 Step[5800/6995], lr: 0.001000, mv_avg_loss: 83.723824, loss: 131.549896
17:37:27.992225 Step[5900/6995], lr: 0.001000, mv_avg_loss: 75.296143, loss: 69.300552
17:37:32.747485 Step[6000/6995], lr: 0.001000, mv_avg_loss: 75.349449, loss: 56.107464
17:37:37.523962 Step[6100/6995], lr: 0.001000, mv_avg_loss: 84.957764, loss: 88.192535
17:37:42.284575 Step[6200/6995], lr: 0.001000, mv_avg_loss: 93.220818, loss: 63.276093
17:37:47.048285 Step[6300/6995], lr: 0.001000, mv_avg_loss: 84.287300, loss: 222.691681
17:37:51.797590 Step[6400/6995], lr: 0.001000, mv_avg_loss: 82.189560, loss: 93.185165
17:37:56.548959 Step[6500/6995], lr: 0.001000, mv_avg_loss: 76.210289, loss: 84.482040
17:38:01.315580 Step[6600/6995], lr: 0.001000, mv_avg_loss: 70.162773, loss: 58.072475
17:38:06.055262 Step[6700/6995], lr: 0.001000, mv_avg_loss: 80.631531, loss: 45.408112
17:38:10.809021 Step[6800/6995], lr: 0.001000, mv_avg_loss: 83.893799, loss: 169.420776
17:38:15.545517 Step[6900/6995], lr: 0.001000, mv_avg_loss: 89.847969, loss: 56.837097
Testing for epoch: 7
Average test PNSR is 24.365122 for 500 images
Start to train epoch 8
17:38:41.583383 Step[0/6995], lr: 0.001000, mv_avg_loss: 78.197983, loss: 45.150829
17:38:46.309933 Step[100/6995], lr: 0.001000, mv_avg_loss: 75.220177, loss: 65.393021
17:38:51.018966 Step[200/6995], lr: 0.001000, mv_avg_loss: 82.919533, loss: 54.914421
17:38:55.726402 Step[300/6995], lr: 0.001000, mv_avg_loss: 76.537361, loss: 43.349438
17:39:00.449720 Step[400/6995], lr: 0.001000, mv_avg_loss: 81.506699, loss: 50.610313
17:39:05.199867 Step[500/6995], lr: 0.001000, mv_avg_loss: 88.207916, loss: 44.478081
17:39:09.933217 Step[600/6995], lr: 0.001000, mv_avg_loss: 82.690903, loss: 75.889771
17:39:14.670080 Step[700/6995], lr: 0.001000, mv_avg_loss: 71.382820, loss: 42.942959
17:39:19.388984 Step[800/6995], lr: 0.001000, mv_avg_loss: 72.565598, loss: 88.771873
17:39:24.116177 Step[900/6995], lr: 0.001000, mv_avg_loss: 74.313385, loss: 75.361877
17:39:28.841228 Step[1000/6995], lr: 0.001000, mv_avg_loss: 68.517654, loss: 61.910126
17:39:33.571012 Step[1100/6995], lr: 0.001000, mv_avg_loss: 74.597260, loss: 70.697197
17:39:38.290868 Step[1200/6995], lr: 0.001000, mv_avg_loss: 81.021484, loss: 49.348747
17:39:43.017122 Step[1300/6995], lr: 0.001000, mv_avg_loss: 70.744904, loss: 106.771118
17:39:47.745801 Step[1400/6995], lr: 0.001000, mv_avg_loss: 78.719772, loss: 51.770233
17:39:52.482682 Step[1500/6995], lr: 0.001000, mv_avg_loss: 66.877922, loss: 68.751404
17:39:57.225758 Step[1600/6995], lr: 0.001000, mv_avg_loss: 68.600121, loss: 58.762779
17:40:01.963817 Step[1700/6995], lr: 0.001000, mv_avg_loss: 78.651070, loss: 91.080826
17:40:06.681203 Step[1800/6995], lr: 0.001000, mv_avg_loss: 68.451660, loss: 38.898575
17:40:11.405910 Step[1900/6995], lr: 0.001000, mv_avg_loss: 87.043961, loss: 62.803101
17:40:16.130848 Step[2000/6995], lr: 0.001000, mv_avg_loss: 65.354172, loss: 75.348694
17:40:20.869050 Step[2100/6995], lr: 0.001000, mv_avg_loss: 68.856544, loss: 65.585190
17:40:25.612793 Step[2200/6995], lr: 0.001000, mv_avg_loss: 64.443840, loss: 57.898964
17:40:30.340580 Step[2300/6995], lr: 0.001000, mv_avg_loss: 71.331352, loss: 46.396042
17:40:35.088971 Step[2400/6995], lr: 0.001000, mv_avg_loss: 78.804581, loss: 117.929199
17:40:39.826986 Step[2500/6995], lr: 0.001000, mv_avg_loss: 68.518555, loss: 44.322689
17:40:44.574163 Step[2600/6995], lr: 0.001000, mv_avg_loss: 71.771782, loss: 113.912315
17:40:49.314470 Step[2700/6995], lr: 0.001000, mv_avg_loss: 82.892189, loss: 51.225601
17:40:54.060111 Step[2800/6995], lr: 0.001000, mv_avg_loss: 69.161568, loss: 45.656059
17:40:58.816131 Step[2900/6995], lr: 0.001000, mv_avg_loss: 69.627129, loss: 58.958580
17:41:03.561279 Step[3000/6995], lr: 0.001000, mv_avg_loss: 76.820457, loss: 52.526276
17:41:08.310262 Step[3100/6995], lr: 0.001000, mv_avg_loss: 78.178391, loss: 48.640347
17:41:13.064689 Step[3200/6995], lr: 0.001000, mv_avg_loss: 74.272301, loss: 60.694427
17:41:17.811729 Step[3300/6995], lr: 0.001000, mv_avg_loss: 72.752220, loss: 70.452774
17:41:22.553343 Step[3400/6995], lr: 0.001000, mv_avg_loss: 86.448776, loss: 121.933899
17:41:27.296873 Step[3500/6995], lr: 0.001000, mv_avg_loss: 72.076309, loss: 35.473972
17:41:32.032684 Step[3600/6995], lr: 0.001000, mv_avg_loss: 82.133492, loss: 187.867035
17:41:36.771407 Step[3700/6995], lr: 0.001000, mv_avg_loss: 77.257797, loss: 69.902878
17:41:41.523547 Step[3800/6995], lr: 0.001000, mv_avg_loss: 79.292007, loss: 38.214550
17:41:46.276992 Step[3900/6995], lr: 0.001000, mv_avg_loss: 76.742027, loss: 68.902519
17:41:51.027679 Step[4000/6995], lr: 0.001000, mv_avg_loss: 74.504761, loss: 80.711739
17:41:55.787514 Step[4100/6995], lr: 0.001000, mv_avg_loss: 75.526703, loss: 58.301922
17:42:00.529954 Step[4200/6995], lr: 0.001000, mv_avg_loss: 74.617729, loss: 74.425583
17:42:05.260978 Step[4300/6995], lr: 0.001000, mv_avg_loss: 77.768990, loss: 85.944336
17:42:09.995218 Step[4400/6995], lr: 0.001000, mv_avg_loss: 73.709000, loss: 62.486198
17:42:14.736072 Step[4500/6995], lr: 0.001000, mv_avg_loss: 71.597702, loss: 51.600414
17:42:19.482775 Step[4600/6995], lr: 0.001000, mv_avg_loss: 70.917946, loss: 81.868889
17:42:24.221325 Step[4700/6995], lr: 0.001000, mv_avg_loss: 77.345390, loss: 75.726578
17:42:28.956199 Step[4800/6995], lr: 0.001000, mv_avg_loss: 69.504845, loss: 44.550655
17:42:33.701516 Step[4900/6995], lr: 0.001000, mv_avg_loss: 81.847229, loss: 91.612793
17:42:38.460453 Step[5000/6995], lr: 0.001000, mv_avg_loss: 84.355362, loss: 97.832970
17:42:43.202725 Step[5100/6995], lr: 0.001000, mv_avg_loss: 75.317642, loss: 89.810875
17:42:47.953724 Step[5200/6995], lr: 0.001000, mv_avg_loss: 72.072571, loss: 112.300888
17:42:52.675850 Step[5300/6995], lr: 0.001000, mv_avg_loss: 73.073212, loss: 69.088036
17:42:57.410147 Step[5400/6995], lr: 0.001000, mv_avg_loss: 79.896416, loss: 83.580994
17:43:02.122184 Step[5500/6995], lr: 0.001000, mv_avg_loss: 83.111343, loss: 56.813454
17:43:06.866542 Step[5600/6995], lr: 0.001000, mv_avg_loss: 74.605362, loss: 66.553993
17:43:11.604935 Step[5700/6995], lr: 0.001000, mv_avg_loss: 71.784637, loss: 97.444450
17:43:16.340828 Step[5800/6995], lr: 0.001000, mv_avg_loss: 72.608688, loss: 59.261108
17:43:21.089568 Step[5900/6995], lr: 0.001000, mv_avg_loss: 70.652107, loss: 44.802547
17:43:25.827003 Step[6000/6995], lr: 0.001000, mv_avg_loss: 69.891975, loss: 52.395748
17:43:30.584036 Step[6100/6995], lr: 0.001000, mv_avg_loss: 84.597824, loss: 176.972198
17:43:35.344971 Step[6200/6995], lr: 0.001000, mv_avg_loss: 75.469711, loss: 80.406509
17:43:40.130760 Step[6300/6995], lr: 0.001000, mv_avg_loss: 71.669205, loss: 78.661247
17:43:44.865172 Step[6400/6995], lr: 0.001000, mv_avg_loss: 75.499184, loss: 87.417206
17:43:49.617629 Step[6500/6995], lr: 0.001000, mv_avg_loss: 80.004463, loss: 62.406982
17:43:54.376935 Step[6600/6995], lr: 0.001000, mv_avg_loss: 98.567711, loss: 108.072693
17:43:59.139559 Step[6700/6995], lr: 0.001000, mv_avg_loss: 73.243889, loss: 60.490250
17:44:03.886604 Step[6800/6995], lr: 0.001000, mv_avg_loss: 72.352310, loss: 52.738129
17:44:08.655286 Step[6900/6995], lr: 0.001000, mv_avg_loss: 70.800926, loss: 49.989891
Testing for epoch: 8
Average test PNSR is 25.053216 for 500 images
Start to train epoch 9
17:44:33.941504 Step[0/6995], lr: 0.001000, mv_avg_loss: 73.108223, loss: 135.717072
17:44:38.682937 Step[100/6995], lr: 0.001000, mv_avg_loss: 64.997536, loss: 196.329926
17:44:43.435683 Step[200/6995], lr: 0.001000, mv_avg_loss: 70.210785, loss: 83.557549
17:44:48.170586 Step[300/6995], lr: 0.001000, mv_avg_loss: 62.878143, loss: 37.580505
17:44:52.921259 Step[400/6995], lr: 0.001000, mv_avg_loss: 74.160378, loss: 102.925987
17:44:57.687189 Step[500/6995], lr: 0.001000, mv_avg_loss: 60.951126, loss: 50.403008
17:45:02.452399 Step[600/6995], lr: 0.001000, mv_avg_loss: 65.076820, loss: 38.134132
17:45:07.209176 Step[700/6995], lr: 0.001000, mv_avg_loss: 64.390633, loss: 53.215324
17:45:11.955429 Step[800/6995], lr: 0.001000, mv_avg_loss: 62.307163, loss: 60.680603
17:45:16.696316 Step[900/6995], lr: 0.001000, mv_avg_loss: 67.120377, loss: 38.133678
17:45:21.438571 Step[1000/6995], lr: 0.001000, mv_avg_loss: 66.032471, loss: 68.150620
17:45:26.191433 Step[1100/6995], lr: 0.001000, mv_avg_loss: 66.893311, loss: 67.797676
17:45:30.944045 Step[1200/6995], lr: 0.001000, mv_avg_loss: 59.205536, loss: 45.143383
17:45:35.698150 Step[1300/6995], lr: 0.001000, mv_avg_loss: 61.585266, loss: 88.220398
17:45:40.483793 Step[1400/6995], lr: 0.001000, mv_avg_loss: 67.037224, loss: 78.292526
17:45:45.225087 Step[1500/6995], lr: 0.001000, mv_avg_loss: 59.305199, loss: 34.550617
17:45:49.957787 Step[1600/6995], lr: 0.001000, mv_avg_loss: 63.308041, loss: 28.227886
17:45:54.708145 Step[1700/6995], lr: 0.001000, mv_avg_loss: 67.336739, loss: 65.639359
17:45:59.456954 Step[1800/6995], lr: 0.001000, mv_avg_loss: 71.688248, loss: 140.445038
17:46:04.205939 Step[1900/6995], lr: 0.001000, mv_avg_loss: 68.842903, loss: 67.065842
17:46:08.951316 Step[2000/6995], lr: 0.001000, mv_avg_loss: 68.603531, loss: 87.417297
17:46:13.747850 Step[2100/6995], lr: 0.001000, mv_avg_loss: 63.006424, loss: 68.072861
17:46:18.515052 Step[2200/6995], lr: 0.001000, mv_avg_loss: 65.201210, loss: 50.443817
17:46:23.278078 Step[2300/6995], lr: 0.001000, mv_avg_loss: 61.567753, loss: 79.088501
17:46:28.025503 Step[2400/6995], lr: 0.001000, mv_avg_loss: 75.553429, loss: 74.026413
17:46:32.779584 Step[2500/6995], lr: 0.001000, mv_avg_loss: 70.334000, loss: 70.553635
17:46:37.532521 Step[2600/6995], lr: 0.001000, mv_avg_loss: 61.820183, loss: 54.542187
17:46:42.291492 Step[2700/6995], lr: 0.001000, mv_avg_loss: 63.582104, loss: 47.300671
17:46:47.050199 Step[2800/6995], lr: 0.001000, mv_avg_loss: 70.552856, loss: 92.966370
17:46:51.809547 Step[2900/6995], lr: 0.001000, mv_avg_loss: 64.009598, loss: 49.031094
17:46:56.565247 Step[3000/6995], lr: 0.001000, mv_avg_loss: 77.373520, loss: 81.250565
17:47:01.327588 Step[3100/6995], lr: 0.001000, mv_avg_loss: 71.269753, loss: 42.343773
17:47:06.097880 Step[3200/6995], lr: 0.001000, mv_avg_loss: 67.600212, loss: 83.680626
17:47:10.867105 Step[3300/6995], lr: 0.001000, mv_avg_loss: 74.404930, loss: 84.484009
17:47:15.636925 Step[3400/6995], lr: 0.001000, mv_avg_loss: 68.859810, loss: 73.226112
17:47:20.402899 Step[3500/6995], lr: 0.001000, mv_avg_loss: 82.212303, loss: 67.011383
17:47:25.172585 Step[3600/6995], lr: 0.001000, mv_avg_loss: 67.483284, loss: 61.701824
17:47:29.949038 Step[3700/6995], lr: 0.001000, mv_avg_loss: 67.888199, loss: 54.535210
17:47:34.699386 Step[3800/6995], lr: 0.001000, mv_avg_loss: 63.939899, loss: 38.980251
17:47:39.447884 Step[3900/6995], lr: 0.001000, mv_avg_loss: 70.173721, loss: 56.153542
17:47:44.210144 Step[4000/6995], lr: 0.001000, mv_avg_loss: 59.564438, loss: 41.936813
17:47:48.956681 Step[4100/6995], lr: 0.001000, mv_avg_loss: 79.364578, loss: 73.537918
17:47:53.708953 Step[4200/6995], lr: 0.001000, mv_avg_loss: 67.021843, loss: 103.057861
17:47:58.448510 Step[4300/6995], lr: 0.001000, mv_avg_loss: 65.607239, loss: 53.398033
17:48:03.190393 Step[4400/6995], lr: 0.001000, mv_avg_loss: 66.181465, loss: 98.849815
17:48:07.931932 Step[4500/6995], lr: 0.001000, mv_avg_loss: 68.708107, loss: 43.695831
17:48:12.679782 Step[4600/6995], lr: 0.001000, mv_avg_loss: 73.513557, loss: 36.015087
17:48:17.429911 Step[4700/6995], lr: 0.001000, mv_avg_loss: 67.297653, loss: 41.753929
17:48:22.171962 Step[4800/6995], lr: 0.001000, mv_avg_loss: 61.002522, loss: 44.632225
17:48:26.927141 Step[4900/6995], lr: 0.001000, mv_avg_loss: 74.299431, loss: 120.052383
17:48:31.686836 Step[5000/6995], lr: 0.001000, mv_avg_loss: 67.196953, loss: 77.351990
17:48:36.452128 Step[5100/6995], lr: 0.001000, mv_avg_loss: 60.382771, loss: 42.665627
17:48:41.195033 Step[5200/6995], lr: 0.001000, mv_avg_loss: 74.373627, loss: 72.657120
17:48:45.937382 Step[5300/6995], lr: 0.001000, mv_avg_loss: 64.341972, loss: 64.089859
17:48:50.679845 Step[5400/6995], lr: 0.001000, mv_avg_loss: 84.481285, loss: 54.446030
17:48:55.427017 Step[5500/6995], lr: 0.001000, mv_avg_loss: 67.480515, loss: 76.712517
17:49:00.163897 Step[5600/6995], lr: 0.001000, mv_avg_loss: 60.429287, loss: 65.519241
17:49:04.897405 Step[5700/6995], lr: 0.001000, mv_avg_loss: 60.382519, loss: 48.391960
17:49:09.642193 Step[5800/6995], lr: 0.001000, mv_avg_loss: 57.625290, loss: 32.226295
17:49:14.382121 Step[5900/6995], lr: 0.001000, mv_avg_loss: 66.457420, loss: 65.967789
17:49:19.128008 Step[6000/6995], lr: 0.001000, mv_avg_loss: 74.511612, loss: 66.084244
17:49:23.875123 Step[6100/6995], lr: 0.001000, mv_avg_loss: 72.465240, loss: 83.242599
17:49:28.630700 Step[6200/6995], lr: 0.001000, mv_avg_loss: 68.255424, loss: 45.550766
17:49:33.359666 Step[6300/6995], lr: 0.001000, mv_avg_loss: 66.454788, loss: 72.844437
17:49:38.106744 Step[6400/6995], lr: 0.001000, mv_avg_loss: 68.909431, loss: 40.624313
17:49:42.846262 Step[6500/6995], lr: 0.001000, mv_avg_loss: 60.861118, loss: 44.596539
17:49:47.573133 Step[6600/6995], lr: 0.001000, mv_avg_loss: 63.543304, loss: 66.550461
17:49:52.308081 Step[6700/6995], lr: 0.001000, mv_avg_loss: 76.395714, loss: 63.525108
17:49:57.050933 Step[6800/6995], lr: 0.001000, mv_avg_loss: 75.182541, loss: 58.190258
17:50:01.800465 Step[6900/6995], lr: 0.001000, mv_avg_loss: 68.605225, loss: 54.909424
saving model for epoch 9
Testing for epoch: 9
Average test PNSR is 24.945179 for 500 images
Start to train epoch 10
17:50:28.077160 Step[0/6995], lr: 0.001000, mv_avg_loss: 57.067768, loss: 47.692287
17:50:32.821480 Step[100/6995], lr: 0.001000, mv_avg_loss: 64.017090, loss: 81.698006
17:50:37.558004 Step[200/6995], lr: 0.001000, mv_avg_loss: 52.960876, loss: 60.012383
17:50:42.294026 Step[300/6995], lr: 0.001000, mv_avg_loss: 59.285595, loss: 51.006481
17:50:47.033613 Step[400/6995], lr: 0.001000, mv_avg_loss: 59.214115, loss: 75.833237
17:50:51.762698 Step[500/6995], lr: 0.001000, mv_avg_loss: 59.568737, loss: 32.354946
17:50:56.491969 Step[600/6995], lr: 0.001000, mv_avg_loss: 56.065090, loss: 35.384819
17:51:01.218367 Step[700/6995], lr: 0.001000, mv_avg_loss: 60.622997, loss: 47.564098
17:51:05.954299 Step[800/6995], lr: 0.001000, mv_avg_loss: 58.056057, loss: 42.523655
17:51:10.685400 Step[900/6995], lr: 0.001000, mv_avg_loss: 73.315819, loss: 150.490570
17:51:15.428649 Step[1000/6995], lr: 0.001000, mv_avg_loss: 58.909126, loss: 41.233334
17:51:20.143931 Step[1100/6995], lr: 0.001000, mv_avg_loss: 63.997429, loss: 67.397705
17:51:24.863600 Step[1200/6995], lr: 0.001000, mv_avg_loss: 66.486893, loss: 97.836960
17:51:29.609828 Step[1300/6995], lr: 0.001000, mv_avg_loss: 66.182503, loss: 85.344353
17:51:34.332823 Step[1400/6995], lr: 0.001000, mv_avg_loss: 57.410057, loss: 48.791786
17:51:39.068673 Step[1500/6995], lr: 0.001000, mv_avg_loss: 65.661980, loss: 49.505306
17:51:43.788934 Step[1600/6995], lr: 0.001000, mv_avg_loss: 69.258324, loss: 47.631042
17:51:48.528375 Step[1700/6995], lr: 0.001000, mv_avg_loss: 57.906258, loss: 135.534531
17:51:53.282257 Step[1800/6995], lr: 0.001000, mv_avg_loss: 58.256508, loss: 84.245987
17:51:58.011173 Step[1900/6995], lr: 0.001000, mv_avg_loss: 60.269382, loss: 27.395367
17:52:02.747846 Step[2000/6995], lr: 0.001000, mv_avg_loss: 64.459297, loss: 57.805237
17:52:07.494343 Step[2100/6995], lr: 0.001000, mv_avg_loss: 63.621517, loss: 69.349121
17:52:12.208799 Step[2200/6995], lr: 0.001000, mv_avg_loss: 78.295273, loss: 92.590271
17:52:16.931641 Step[2300/6995], lr: 0.001000, mv_avg_loss: 66.109680, loss: 90.010986
17:52:21.671870 Step[2400/6995], lr: 0.001000, mv_avg_loss: 58.504208, loss: 45.108074
17:52:26.391809 Step[2500/6995], lr: 0.001000, mv_avg_loss: 61.933220, loss: 49.646759
17:52:31.112393 Step[2600/6995], lr: 0.001000, mv_avg_loss: 64.116936, loss: 89.971703
17:52:35.852401 Step[2700/6995], lr: 0.001000, mv_avg_loss: 67.686714, loss: 65.691277
17:52:40.595721 Step[2800/6995], lr: 0.001000, mv_avg_loss: 69.415398, loss: 81.962250
17:52:45.356444 Step[2900/6995], lr: 0.001000, mv_avg_loss: 66.409142, loss: 100.475212
17:52:50.085921 Step[3000/6995], lr: 0.001000, mv_avg_loss: 64.600319, loss: 42.316303
17:52:54.828446 Step[3100/6995], lr: 0.001000, mv_avg_loss: 64.051422, loss: 26.584021
17:52:59.576638 Step[3200/6995], lr: 0.001000, mv_avg_loss: 58.362095, loss: 47.676174
17:53:04.289846 Step[3300/6995], lr: 0.001000, mv_avg_loss: 68.487236, loss: 53.365353
17:53:09.008435 Step[3400/6995], lr: 0.001000, mv_avg_loss: 55.337204, loss: 50.585716
17:53:13.729223 Step[3500/6995], lr: 0.001000, mv_avg_loss: 64.693481, loss: 65.950539
17:53:18.454183 Step[3600/6995], lr: 0.001000, mv_avg_loss: 63.071545, loss: 63.575500
17:53:23.182743 Step[3700/6995], lr: 0.001000, mv_avg_loss: 63.440289, loss: 47.914970
17:53:27.899999 Step[3800/6995], lr: 0.001000, mv_avg_loss: 60.059612, loss: 40.288185
17:53:32.617008 Step[3900/6995], lr: 0.001000, mv_avg_loss: 76.728622, loss: 72.076050
17:53:37.347117 Step[4000/6995], lr: 0.001000, mv_avg_loss: 60.891384, loss: 71.658829
17:53:42.077477 Step[4100/6995], lr: 0.001000, mv_avg_loss: 52.199169, loss: 40.173523
17:53:46.810760 Step[4200/6995], lr: 0.001000, mv_avg_loss: 61.112099, loss: 55.696091
17:53:51.541023 Step[4300/6995], lr: 0.001000, mv_avg_loss: 66.293449, loss: 83.688805
17:53:56.270084 Step[4400/6995], lr: 0.001000, mv_avg_loss: 68.058372, loss: 47.563644
17:54:01.010690 Step[4500/6995], lr: 0.001000, mv_avg_loss: 60.445698, loss: 62.416069
17:54:05.742919 Step[4600/6995], lr: 0.001000, mv_avg_loss: 63.875824, loss: 69.437614
17:54:10.483393 Step[4700/6995], lr: 0.001000, mv_avg_loss: 63.002380, loss: 40.960323
17:54:15.233267 Step[4800/6995], lr: 0.001000, mv_avg_loss: 76.115669, loss: 66.051498
17:54:19.981501 Step[4900/6995], lr: 0.001000, mv_avg_loss: 65.474022, loss: 44.094299
17:54:24.727901 Step[5000/6995], lr: 0.001000, mv_avg_loss: 65.695290, loss: 49.256104
17:54:29.478339 Step[5100/6995], lr: 0.001000, mv_avg_loss: 62.768398, loss: 135.939392
17:54:34.194400 Step[5200/6995], lr: 0.001000, mv_avg_loss: 60.919388, loss: 47.095196
17:54:38.922642 Step[5300/6995], lr: 0.001000, mv_avg_loss: 55.481747, loss: 49.803467
17:54:43.653647 Step[5400/6995], lr: 0.001000, mv_avg_loss: 53.855965, loss: 68.906036
17:54:48.370714 Step[5500/6995], lr: 0.001000, mv_avg_loss: 79.003952, loss: 75.690392
17:54:53.104550 Step[5600/6995], lr: 0.001000, mv_avg_loss: 64.665878, loss: 53.021679
17:54:57.826033 Step[5700/6995], lr: 0.001000, mv_avg_loss: 60.760662, loss: 86.738152
17:55:02.546942 Step[5800/6995], lr: 0.001000, mv_avg_loss: 62.994473, loss: 46.312653
17:55:07.278484 Step[5900/6995], lr: 0.001000, mv_avg_loss: 66.544098, loss: 56.375408
17:55:12.000760 Step[6000/6995], lr: 0.001000, mv_avg_loss: 59.895073, loss: 34.957893
17:55:16.742571 Step[6100/6995], lr: 0.001000, mv_avg_loss: 55.287334, loss: 59.335217
17:55:21.482483 Step[6200/6995], lr: 0.001000, mv_avg_loss: 61.861485, loss: 98.154076
17:55:26.221464 Step[6300/6995], lr: 0.001000, mv_avg_loss: 53.963360, loss: 53.003891
17:55:30.938202 Step[6400/6995], lr: 0.001000, mv_avg_loss: 57.503311, loss: 33.296478
17:55:35.654496 Step[6500/6995], lr: 0.001000, mv_avg_loss: 59.151131, loss: 30.459112
17:55:40.384546 Step[6600/6995], lr: 0.001000, mv_avg_loss: 65.767784, loss: 51.965145
17:55:45.104964 Step[6700/6995], lr: 0.001000, mv_avg_loss: 55.051983, loss: 53.683105
17:55:49.826802 Step[6800/6995], lr: 0.001000, mv_avg_loss: 56.251213, loss: 36.461849
17:55:54.549779 Step[6900/6995], lr: 0.001000, mv_avg_loss: 63.023483, loss: 59.532513
Testing for epoch: 10
Average test PNSR is 25.053628 for 500 images
Start to train epoch 11
17:56:21.196321 Step[0/6995], lr: 0.001000, mv_avg_loss: 64.914497, loss: 59.725399
17:56:25.935210 Step[100/6995], lr: 0.001000, mv_avg_loss: 56.140686, loss: 59.758293
17:56:30.658343 Step[200/6995], lr: 0.001000, mv_avg_loss: 55.236221, loss: 52.930099
17:56:35.398459 Step[300/6995], lr: 0.001000, mv_avg_loss: 53.082222, loss: 57.586605
17:56:40.131548 Step[400/6995], lr: 0.001000, mv_avg_loss: 58.772472, loss: 49.007019
17:56:44.871196 Step[500/6995], lr: 0.001000, mv_avg_loss: 55.748917, loss: 56.592888
17:56:49.612000 Step[600/6995], lr: 0.001000, mv_avg_loss: 56.092941, loss: 38.426662
17:56:54.352158 Step[700/6995], lr: 0.001000, mv_avg_loss: 57.484333, loss: 71.949394
17:56:59.083224 Step[800/6995], lr: 0.001000, mv_avg_loss: 55.359676, loss: 37.883705
17:57:03.816192 Step[900/6995], lr: 0.001000, mv_avg_loss: 58.233166, loss: 71.772903
17:57:08.563480 Step[1000/6995], lr: 0.001000, mv_avg_loss: 67.443687, loss: 64.473351
17:57:13.315485 Step[1100/6995], lr: 0.001000, mv_avg_loss: 77.181610, loss: 65.468109
17:57:18.055298 Step[1200/6995], lr: 0.001000, mv_avg_loss: 63.314632, loss: 69.210884
17:57:22.816074 Step[1300/6995], lr: 0.001000, mv_avg_loss: 55.526287, loss: 36.988182
17:57:27.572003 Step[1400/6995], lr: 0.001000, mv_avg_loss: 60.524174, loss: 49.220169
17:57:32.324841 Step[1500/6995], lr: 0.001000, mv_avg_loss: 63.112717, loss: 59.163635
17:57:37.082428 Step[1600/6995], lr: 0.001000, mv_avg_loss: 60.656303, loss: 65.220673
17:57:41.846109 Step[1700/6995], lr: 0.001000, mv_avg_loss: 60.430935, loss: 51.704540
17:57:46.607704 Step[1800/6995], lr: 0.001000, mv_avg_loss: 61.844746, loss: 63.119598
17:57:51.366277 Step[1900/6995], lr: 0.001000, mv_avg_loss: 59.046154, loss: 77.912521
17:57:56.127071 Step[2000/6995], lr: 0.001000, mv_avg_loss: 55.973103, loss: 82.923119
17:58:00.899010 Step[2100/6995], lr: 0.001000, mv_avg_loss: 68.009743, loss: 43.283249
17:58:05.660025 Step[2200/6995], lr: 0.001000, mv_avg_loss: 55.962688, loss: 44.630459
17:58:10.433986 Step[2300/6995], lr: 0.001000, mv_avg_loss: 59.937824, loss: 54.629112
17:58:15.191835 Step[2400/6995], lr: 0.001000, mv_avg_loss: 55.665329, loss: 47.582603
17:58:19.950481 Step[2500/6995], lr: 0.001000, mv_avg_loss: 57.251831, loss: 77.543320
17:58:24.722171 Step[2600/6995], lr: 0.001000, mv_avg_loss: 57.665882, loss: 41.203350
17:58:29.504302 Step[2700/6995], lr: 0.001000, mv_avg_loss: 56.131557, loss: 52.583111
17:58:34.264738 Step[2800/6995], lr: 0.001000, mv_avg_loss: 54.414543, loss: 46.477612
17:58:39.032000 Step[2900/6995], lr: 0.001000, mv_avg_loss: 59.322571, loss: 57.897736
17:58:43.822220 Step[3000/6995], lr: 0.001000, mv_avg_loss: 57.398285, loss: 42.317169
17:58:48.651145 Step[3100/6995], lr: 0.001000, mv_avg_loss: 60.601555, loss: 88.607147
17:58:53.449275 Step[3200/6995], lr: 0.001000, mv_avg_loss: 56.411011, loss: 59.248856
17:58:58.245144 Step[3300/6995], lr: 0.001000, mv_avg_loss: 61.838451, loss: 63.478374
17:59:03.040733 Step[3400/6995], lr: 0.001000, mv_avg_loss: 59.702747, loss: 49.251141
17:59:07.924746 Step[3500/6995], lr: 0.001000, mv_avg_loss: 62.685760, loss: 76.814110
17:59:12.724241 Step[3600/6995], lr: 0.001000, mv_avg_loss: 63.994656, loss: 44.033997
17:59:17.539368 Step[3700/6995], lr: 0.001000, mv_avg_loss: 58.304375, loss: 36.021927
17:59:22.368446 Step[3800/6995], lr: 0.001000, mv_avg_loss: 67.022476, loss: 59.142143
17:59:27.191676 Step[3900/6995], lr: 0.001000, mv_avg_loss: 60.191185, loss: 44.204002
17:59:32.007040 Step[4000/6995], lr: 0.001000, mv_avg_loss: 62.259510, loss: 66.648071
17:59:36.807402 Step[4100/6995], lr: 0.001000, mv_avg_loss: 64.141212, loss: 66.726814
17:59:41.612598 Step[4200/6995], lr: 0.001000, mv_avg_loss: 62.406364, loss: 93.386307
17:59:46.429173 Step[4300/6995], lr: 0.001000, mv_avg_loss: 56.776810, loss: 48.727783
17:59:51.287472 Step[4400/6995], lr: 0.001000, mv_avg_loss: 61.662460, loss: 46.595802
17:59:56.144939 Step[4500/6995], lr: 0.001000, mv_avg_loss: 77.421135, loss: 66.516769
18:00:01.022170 Step[4600/6995], lr: 0.001000, mv_avg_loss: 59.158710, loss: 39.313141
18:00:05.828180 Step[4700/6995], lr: 0.001000, mv_avg_loss: 56.982601, loss: 96.256554
18:00:10.624126 Step[4800/6995], lr: 0.001000, mv_avg_loss: 61.398392, loss: 39.408283
18:00:15.464507 Step[4900/6995], lr: 0.001000, mv_avg_loss: 62.755581, loss: 60.650322
18:00:20.267988 Step[5000/6995], lr: 0.001000, mv_avg_loss: 59.080776, loss: 56.282784
18:00:25.065604 Step[5100/6995], lr: 0.001000, mv_avg_loss: 63.408485, loss: 51.644470
18:00:29.855990 Step[5200/6995], lr: 0.001000, mv_avg_loss: 58.577766, loss: 46.085197
18:00:34.656858 Step[5300/6995], lr: 0.001000, mv_avg_loss: 58.390278, loss: 45.924377
18:00:39.465371 Step[5400/6995], lr: 0.001000, mv_avg_loss: 55.516933, loss: 54.101757
18:00:44.265727 Step[5500/6995], lr: 0.001000, mv_avg_loss: 56.358318, loss: 148.470764
18:00:49.062643 Step[5600/6995], lr: 0.001000, mv_avg_loss: 63.268711, loss: 57.908081
18:00:53.853056 Step[5700/6995], lr: 0.001000, mv_avg_loss: 57.781670, loss: 38.223392
18:00:58.632558 Step[5800/6995], lr: 0.001000, mv_avg_loss: 59.152874, loss: 56.207603
18:01:03.428408 Step[5900/6995], lr: 0.001000, mv_avg_loss: 53.701355, loss: 41.042164
18:01:08.224213 Step[6000/6995], lr: 0.001000, mv_avg_loss: 52.775471, loss: 37.895199
18:01:13.012084 Step[6100/6995], lr: 0.001000, mv_avg_loss: 52.590252, loss: 57.545990
18:01:17.819204 Step[6200/6995], lr: 0.001000, mv_avg_loss: 51.679424, loss: 54.557495
18:01:22.609168 Step[6300/6995], lr: 0.001000, mv_avg_loss: 59.949551, loss: 54.598400
18:01:27.404204 Step[6400/6995], lr: 0.001000, mv_avg_loss: 53.255848, loss: 120.359741
18:01:32.193572 Step[6500/6995], lr: 0.001000, mv_avg_loss: 52.838070, loss: 46.515453
18:01:36.980772 Step[6600/6995], lr: 0.001000, mv_avg_loss: 55.914719, loss: 56.341106
18:01:41.778882 Step[6700/6995], lr: 0.001000, mv_avg_loss: 55.473320, loss: 45.864452
18:01:46.567446 Step[6800/6995], lr: 0.001000, mv_avg_loss: 52.931484, loss: 62.706799
18:01:51.349202 Step[6900/6995], lr: 0.001000, mv_avg_loss: 52.032120, loss: 55.700966
Testing for epoch: 11
Average test PNSR is 24.992099 for 500 images
Start to train epoch 12
18:02:21.106131 Step[0/6995], lr: 0.001000, mv_avg_loss: 55.598225, loss: 50.885040
18:02:25.866872 Step[100/6995], lr: 0.001000, mv_avg_loss: 53.791939, loss: 88.025597
18:02:30.643377 Step[200/6995], lr: 0.001000, mv_avg_loss: 52.032471, loss: 47.525402
18:02:35.420962 Step[300/6995], lr: 0.001000, mv_avg_loss: 50.303852, loss: 24.533047
18:02:40.197974 Step[400/6995], lr: 0.001000, mv_avg_loss: 52.670959, loss: 34.025608
18:02:44.976669 Step[500/6995], lr: 0.001000, mv_avg_loss: 55.122807, loss: 52.071144
18:02:49.751894 Step[600/6995], lr: 0.001000, mv_avg_loss: 63.660400, loss: 66.176384
18:02:54.527660 Step[700/6995], lr: 0.001000, mv_avg_loss: 53.923431, loss: 50.086235
18:02:59.305427 Step[800/6995], lr: 0.001000, mv_avg_loss: 58.446281, loss: 43.272545
18:03:04.074048 Step[900/6995], lr: 0.001000, mv_avg_loss: 61.291824, loss: 52.151043
18:03:08.847784 Step[1000/6995], lr: 0.001000, mv_avg_loss: 55.944237, loss: 36.380951
18:03:13.640698 Step[1100/6995], lr: 0.001000, mv_avg_loss: 53.802368, loss: 52.064907
18:03:18.385819 Step[1200/6995], lr: 0.001000, mv_avg_loss: 53.425014, loss: 38.594219
18:03:23.126733 Step[1300/6995], lr: 0.001000, mv_avg_loss: 51.075684, loss: 42.868141
18:03:27.893978 Step[1400/6995], lr: 0.001000, mv_avg_loss: 59.125351, loss: 178.389526
18:03:32.663142 Step[1500/6995], lr: 0.001000, mv_avg_loss: 59.638588, loss: 53.142223
18:03:37.410940 Step[1600/6995], lr: 0.001000, mv_avg_loss: 54.652534, loss: 40.989838
18:03:42.193140 Step[1700/6995], lr: 0.001000, mv_avg_loss: 56.343803, loss: 42.256054
18:03:46.952655 Step[1800/6995], lr: 0.001000, mv_avg_loss: 56.377312, loss: 71.585571
18:03:51.717238 Step[1900/6995], lr: 0.001000, mv_avg_loss: 61.893425, loss: 75.651268
18:03:56.459331 Step[2000/6995], lr: 0.001000, mv_avg_loss: 56.200962, loss: 42.923840
18:04:01.201174 Step[2100/6995], lr: 0.001000, mv_avg_loss: 56.838089, loss: 40.252815
18:04:05.960640 Step[2200/6995], lr: 0.001000, mv_avg_loss: 52.317177, loss: 43.093700
18:04:10.711837 Step[2300/6995], lr: 0.001000, mv_avg_loss: 60.145199, loss: 54.601131
18:04:15.458852 Step[2400/6995], lr: 0.001000, mv_avg_loss: 47.936794, loss: 58.264660
18:04:20.198711 Step[2500/6995], lr: 0.001000, mv_avg_loss: 50.040634, loss: 59.620770
18:04:24.928657 Step[2600/6995], lr: 0.001000, mv_avg_loss: 60.102913, loss: 82.514992
18:04:29.672420 Step[2700/6995], lr: 0.001000, mv_avg_loss: 63.424355, loss: 135.303406
18:04:34.429060 Step[2800/6995], lr: 0.001000, mv_avg_loss: 52.875847, loss: 33.154701
18:04:39.170183 Step[2900/6995], lr: 0.001000, mv_avg_loss: 49.248173, loss: 53.629406
18:04:43.929240 Step[3000/6995], lr: 0.001000, mv_avg_loss: 55.809780, loss: 93.315880
18:04:48.668039 Step[3100/6995], lr: 0.001000, mv_avg_loss: 55.361366, loss: 87.837219
18:04:53.415872 Step[3200/6995], lr: 0.001000, mv_avg_loss: 57.586254, loss: 48.745373
18:04:58.157760 Step[3300/6995], lr: 0.001000, mv_avg_loss: 54.844894, loss: 47.512321
18:05:02.900097 Step[3400/6995], lr: 0.001000, mv_avg_loss: 52.072674, loss: 46.770367
18:05:07.623944 Step[3500/6995], lr: 0.001000, mv_avg_loss: 57.852386, loss: 54.597763
18:05:12.412317 Step[3600/6995], lr: 0.001000, mv_avg_loss: 51.940342, loss: 46.552319
18:05:17.161296 Step[3700/6995], lr: 0.001000, mv_avg_loss: 50.765209, loss: 60.454590
18:05:21.901080 Step[3800/6995], lr: 0.001000, mv_avg_loss: 54.435291, loss: 43.837753
18:05:26.638170 Step[3900/6995], lr: 0.001000, mv_avg_loss: 48.500538, loss: 46.687622
18:05:31.385781 Step[4000/6995], lr: 0.001000, mv_avg_loss: 53.550583, loss: 90.702057
18:05:36.116383 Step[4100/6995], lr: 0.001000, mv_avg_loss: 49.892319, loss: 62.751709
18:05:40.864138 Step[4200/6995], lr: 0.001000, mv_avg_loss: 62.790688, loss: 70.373718
18:05:45.592449 Step[4300/6995], lr: 0.001000, mv_avg_loss: 61.744640, loss: 72.183121
18:05:50.327729 Step[4400/6995], lr: 0.001000, mv_avg_loss: 50.052441, loss: 45.539623
18:05:55.069167 Step[4500/6995], lr: 0.001000, mv_avg_loss: 51.979637, loss: 32.179199
18:05:59.836797 Step[4600/6995], lr: 0.001000, mv_avg_loss: 57.520199, loss: 43.732342
18:06:04.575798 Step[4700/6995], lr: 0.001000, mv_avg_loss: 58.594711, loss: 80.830750
18:06:09.318777 Step[4800/6995], lr: 0.001000, mv_avg_loss: 58.651669, loss: 59.575562
18:06:14.052014 Step[4900/6995], lr: 0.001000, mv_avg_loss: 52.996864, loss: 40.100838
18:06:18.781873 Step[5000/6995], lr: 0.001000, mv_avg_loss: 66.953659, loss: 89.562668
18:06:23.531216 Step[5100/6995], lr: 0.001000, mv_avg_loss: 55.070435, loss: 53.818687
18:06:28.284541 Step[5200/6995], lr: 0.001000, mv_avg_loss: 62.217670, loss: 30.554251
18:06:33.041333 Step[5300/6995], lr: 0.001000, mv_avg_loss: 51.742691, loss: 31.342520
18:06:37.786368 Step[5400/6995], lr: 0.001000, mv_avg_loss: 57.854282, loss: 66.121643
18:06:42.521019 Step[5500/6995], lr: 0.001000, mv_avg_loss: 47.525928, loss: 54.468750
18:06:47.264583 Step[5600/6995], lr: 0.001000, mv_avg_loss: 47.077988, loss: 44.225578
18:06:52.000357 Step[5700/6995], lr: 0.001000, mv_avg_loss: 51.871201, loss: 45.388664
18:06:56.755782 Step[5800/6995], lr: 0.001000, mv_avg_loss: 58.050018, loss: 61.055172
18:07:01.512646 Step[5900/6995], lr: 0.001000, mv_avg_loss: 47.667553, loss: 50.719311
18:07:06.266479 Step[6000/6995], lr: 0.001000, mv_avg_loss: 46.519310, loss: 35.925587
18:07:11.027753 Step[6100/6995], lr: 0.001000, mv_avg_loss: 55.911102, loss: 31.882360
18:07:15.777458 Step[6200/6995], lr: 0.001000, mv_avg_loss: 48.820484, loss: 41.694878
18:07:20.508433 Step[6300/6995], lr: 0.001000, mv_avg_loss: 54.560371, loss: 47.377113
18:07:25.245514 Step[6400/6995], lr: 0.001000, mv_avg_loss: 52.893684, loss: 46.717316
18:07:29.996748 Step[6500/6995], lr: 0.001000, mv_avg_loss: 50.462933, loss: 74.961121
18:07:34.745010 Step[6600/6995], lr: 0.001000, mv_avg_loss: 60.289734, loss: 49.318840
18:07:39.492744 Step[6700/6995], lr: 0.001000, mv_avg_loss: 47.632362, loss: 52.148071
18:07:44.227899 Step[6800/6995], lr: 0.001000, mv_avg_loss: 56.283745, loss: 49.338268
18:07:48.962412 Step[6900/6995], lr: 0.001000, mv_avg_loss: 59.144299, loss: 51.540192
Testing for epoch: 12
Average test PNSR is 25.144157 for 500 images
Start to train epoch 13
18:08:16.650489 Step[0/6995], lr: 0.001000, mv_avg_loss: 53.620647, loss: 29.303526
18:08:21.390742 Step[100/6995], lr: 0.001000, mv_avg_loss: 44.204918, loss: 39.236969
18:08:26.114397 Step[200/6995], lr: 0.001000, mv_avg_loss: 56.512245, loss: 34.780682
18:08:30.835608 Step[300/6995], lr: 0.001000, mv_avg_loss: 54.066589, loss: 47.811012
18:08:35.575606 Step[400/6995], lr: 0.001000, mv_avg_loss: 60.710674, loss: 78.976021
18:08:40.308939 Step[500/6995], lr: 0.001000, mv_avg_loss: 58.847572, loss: 74.714348
18:08:45.029588 Step[600/6995], lr: 0.001000, mv_avg_loss: 60.528324, loss: 55.095779
18:08:49.763098 Step[700/6995], lr: 0.001000, mv_avg_loss: 52.024700, loss: 35.009342
18:08:54.503724 Step[800/6995], lr: 0.001000, mv_avg_loss: 49.303120, loss: 28.693558
18:08:59.229948 Step[900/6995], lr: 0.001000, mv_avg_loss: 57.876911, loss: 59.322556
18:09:03.970904 Step[1000/6995], lr: 0.001000, mv_avg_loss: 47.704338, loss: 34.919781
18:09:08.729035 Step[1100/6995], lr: 0.001000, mv_avg_loss: 52.601799, loss: 34.893002
18:09:13.470544 Step[1200/6995], lr: 0.001000, mv_avg_loss: 54.650566, loss: 55.650112
18:09:18.218432 Step[1300/6995], lr: 0.001000, mv_avg_loss: 52.118130, loss: 39.523811
18:09:22.952762 Step[1400/6995], lr: 0.001000, mv_avg_loss: 53.059341, loss: 57.191895
18:09:27.697916 Step[1500/6995], lr: 0.001000, mv_avg_loss: 50.763695, loss: 44.119156
18:09:32.473448 Step[1600/6995], lr: 0.001000, mv_avg_loss: 50.830082, loss: 69.639473
18:09:37.209240 Step[1700/6995], lr: 0.001000, mv_avg_loss: 54.554428, loss: 41.565212
18:09:41.951122 Step[1800/6995], lr: 0.001000, mv_avg_loss: 47.846912, loss: 46.117088
18:09:46.698977 Step[1900/6995], lr: 0.001000, mv_avg_loss: 52.180893, loss: 71.381638
18:09:51.457607 Step[2000/6995], lr: 0.001000, mv_avg_loss: 49.026264, loss: 40.655746
18:09:56.200495 Step[2100/6995], lr: 0.001000, mv_avg_loss: 47.029858, loss: 31.364738
18:10:00.948789 Step[2200/6995], lr: 0.001000, mv_avg_loss: 44.289623, loss: 21.053936
18:10:05.704092 Step[2300/6995], lr: 0.001000, mv_avg_loss: 50.738926, loss: 66.719856
18:10:10.446022 Step[2400/6995], lr: 0.001000, mv_avg_loss: 48.450901, loss: 48.341991
18:10:15.182548 Step[2500/6995], lr: 0.001000, mv_avg_loss: 54.402020, loss: 68.104485
18:10:19.934217 Step[2600/6995], lr: 0.001000, mv_avg_loss: 53.930405, loss: 65.917122
18:10:24.688281 Step[2700/6995], lr: 0.001000, mv_avg_loss: 49.902977, loss: 52.655071
18:10:29.432826 Step[2800/6995], lr: 0.001000, mv_avg_loss: 65.706917, loss: 58.886520
18:10:34.182221 Step[2900/6995], lr: 0.001000, mv_avg_loss: 55.038525, loss: 36.862984
18:10:38.940938 Step[3000/6995], lr: 0.001000, mv_avg_loss: 54.483688, loss: 37.043533
18:10:43.692947 Step[3100/6995], lr: 0.001000, mv_avg_loss: 54.360775, loss: 49.161663
18:10:48.477644 Step[3200/6995], lr: 0.001000, mv_avg_loss: 48.538578, loss: 22.616962
18:10:53.240262 Step[3300/6995], lr: 0.001000, mv_avg_loss: 52.673176, loss: 48.173080
18:10:58.000178 Step[3400/6995], lr: 0.001000, mv_avg_loss: 51.395798, loss: 36.659752
18:11:02.755473 Step[3500/6995], lr: 0.001000, mv_avg_loss: 49.330742, loss: 31.506008
18:11:07.521550 Step[3600/6995], lr: 0.001000, mv_avg_loss: 53.692673, loss: 46.409573
18:11:12.269824 Step[3700/6995], lr: 0.001000, mv_avg_loss: 54.738228, loss: 71.396759
18:11:17.028703 Step[3800/6995], lr: 0.001000, mv_avg_loss: 43.277676, loss: 30.313793
18:11:21.790260 Step[3900/6995], lr: 0.001000, mv_avg_loss: 50.033848, loss: 55.081429
18:11:26.554598 Step[4000/6995], lr: 0.001000, mv_avg_loss: 47.889996, loss: 28.849136
18:11:31.336425 Step[4100/6995], lr: 0.001000, mv_avg_loss: 61.130898, loss: 60.527489
18:11:36.092259 Step[4200/6995], lr: 0.001000, mv_avg_loss: 57.418743, loss: 45.549187
18:11:40.850194 Step[4300/6995], lr: 0.001000, mv_avg_loss: 59.377876, loss: 41.310417
18:11:45.612330 Step[4400/6995], lr: 0.001000, mv_avg_loss: 48.024517, loss: 30.174818
18:11:50.385228 Step[4500/6995], lr: 0.001000, mv_avg_loss: 47.783401, loss: 80.702133
18:11:55.148060 Step[4600/6995], lr: 0.001000, mv_avg_loss: 45.759506, loss: 38.242676
18:11:59.908610 Step[4700/6995], lr: 0.001000, mv_avg_loss: 51.319012, loss: 84.355690
18:12:04.680154 Step[4800/6995], lr: 0.001000, mv_avg_loss: 52.196430, loss: 76.900543
18:12:09.445844 Step[4900/6995], lr: 0.001000, mv_avg_loss: 51.158260, loss: 76.030106
18:12:14.208624 Step[5000/6995], lr: 0.001000, mv_avg_loss: 51.748840, loss: 37.597237
18:12:18.972987 Step[5100/6995], lr: 0.001000, mv_avg_loss: 49.539566, loss: 37.569595
18:12:23.736324 Step[5200/6995], lr: 0.001000, mv_avg_loss: 49.723701, loss: 38.266365
18:12:28.493132 Step[5300/6995], lr: 0.001000, mv_avg_loss: 52.753616, loss: 43.572155
18:12:33.254796 Step[5400/6995], lr: 0.001000, mv_avg_loss: 51.340542, loss: 56.257828
18:12:38.015061 Step[5500/6995], lr: 0.001000, mv_avg_loss: 53.658119, loss: 70.590096
18:12:42.778534 Step[5600/6995], lr: 0.001000, mv_avg_loss: 51.921196, loss: 39.180645
18:12:47.532386 Step[5700/6995], lr: 0.001000, mv_avg_loss: 56.854374, loss: 40.924789
18:12:52.291089 Step[5800/6995], lr: 0.001000, mv_avg_loss: 50.406414, loss: 68.362183
18:12:57.045745 Step[5900/6995], lr: 0.001000, mv_avg_loss: 52.072823, loss: 45.025482
18:13:01.821885 Step[6000/6995], lr: 0.001000, mv_avg_loss: 47.571526, loss: 40.115868
18:13:06.588691 Step[6100/6995], lr: 0.001000, mv_avg_loss: 48.677341, loss: 73.702057
18:13:11.362392 Step[6200/6995], lr: 0.001000, mv_avg_loss: 52.677200, loss: 59.426353
18:13:16.144300 Step[6300/6995], lr: 0.001000, mv_avg_loss: 58.309456, loss: 42.191063
18:13:20.909013 Step[6400/6995], lr: 0.001000, mv_avg_loss: 51.749134, loss: 38.923401
18:13:25.682744 Step[6500/6995], lr: 0.001000, mv_avg_loss: 52.595554, loss: 51.264202
18:13:30.448346 Step[6600/6995], lr: 0.001000, mv_avg_loss: 52.545071, loss: 43.469006
18:13:35.222659 Step[6700/6995], lr: 0.001000, mv_avg_loss: 56.785564, loss: 59.058208
18:13:39.993540 Step[6800/6995], lr: 0.001000, mv_avg_loss: 52.011868, loss: 64.241272
18:13:44.769859 Step[6900/6995], lr: 0.001000, mv_avg_loss: 55.304131, loss: 102.625816
Testing for epoch: 13
Average test PNSR is 24.766230 for 500 images
Start to train epoch 14
18:14:10.205136 Step[0/6995], lr: 0.001000, mv_avg_loss: 52.554287, loss: 38.845764
18:14:14.967048 Step[100/6995], lr: 0.001000, mv_avg_loss: 47.590080, loss: 54.541954
18:14:19.760456 Step[200/6995], lr: 0.001000, mv_avg_loss: 45.412930, loss: 39.765404
18:14:24.522547 Step[300/6995], lr: 0.001000, mv_avg_loss: 49.628975, loss: 38.134529
18:14:29.302351 Step[400/6995], lr: 0.001000, mv_avg_loss: 44.552795, loss: 48.001511
18:14:34.065298 Step[500/6995], lr: 0.001000, mv_avg_loss: 45.882160, loss: 39.502281
18:14:38.819816 Step[600/6995], lr: 0.001000, mv_avg_loss: 42.606884, loss: 41.949539
18:14:43.604012 Step[700/6995], lr: 0.001000, mv_avg_loss: 42.602234, loss: 29.807808
18:14:48.366855 Step[800/6995], lr: 0.001000, mv_avg_loss: 51.662399, loss: 34.022671
18:14:53.148591 Step[900/6995], lr: 0.001000, mv_avg_loss: 52.586098, loss: 44.036003
18:14:57.919503 Step[1000/6995], lr: 0.001000, mv_avg_loss: 51.106621, loss: 65.524345
18:15:02.686458 Step[1100/6995], lr: 0.001000, mv_avg_loss: 45.511902, loss: 51.225121
18:15:07.453707 Step[1200/6995], lr: 0.001000, mv_avg_loss: 45.178005, loss: 63.184566
18:15:12.211561 Step[1300/6995], lr: 0.001000, mv_avg_loss: 50.135399, loss: 39.361938
18:15:16.971951 Step[1400/6995], lr: 0.001000, mv_avg_loss: 48.422443, loss: 42.701508
18:15:21.740926 Step[1500/6995], lr: 0.001000, mv_avg_loss: 59.929363, loss: 66.198029
18:15:26.496584 Step[1600/6995], lr: 0.001000, mv_avg_loss: 49.355591, loss: 53.843964
18:15:31.273039 Step[1700/6995], lr: 0.001000, mv_avg_loss: 49.741959, loss: 36.331505
18:15:36.036428 Step[1800/6995], lr: 0.001000, mv_avg_loss: 55.010098, loss: 57.802399
18:15:40.811085 Step[1900/6995], lr: 0.001000, mv_avg_loss: 50.207203, loss: 41.073036
18:15:45.575209 Step[2000/6995], lr: 0.001000, mv_avg_loss: 53.173054, loss: 42.733894
18:15:50.356284 Step[2100/6995], lr: 0.001000, mv_avg_loss: 47.591507, loss: 31.451895
18:15:55.119447 Step[2200/6995], lr: 0.001000, mv_avg_loss: 45.015739, loss: 34.010887
18:15:59.888651 Step[2300/6995], lr: 0.001000, mv_avg_loss: 58.162693, loss: 56.860847
18:16:04.647367 Step[2400/6995], lr: 0.001000, mv_avg_loss: 50.596836, loss: 37.675068
18:16:09.406256 Step[2500/6995], lr: 0.001000, mv_avg_loss: 51.658733, loss: 27.048302
18:16:14.179328 Step[2600/6995], lr: 0.001000, mv_avg_loss: 53.673759, loss: 65.001846
18:16:18.947737 Step[2700/6995], lr: 0.001000, mv_avg_loss: 47.135403, loss: 45.968201
18:16:23.711895 Step[2800/6995], lr: 0.001000, mv_avg_loss: 42.216236, loss: 46.213879
18:16:28.473819 Step[2900/6995], lr: 0.001000, mv_avg_loss: 49.733128, loss: 72.456360
18:16:33.230757 Step[3000/6995], lr: 0.001000, mv_avg_loss: 47.663776, loss: 35.855026
18:16:37.991880 Step[3100/6995], lr: 0.001000, mv_avg_loss: 49.421436, loss: 44.429741
18:16:42.751062 Step[3200/6995], lr: 0.001000, mv_avg_loss: 50.537167, loss: 54.257004
18:16:47.526963 Step[3300/6995], lr: 0.001000, mv_avg_loss: 47.441975, loss: 39.227451
18:16:52.292484 Step[3400/6995], lr: 0.001000, mv_avg_loss: 50.913990, loss: 45.915802
18:16:57.053645 Step[3500/6995], lr: 0.001000, mv_avg_loss: 49.991718, loss: 55.607300
18:17:01.815163 Step[3600/6995], lr: 0.001000, mv_avg_loss: 52.334675, loss: 42.192883
18:17:06.591674 Step[3700/6995], lr: 0.001000, mv_avg_loss: 51.094315, loss: 67.298241
18:17:11.350804 Step[3800/6995], lr: 0.001000, mv_avg_loss: 46.907860, loss: 33.588524
18:17:16.122000 Step[3900/6995], lr: 0.001000, mv_avg_loss: 45.807606, loss: 48.337761
18:17:20.877522 Step[4000/6995], lr: 0.001000, mv_avg_loss: 49.016277, loss: 55.606335
18:17:25.637002 Step[4100/6995], lr: 0.001000, mv_avg_loss: 41.989143, loss: 33.217312
18:17:30.412498 Step[4200/6995], lr: 0.001000, mv_avg_loss: 58.696865, loss: 123.477585
18:17:35.194990 Step[4300/6995], lr: 0.001000, mv_avg_loss: 49.691292, loss: 37.860752
18:17:39.973601 Step[4400/6995], lr: 0.001000, mv_avg_loss: 50.237118, loss: 52.477043
18:17:44.741442 Step[4500/6995], lr: 0.001000, mv_avg_loss: 48.445599, loss: 89.983727
18:17:49.501726 Step[4600/6995], lr: 0.001000, mv_avg_loss: 44.350857, loss: 39.467346
18:17:54.286208 Step[4700/6995], lr: 0.001000, mv_avg_loss: 51.219456, loss: 62.960716
18:17:59.056793 Step[4800/6995], lr: 0.001000, mv_avg_loss: 46.546047, loss: 45.827171
18:18:03.811782 Step[4900/6995], lr: 0.001000, mv_avg_loss: 52.721317, loss: 47.924652
18:18:08.584969 Step[5000/6995], lr: 0.001000, mv_avg_loss: 47.581455, loss: 68.666283
18:18:13.342922 Step[5100/6995], lr: 0.001000, mv_avg_loss: 48.243164, loss: 38.869064
18:18:18.108698 Step[5200/6995], lr: 0.001000, mv_avg_loss: 47.191376, loss: 43.023941
18:18:22.858589 Step[5300/6995], lr: 0.001000, mv_avg_loss: 48.249168, loss: 27.738754
18:18:27.618289 Step[5400/6995], lr: 0.001000, mv_avg_loss: 59.013084, loss: 66.819420
18:18:32.393420 Step[5500/6995], lr: 0.001000, mv_avg_loss: 47.530052, loss: 26.564871
18:18:37.164722 Step[5600/6995], lr: 0.001000, mv_avg_loss: 43.718899, loss: 40.402184
18:18:41.932756 Step[5700/6995], lr: 0.001000, mv_avg_loss: 46.911018, loss: 34.392879
18:18:46.703030 Step[5800/6995], lr: 0.001000, mv_avg_loss: 45.617577, loss: 62.836044
18:18:51.455233 Step[5900/6995], lr: 0.001000, mv_avg_loss: 43.068436, loss: 31.397545
18:18:56.216041 Step[6000/6995], lr: 0.001000, mv_avg_loss: 51.998314, loss: 84.388504
18:19:00.971342 Step[6100/6995], lr: 0.001000, mv_avg_loss: 41.141441, loss: 50.279007
18:19:05.713496 Step[6200/6995], lr: 0.001000, mv_avg_loss: 49.599735, loss: 30.671469
18:19:10.463869 Step[6300/6995], lr: 0.001000, mv_avg_loss: 53.716866, loss: 64.801926
18:19:15.210678 Step[6400/6995], lr: 0.001000, mv_avg_loss: 48.562290, loss: 38.378204
18:19:19.965639 Step[6500/6995], lr: 0.001000, mv_avg_loss: 48.633095, loss: 27.058369
18:19:24.716435 Step[6600/6995], lr: 0.001000, mv_avg_loss: 51.270630, loss: 101.868980
18:19:29.472028 Step[6700/6995], lr: 0.001000, mv_avg_loss: 46.507240, loss: 47.215897
18:19:34.218990 Step[6800/6995], lr: 0.001000, mv_avg_loss: 46.655594, loss: 53.806137
18:19:38.961157 Step[6900/6995], lr: 0.001000, mv_avg_loss: 50.244663, loss: 51.371834
saving model for epoch 14
Testing for epoch: 14
Average test PNSR is 24.941186 for 500 images
Start to train epoch 15
18:20:04.336841 Step[0/6995], lr: 0.001000, mv_avg_loss: 46.661797, loss: 36.688931
18:20:09.092929 Step[100/6995], lr: 0.001000, mv_avg_loss: 46.404469, loss: 33.889923
18:20:13.835113 Step[200/6995], lr: 0.001000, mv_avg_loss: 47.733826, loss: 58.867920
18:20:18.571502 Step[300/6995], lr: 0.001000, mv_avg_loss: 44.076817, loss: 42.714378
18:20:23.311138 Step[400/6995], lr: 0.001000, mv_avg_loss: 45.651794, loss: 45.770302
18:20:28.041912 Step[500/6995], lr: 0.001000, mv_avg_loss: 39.621616, loss: 52.312405
18:20:32.774232 Step[600/6995], lr: 0.001000, mv_avg_loss: 41.603676, loss: 43.440407
18:20:37.510698 Step[700/6995], lr: 0.001000, mv_avg_loss: 46.718060, loss: 32.657906
18:20:42.249391 Step[800/6995], lr: 0.001000, mv_avg_loss: 49.747913, loss: 28.565445
18:20:46.988933 Step[900/6995], lr: 0.001000, mv_avg_loss: 45.245399, loss: 26.089554
18:20:51.721827 Step[1000/6995], lr: 0.001000, mv_avg_loss: 45.126350, loss: 72.826920
18:20:56.465814 Step[1100/6995], lr: 0.001000, mv_avg_loss: 47.513535, loss: 34.921829
18:21:01.206320 Step[1200/6995], lr: 0.001000, mv_avg_loss: 44.678501, loss: 28.624105
18:21:05.955772 Step[1300/6995], lr: 0.001000, mv_avg_loss: 47.458920, loss: 26.929977
18:21:10.696650 Step[1400/6995], lr: 0.001000, mv_avg_loss: 47.300575, loss: 59.211838
18:21:15.419645 Step[1500/6995], lr: 0.001000, mv_avg_loss: 47.425919, loss: 40.246460
18:21:20.156362 Step[1600/6995], lr: 0.001000, mv_avg_loss: 42.573589, loss: 40.120041
18:21:24.894437 Step[1700/6995], lr: 0.001000, mv_avg_loss: 43.437840, loss: 83.641479
18:21:29.618363 Step[1800/6995], lr: 0.001000, mv_avg_loss: 46.327415, loss: 36.757229
18:21:34.345721 Step[1900/6995], lr: 0.001000, mv_avg_loss: 45.708618, loss: 25.464317
18:21:39.078565 Step[2000/6995], lr: 0.001000, mv_avg_loss: 44.837860, loss: 48.184460
18:21:43.816981 Step[2100/6995], lr: 0.001000, mv_avg_loss: 43.176113, loss: 36.660736
18:21:48.560066 Step[2200/6995], lr: 0.001000, mv_avg_loss: 50.471348, loss: 24.999268
18:21:53.299136 Step[2300/6995], lr: 0.001000, mv_avg_loss: 48.315754, loss: 44.871429
18:21:58.043438 Step[2400/6995], lr: 0.001000, mv_avg_loss: 49.119122, loss: 55.397301
18:22:02.791863 Step[2500/6995], lr: 0.001000, mv_avg_loss: 50.770752, loss: 30.069134
18:22:07.528444 Step[2600/6995], lr: 0.001000, mv_avg_loss: 46.014606, loss: 36.076859
18:22:12.259603 Step[2700/6995], lr: 0.001000, mv_avg_loss: 89.370102, loss: 85.201157
18:22:16.991224 Step[2800/6995], lr: 0.001000, mv_avg_loss: 55.994755, loss: 50.870018
18:22:21.727655 Step[2900/6995], lr: 0.001000, mv_avg_loss: 53.065971, loss: 45.127422
18:22:26.476407 Step[3000/6995], lr: 0.001000, mv_avg_loss: 50.202534, loss: 57.247871
18:22:31.220889 Step[3100/6995], lr: 0.001000, mv_avg_loss: 55.785908, loss: 41.741348
18:22:35.952916 Step[3200/6995], lr: 0.001000, mv_avg_loss: 44.721661, loss: 47.616333
18:22:40.686309 Step[3300/6995], lr: 0.001000, mv_avg_loss: 47.421936, loss: 71.804482
18:22:45.418527 Step[3400/6995], lr: 0.001000, mv_avg_loss: 43.698792, loss: 47.458660
18:22:50.146168 Step[3500/6995], lr: 0.001000, mv_avg_loss: 43.746250, loss: 91.972069
18:22:54.898737 Step[3600/6995], lr: 0.001000, mv_avg_loss: 45.066673, loss: 44.912430
18:22:59.642116 Step[3700/6995], lr: 0.001000, mv_avg_loss: 41.888512, loss: 33.608559
18:23:04.381860 Step[3800/6995], lr: 0.001000, mv_avg_loss: 42.724907, loss: 44.765270
18:23:09.123358 Step[3900/6995], lr: 0.001000, mv_avg_loss: 47.674908, loss: 42.760063
18:23:13.864687 Step[4000/6995], lr: 0.001000, mv_avg_loss: 51.700176, loss: 34.127754
18:23:18.598347 Step[4100/6995], lr: 0.001000, mv_avg_loss: 47.660809, loss: 56.052395
18:23:23.341811 Step[4200/6995], lr: 0.001000, mv_avg_loss: 46.471462, loss: 53.687660
18:23:28.078892 Step[4300/6995], lr: 0.001000, mv_avg_loss: 42.207256, loss: 34.226601
18:23:32.816984 Step[4400/6995], lr: 0.001000, mv_avg_loss: 42.301929, loss: 35.328178
18:23:37.557436 Step[4500/6995], lr: 0.001000, mv_avg_loss: 41.985287, loss: 32.753120
18:23:42.304358 Step[4600/6995], lr: 0.001000, mv_avg_loss: 42.607407, loss: 36.314861
18:23:47.064423 Step[4700/6995], lr: 0.001000, mv_avg_loss: 43.524300, loss: 18.686581
18:23:51.795079 Step[4800/6995], lr: 0.001000, mv_avg_loss: 41.502918, loss: 57.833508
18:23:56.544625 Step[4900/6995], lr: 0.001000, mv_avg_loss: 50.699802, loss: 34.092205
18:24:01.282500 Step[5000/6995], lr: 0.001000, mv_avg_loss: 43.974800, loss: 32.759026
18:24:06.052462 Step[5100/6995], lr: 0.001000, mv_avg_loss: 46.327557, loss: 61.047588
18:24:10.794516 Step[5200/6995], lr: 0.001000, mv_avg_loss: 48.494995, loss: 59.552685
18:24:15.536948 Step[5300/6995], lr: 0.001000, mv_avg_loss: 43.297939, loss: 41.978691
18:24:20.291233 Step[5400/6995], lr: 0.001000, mv_avg_loss: 46.848511, loss: 41.098125
18:24:25.034334 Step[5500/6995], lr: 0.001000, mv_avg_loss: 51.347729, loss: 32.166351
18:24:29.773468 Step[5600/6995], lr: 0.001000, mv_avg_loss: 49.421429, loss: 37.974941
18:24:34.532677 Step[5700/6995], lr: 0.001000, mv_avg_loss: 47.814533, loss: 66.832306
18:24:39.277212 Step[5800/6995], lr: 0.001000, mv_avg_loss: 47.450722, loss: 67.928467
18:24:44.019692 Step[5900/6995], lr: 0.001000, mv_avg_loss: 48.027264, loss: 41.107872
18:24:48.761306 Step[6000/6995], lr: 0.001000, mv_avg_loss: 48.554050, loss: 45.694145
18:24:53.508064 Step[6100/6995], lr: 0.001000, mv_avg_loss: 46.029842, loss: 40.693741
18:24:58.244921 Step[6200/6995], lr: 0.001000, mv_avg_loss: 46.404011, loss: 26.865126
18:25:03.012062 Step[6300/6995], lr: 0.001000, mv_avg_loss: 44.186825, loss: 33.208416
18:25:07.770192 Step[6400/6995], lr: 0.001000, mv_avg_loss: 45.471916, loss: 47.577862
18:25:12.529099 Step[6500/6995], lr: 0.001000, mv_avg_loss: 45.245075, loss: 61.527828
18:25:17.291051 Step[6600/6995], lr: 0.001000, mv_avg_loss: 41.830837, loss: 31.533230
18:25:22.037931 Step[6700/6995], lr: 0.001000, mv_avg_loss: 44.556099, loss: 28.650818
18:25:26.793619 Step[6800/6995], lr: 0.001000, mv_avg_loss: 40.209229, loss: 59.301098
18:25:31.543804 Step[6900/6995], lr: 0.001000, mv_avg_loss: 55.774860, loss: 40.535572
Testing for epoch: 15
Average test PNSR is 24.810111 for 500 images
Start to train epoch 16
18:25:57.665103 Step[0/6995], lr: 0.001000, mv_avg_loss: 45.581577, loss: 79.569321
18:26:02.416540 Step[100/6995], lr: 0.001000, mv_avg_loss: 55.444466, loss: 66.249832
18:26:07.166192 Step[200/6995], lr: 0.001000, mv_avg_loss: 49.887653, loss: 78.689789
18:26:11.933655 Step[300/6995], lr: 0.001000, mv_avg_loss: 42.410591, loss: 36.556595
18:26:16.692328 Step[400/6995], lr: 0.001000, mv_avg_loss: 49.587021, loss: 58.015106
18:26:21.452983 Step[500/6995], lr: 0.001000, mv_avg_loss: 41.570683, loss: 48.011894
18:26:26.198808 Step[600/6995], lr: 0.001000, mv_avg_loss: 38.155045, loss: 50.531914
18:26:30.972334 Step[700/6995], lr: 0.001000, mv_avg_loss: 47.207542, loss: 32.536633
18:26:35.741840 Step[800/6995], lr: 0.001000, mv_avg_loss: 41.045780, loss: 69.815491
18:26:40.505544 Step[900/6995], lr: 0.001000, mv_avg_loss: 42.347626, loss: 47.217789
18:26:45.274722 Step[1000/6995], lr: 0.001000, mv_avg_loss: 44.505867, loss: 40.069462
18:26:50.026511 Step[1100/6995], lr: 0.001000, mv_avg_loss: 41.900589, loss: 21.158470
18:26:54.775767 Step[1200/6995], lr: 0.001000, mv_avg_loss: 47.357693, loss: 73.464378
18:26:59.536647 Step[1300/6995], lr: 0.001000, mv_avg_loss: 44.391350, loss: 39.469460
18:27:04.290539 Step[1400/6995], lr: 0.001000, mv_avg_loss: 39.802097, loss: 26.464293
18:27:09.059060 Step[1500/6995], lr: 0.001000, mv_avg_loss: 46.088242, loss: 28.111647
18:27:13.823327 Step[1600/6995], lr: 0.001000, mv_avg_loss: 43.482483, loss: 36.426067
18:27:18.584086 Step[1700/6995], lr: 0.001000, mv_avg_loss: 48.227520, loss: 133.289597
18:27:23.344048 Step[1800/6995], lr: 0.001000, mv_avg_loss: 43.725758, loss: 52.648785
18:27:28.119797 Step[1900/6995], lr: 0.001000, mv_avg_loss: 42.436401, loss: 69.171188
18:27:32.887844 Step[2000/6995], lr: 0.001000, mv_avg_loss: 36.787769, loss: 66.213188
18:27:37.675576 Step[2100/6995], lr: 0.001000, mv_avg_loss: 44.314388, loss: 66.290298
18:27:42.450744 Step[2200/6995], lr: 0.001000, mv_avg_loss: 40.669735, loss: 28.114294
18:27:47.232379 Step[2300/6995], lr: 0.001000, mv_avg_loss: 46.764271, loss: 61.317932
18:27:52.001183 Step[2400/6995], lr: 0.001000, mv_avg_loss: 47.693256, loss: 28.316498
18:27:56.761755 Step[2500/6995], lr: 0.001000, mv_avg_loss: 45.780170, loss: 38.426823
18:28:01.520943 Step[2600/6995], lr: 0.001000, mv_avg_loss: 45.014423, loss: 27.240133
18:28:06.295617 Step[2700/6995], lr: 0.001000, mv_avg_loss: 45.031086, loss: 31.712883
18:28:11.084980 Step[2800/6995], lr: 0.001000, mv_avg_loss: 49.691910, loss: 35.886574
18:28:15.862349 Step[2900/6995], lr: 0.001000, mv_avg_loss: 45.838085, loss: 45.597015
18:28:20.653410 Step[3000/6995], lr: 0.001000, mv_avg_loss: 52.354172, loss: 48.504364
18:28:25.446878 Step[3100/6995], lr: 0.001000, mv_avg_loss: 48.437943, loss: 22.665508
18:28:30.213124 Step[3200/6995], lr: 0.001000, mv_avg_loss: 43.220814, loss: 36.312988
18:28:35.007115 Step[3300/6995], lr: 0.001000, mv_avg_loss: 43.357960, loss: 56.087566
18:28:39.784775 Step[3400/6995], lr: 0.001000, mv_avg_loss: 40.447876, loss: 23.076321
18:28:44.552813 Step[3500/6995], lr: 0.001000, mv_avg_loss: 41.267967, loss: 37.926785
18:28:49.330107 Step[3600/6995], lr: 0.001000, mv_avg_loss: 44.963013, loss: 49.086605
18:28:54.113401 Step[3700/6995], lr: 0.001000, mv_avg_loss: 42.626263, loss: 44.228725
18:28:58.886506 Step[3800/6995], lr: 0.001000, mv_avg_loss: 45.585354, loss: 56.359005
18:29:03.663279 Step[3900/6995], lr: 0.001000, mv_avg_loss: 44.212807, loss: 34.196762
18:29:08.440547 Step[4000/6995], lr: 0.001000, mv_avg_loss: 45.123386, loss: 34.041100
18:29:13.223363 Step[4100/6995], lr: 0.001000, mv_avg_loss: 45.581291, loss: 64.430161
18:29:18.007054 Step[4200/6995], lr: 0.001000, mv_avg_loss: 44.282349, loss: 55.995434
18:29:22.775417 Step[4300/6995], lr: 0.001000, mv_avg_loss: 45.321121, loss: 31.765234
18:29:27.554776 Step[4400/6995], lr: 0.001000, mv_avg_loss: 44.057621, loss: 33.022778
18:29:32.323421 Step[4500/6995], lr: 0.001000, mv_avg_loss: 44.466633, loss: 34.503021
18:29:37.087450 Step[4600/6995], lr: 0.001000, mv_avg_loss: 47.194687, loss: 48.349289
18:29:41.884394 Step[4700/6995], lr: 0.001000, mv_avg_loss: 41.892544, loss: 50.188377
18:29:46.662952 Step[4800/6995], lr: 0.001000, mv_avg_loss: 50.079685, loss: 83.722237
18:29:51.431998 Step[4900/6995], lr: 0.001000, mv_avg_loss: 48.190643, loss: 44.669792
18:29:56.200492 Step[5000/6995], lr: 0.001000, mv_avg_loss: 50.149475, loss: 42.151989
18:30:00.961836 Step[5100/6995], lr: 0.001000, mv_avg_loss: 44.283287, loss: 47.938824
18:30:05.720046 Step[5200/6995], lr: 0.001000, mv_avg_loss: 41.344955, loss: 62.376019
18:30:10.489114 Step[5300/6995], lr: 0.001000, mv_avg_loss: 36.886940, loss: 60.010265
18:30:15.280856 Step[5400/6995], lr: 0.001000, mv_avg_loss: 42.969463, loss: 42.506401
18:30:20.034420 Step[5500/6995], lr: 0.001000, mv_avg_loss: 40.495239, loss: 48.357861
18:30:24.813593 Step[5600/6995], lr: 0.001000, mv_avg_loss: 41.519131, loss: 34.883186
18:30:29.589723 Step[5700/6995], lr: 0.001000, mv_avg_loss: 41.469868, loss: 23.804539
18:30:34.370525 Step[5800/6995], lr: 0.001000, mv_avg_loss: 52.850189, loss: 34.244026
18:30:39.118702 Step[5900/6995], lr: 0.001000, mv_avg_loss: 41.407829, loss: 35.200027
18:30:43.890571 Step[6000/6995], lr: 0.001000, mv_avg_loss: 44.440647, loss: 26.727390
18:30:48.661905 Step[6100/6995], lr: 0.001000, mv_avg_loss: 41.267971, loss: 46.558178
18:30:53.435262 Step[6200/6995], lr: 0.001000, mv_avg_loss: 40.782528, loss: 42.924221
18:30:58.195301 Step[6300/6995], lr: 0.001000, mv_avg_loss: 39.191555, loss: 42.886120
18:31:03.083484 Step[6400/6995], lr: 0.001000, mv_avg_loss: 43.080956, loss: 41.097549
18:31:07.852207 Step[6500/6995], lr: 0.001000, mv_avg_loss: 43.634212, loss: 27.198700
18:31:12.602395 Step[6600/6995], lr: 0.001000, mv_avg_loss: 51.106850, loss: 32.281998
18:31:17.362510 Step[6700/6995], lr: 0.001000, mv_avg_loss: 42.506756, loss: 49.714062
18:31:22.134972 Step[6800/6995], lr: 0.001000, mv_avg_loss: 45.760815, loss: 36.361385
18:31:26.913454 Step[6900/6995], lr: 0.001000, mv_avg_loss: 53.514141, loss: 46.367790
Testing for epoch: 16
Average test PNSR is 25.227691 for 500 images
Start to train epoch 17
18:31:52.770112 Step[0/6995], lr: 0.001000, mv_avg_loss: 46.671303, loss: 31.447430
18:31:57.515901 Step[100/6995], lr: 0.001000, mv_avg_loss: 42.234356, loss: 36.737747
18:32:02.263872 Step[200/6995], lr: 0.001000, mv_avg_loss: 39.578167, loss: 34.704426
18:32:07.016240 Step[300/6995], lr: 0.001000, mv_avg_loss: 41.656063, loss: 38.392296
18:32:11.757343 Step[400/6995], lr: 0.001000, mv_avg_loss: 41.817406, loss: 35.167057
18:32:16.501672 Step[500/6995], lr: 0.001000, mv_avg_loss: 44.353680, loss: 28.194929
18:32:21.250398 Step[600/6995], lr: 0.001000, mv_avg_loss: 35.838692, loss: 39.345032
18:32:26.003452 Step[700/6995], lr: 0.001000, mv_avg_loss: 50.296043, loss: 75.936935
18:32:30.763958 Step[800/6995], lr: 0.001000, mv_avg_loss: 40.389324, loss: 27.798704
18:32:35.514318 Step[900/6995], lr: 0.001000, mv_avg_loss: 41.733421, loss: 43.340874
18:32:40.272612 Step[1000/6995], lr: 0.001000, mv_avg_loss: 44.082031, loss: 31.433670
18:32:45.041053 Step[1100/6995], lr: 0.001000, mv_avg_loss: 40.481766, loss: 29.345383
18:32:49.782946 Step[1200/6995], lr: 0.001000, mv_avg_loss: 44.444756, loss: 43.083252
18:32:54.544047 Step[1300/6995], lr: 0.001000, mv_avg_loss: 44.314751, loss: 71.022125
18:32:59.295867 Step[1400/6995], lr: 0.001000, mv_avg_loss: 38.866188, loss: 33.508064
18:33:04.038971 Step[1500/6995], lr: 0.001000, mv_avg_loss: 43.584805, loss: 30.352552
18:33:08.795711 Step[1600/6995], lr: 0.001000, mv_avg_loss: 40.805294, loss: 35.549835
18:33:13.541553 Step[1700/6995], lr: 0.001000, mv_avg_loss: 42.065159, loss: 49.732864
18:33:18.304908 Step[1800/6995], lr: 0.001000, mv_avg_loss: 38.844677, loss: 58.876263
18:33:23.047736 Step[1900/6995], lr: 0.001000, mv_avg_loss: 46.002098, loss: 55.380749
18:33:27.797947 Step[2000/6995], lr: 0.001000, mv_avg_loss: 45.931534, loss: 47.203106
18:33:32.545647 Step[2100/6995], lr: 0.001000, mv_avg_loss: 44.392216, loss: 25.634253
18:33:37.280471 Step[2200/6995], lr: 0.001000, mv_avg_loss: 42.214226, loss: 31.931730
18:33:42.049096 Step[2300/6995], lr: 0.001000, mv_avg_loss: 46.130527, loss: 74.508087
18:33:46.801188 Step[2400/6995], lr: 0.001000, mv_avg_loss: 43.204842, loss: 44.075684
18:33:51.562000 Step[2500/6995], lr: 0.001000, mv_avg_loss: 45.765388, loss: 41.875675
18:33:56.307962 Step[2600/6995], lr: 0.001000, mv_avg_loss: 42.968372, loss: 36.678787
18:34:01.060910 Step[2700/6995], lr: 0.001000, mv_avg_loss: 44.269348, loss: 33.810066
18:34:05.815028 Step[2800/6995], lr: 0.001000, mv_avg_loss: 44.390976, loss: 35.768707
18:34:10.561050 Step[2900/6995], lr: 0.001000, mv_avg_loss: 44.995262, loss: 42.185982
18:34:15.325063 Step[3000/6995], lr: 0.001000, mv_avg_loss: 43.099834, loss: 57.952797
18:34:20.080263 Step[3100/6995], lr: 0.001000, mv_avg_loss: 46.617172, loss: 35.387177
18:34:24.831959 Step[3200/6995], lr: 0.001000, mv_avg_loss: 39.100517, loss: 47.816860
18:34:29.571590 Step[3300/6995], lr: 0.001000, mv_avg_loss: 44.443752, loss: 36.724697
18:34:34.335711 Step[3400/6995], lr: 0.001000, mv_avg_loss: 42.721523, loss: 51.029232
18:34:39.080636 Step[3500/6995], lr: 0.001000, mv_avg_loss: 41.573765, loss: 41.486595
18:34:43.824478 Step[3600/6995], lr: 0.001000, mv_avg_loss: 40.249645, loss: 67.647537
18:34:48.578333 Step[3700/6995], lr: 0.001000, mv_avg_loss: 39.947273, loss: 43.261101
18:34:53.324340 Step[3800/6995], lr: 0.001000, mv_avg_loss: 36.028774, loss: 41.435020
18:34:58.070556 Step[3900/6995], lr: 0.001000, mv_avg_loss: 42.094173, loss: 27.967787
18:35:02.824126 Step[4000/6995], lr: 0.001000, mv_avg_loss: 43.235180, loss: 32.199799
18:35:07.587783 Step[4100/6995], lr: 0.001000, mv_avg_loss: 36.722916, loss: 28.238134
18:35:12.328026 Step[4200/6995], lr: 0.001000, mv_avg_loss: 41.331146, loss: 41.941906
18:35:17.078374 Step[4300/6995], lr: 0.001000, mv_avg_loss: 42.496693, loss: 32.472198
18:35:21.826510 Step[4400/6995], lr: 0.001000, mv_avg_loss: 33.891354, loss: 39.045082
18:35:26.581513 Step[4500/6995], lr: 0.001000, mv_avg_loss: 46.604450, loss: 57.402237
18:35:31.319247 Step[4600/6995], lr: 0.001000, mv_avg_loss: 40.349743, loss: 40.555500
18:35:36.069449 Step[4700/6995], lr: 0.001000, mv_avg_loss: 45.300251, loss: 37.693184
18:35:40.828139 Step[4800/6995], lr: 0.001000, mv_avg_loss: 42.109760, loss: 25.079037
18:35:45.576290 Step[4900/6995], lr: 0.001000, mv_avg_loss: 38.474724, loss: 25.615437
18:35:50.317864 Step[5000/6995], lr: 0.001000, mv_avg_loss: 44.398510, loss: 43.263607
18:35:55.065722 Step[5100/6995], lr: 0.001000, mv_avg_loss: 38.496948, loss: 33.851501
18:35:59.813875 Step[5200/6995], lr: 0.001000, mv_avg_loss: 45.149937, loss: 51.151527
18:36:04.562743 Step[5300/6995], lr: 0.001000, mv_avg_loss: 42.954063, loss: 41.268429
18:36:09.306320 Step[5400/6995], lr: 0.001000, mv_avg_loss: 40.259365, loss: 25.554718
18:36:14.076898 Step[5500/6995], lr: 0.001000, mv_avg_loss: 38.923828, loss: 60.825176
18:36:18.837602 Step[5600/6995], lr: 0.001000, mv_avg_loss: 45.626678, loss: 55.487335
18:36:23.592139 Step[5700/6995], lr: 0.001000, mv_avg_loss: 49.283707, loss: 63.814827
18:36:28.334920 Step[5800/6995], lr: 0.001000, mv_avg_loss: 54.060955, loss: 56.167580
18:36:33.076607 Step[5900/6995], lr: 0.001000, mv_avg_loss: 42.158695, loss: 58.808331
18:36:37.852119 Step[6000/6995], lr: 0.001000, mv_avg_loss: 45.602562, loss: 60.111099
18:36:42.623788 Step[6100/6995], lr: 0.001000, mv_avg_loss: 42.739502, loss: 31.871700
18:36:47.387820 Step[6200/6995], lr: 0.001000, mv_avg_loss: 45.940937, loss: 35.020512
18:36:52.125801 Step[6300/6995], lr: 0.001000, mv_avg_loss: 53.599632, loss: 46.285461
18:36:56.868805 Step[6400/6995], lr: 0.001000, mv_avg_loss: 41.690666, loss: 55.482372
18:37:01.608529 Step[6500/6995], lr: 0.001000, mv_avg_loss: 38.732769, loss: 28.098286
18:37:06.353969 Step[6600/6995], lr: 0.001000, mv_avg_loss: 43.648731, loss: 61.429489
18:37:11.107056 Step[6700/6995], lr: 0.001000, mv_avg_loss: 48.224834, loss: 29.196423
18:37:15.845779 Step[6800/6995], lr: 0.001000, mv_avg_loss: 44.509758, loss: 54.056854
18:37:20.595577 Step[6900/6995], lr: 0.001000, mv_avg_loss: 40.938351, loss: 50.506500
Testing for epoch: 17
Average test PNSR is 24.824637 for 500 images
Start to train epoch 18
18:37:49.109617 Step[0/6995], lr: 0.001000, mv_avg_loss: 43.503578, loss: 49.378124
18:37:53.836044 Step[100/6995], lr: 0.001000, mv_avg_loss: 40.744556, loss: 35.713486
18:37:58.554548 Step[200/6995], lr: 0.001000, mv_avg_loss: 37.544357, loss: 26.554302
18:38:03.281774 Step[300/6995], lr: 0.001000, mv_avg_loss: 41.521435, loss: 51.537731
18:38:08.004678 Step[400/6995], lr: 0.001000, mv_avg_loss: 37.053337, loss: 33.561554
18:38:12.731682 Step[500/6995], lr: 0.001000, mv_avg_loss: 40.191624, loss: 35.009167
18:38:17.474886 Step[600/6995], lr: 0.001000, mv_avg_loss: 51.576546, loss: 47.431732
18:38:22.202545 Step[700/6995], lr: 0.001000, mv_avg_loss: 42.315178, loss: 28.790928
18:38:26.936691 Step[800/6995], lr: 0.001000, mv_avg_loss: 40.402962, loss: 60.654064
18:38:31.686996 Step[900/6995], lr: 0.001000, mv_avg_loss: 46.674942, loss: 43.857540
18:38:36.421192 Step[1000/6995], lr: 0.001000, mv_avg_loss: 44.312866, loss: 45.683380
18:38:41.157979 Step[1100/6995], lr: 0.001000, mv_avg_loss: 41.531998, loss: 80.977280
18:38:45.895974 Step[1200/6995], lr: 0.001000, mv_avg_loss: 38.550877, loss: 30.088743
18:38:50.644844 Step[1300/6995], lr: 0.001000, mv_avg_loss: 38.161510, loss: 28.300594
18:38:55.377409 Step[1400/6995], lr: 0.001000, mv_avg_loss: 38.728012, loss: 43.863991
18:39:00.130690 Step[1500/6995], lr: 0.001000, mv_avg_loss: 37.333054, loss: 51.667915
18:39:04.868148 Step[1600/6995], lr: 0.001000, mv_avg_loss: 44.725681, loss: 25.578842
18:39:09.618649 Step[1700/6995], lr: 0.001000, mv_avg_loss: 39.529991, loss: 35.874210
18:39:14.358471 Step[1800/6995], lr: 0.001000, mv_avg_loss: 34.893990, loss: 51.098801
18:39:19.118613 Step[1900/6995], lr: 0.001000, mv_avg_loss: 40.265911, loss: 19.606878
18:39:23.886055 Step[2000/6995], lr: 0.001000, mv_avg_loss: 35.273346, loss: 24.395462
18:39:28.644566 Step[2100/6995], lr: 0.001000, mv_avg_loss: 43.279354, loss: 42.365829
18:39:33.387919 Step[2200/6995], lr: 0.001000, mv_avg_loss: 36.676277, loss: 41.510719
18:39:38.145041 Step[2300/6995], lr: 0.001000, mv_avg_loss: 36.873840, loss: 39.036938
18:39:42.888551 Step[2400/6995], lr: 0.001000, mv_avg_loss: 38.278126, loss: 28.532812
18:39:47.651741 Step[2500/6995], lr: 0.001000, mv_avg_loss: 42.310032, loss: 55.301537
18:39:52.396006 Step[2600/6995], lr: 0.001000, mv_avg_loss: 41.906410, loss: 25.570446
18:39:57.150952 Step[2700/6995], lr: 0.001000, mv_avg_loss: 40.512379, loss: 43.798656
18:40:01.921175 Step[2800/6995], lr: 0.001000, mv_avg_loss: 36.947620, loss: 26.883583
18:40:06.676778 Step[2900/6995], lr: 0.001000, mv_avg_loss: 40.502392, loss: 25.522100
18:40:11.419494 Step[3000/6995], lr: 0.001000, mv_avg_loss: 42.267830, loss: 49.778790
18:40:16.177343 Step[3100/6995], lr: 0.001000, mv_avg_loss: 36.467396, loss: 31.444546
18:40:20.933346 Step[3200/6995], lr: 0.001000, mv_avg_loss: 40.273098, loss: 29.693542
18:40:25.689991 Step[3300/6995], lr: 0.001000, mv_avg_loss: 44.568031, loss: 42.477982
18:40:30.455262 Step[3400/6995], lr: 0.001000, mv_avg_loss: 44.204720, loss: 62.503723
18:40:35.214671 Step[3500/6995], lr: 0.001000, mv_avg_loss: 44.962193, loss: 66.402481
18:40:39.960612 Step[3600/6995], lr: 0.001000, mv_avg_loss: 37.118687, loss: 40.710125
18:40:44.735837 Step[3700/6995], lr: 0.001000, mv_avg_loss: 47.247086, loss: 41.162933
18:40:49.481412 Step[3800/6995], lr: 0.001000, mv_avg_loss: 45.160877, loss: 64.160599
18:40:54.226905 Step[3900/6995], lr: 0.001000, mv_avg_loss: 45.493851, loss: 32.148666
18:40:58.988907 Step[4000/6995], lr: 0.001000, mv_avg_loss: 41.106819, loss: 30.959457
18:41:03.767176 Step[4100/6995], lr: 0.001000, mv_avg_loss: 38.576218, loss: 58.360455
18:41:08.527291 Step[4200/6995], lr: 0.001000, mv_avg_loss: 44.096245, loss: 23.778488
18:41:13.295629 Step[4300/6995], lr: 0.001000, mv_avg_loss: 39.030064, loss: 36.950600
18:41:18.048717 Step[4400/6995], lr: 0.001000, mv_avg_loss: 44.116425, loss: 48.890503
18:41:22.799365 Step[4500/6995], lr: 0.001000, mv_avg_loss: 37.234470, loss: 59.565441
18:41:27.573215 Step[4600/6995], lr: 0.001000, mv_avg_loss: 38.718212, loss: 22.378828
18:41:32.346061 Step[4700/6995], lr: 0.001000, mv_avg_loss: 42.389244, loss: 30.149137
18:41:37.107114 Step[4800/6995], lr: 0.001000, mv_avg_loss: 38.368450, loss: 25.124228
18:41:41.870120 Step[4900/6995], lr: 0.001000, mv_avg_loss: 39.688950, loss: 36.377487
18:41:46.653397 Step[5000/6995], lr: 0.001000, mv_avg_loss: 42.665909, loss: 50.586578
18:41:51.405097 Step[5100/6995], lr: 0.001000, mv_avg_loss: 37.051765, loss: 35.339851
18:41:56.182151 Step[5200/6995], lr: 0.001000, mv_avg_loss: 39.173294, loss: 49.637741
18:42:00.940972 Step[5300/6995], lr: 0.001000, mv_avg_loss: 39.655098, loss: 30.304693
18:42:05.711578 Step[5400/6995], lr: 0.001000, mv_avg_loss: 35.636665, loss: 33.177120
18:42:10.476536 Step[5500/6995], lr: 0.001000, mv_avg_loss: 36.529320, loss: 28.827145
18:42:15.254495 Step[5600/6995], lr: 0.001000, mv_avg_loss: 38.196167, loss: 35.071007
18:42:20.021128 Step[5700/6995], lr: 0.001000, mv_avg_loss: 39.940197, loss: 36.121071
18:42:24.785596 Step[5800/6995], lr: 0.001000, mv_avg_loss: 40.880276, loss: 22.337292
18:42:29.558601 Step[5900/6995], lr: 0.001000, mv_avg_loss: 36.549271, loss: 32.547668
18:42:34.338853 Step[6000/6995], lr: 0.001000, mv_avg_loss: 47.064388, loss: 33.170704
18:42:39.091988 Step[6100/6995], lr: 0.001000, mv_avg_loss: 43.540020, loss: 64.842667
18:42:43.867379 Step[6200/6995], lr: 0.001000, mv_avg_loss: 42.677097, loss: 26.824425
18:42:48.631333 Step[6300/6995], lr: 0.001000, mv_avg_loss: 39.239830, loss: 34.127758
18:42:53.389255 Step[6400/6995], lr: 0.001000, mv_avg_loss: 38.013672, loss: 49.180992
18:42:58.165315 Step[6500/6995], lr: 0.001000, mv_avg_loss: 40.266026, loss: 27.708752
18:43:02.941782 Step[6600/6995], lr: 0.001000, mv_avg_loss: 40.831562, loss: 39.073528
18:43:07.720767 Step[6700/6995], lr: 0.001000, mv_avg_loss: 42.705147, loss: 26.295103
18:43:12.485595 Step[6800/6995], lr: 0.001000, mv_avg_loss: 39.519436, loss: 49.991035
18:43:17.254481 Step[6900/6995], lr: 0.001000, mv_avg_loss: 40.402821, loss: 39.407719
Testing for epoch: 18
Average test PNSR is 25.252302 for 500 images
Start to train epoch 19
18:43:42.895576 Step[0/6995], lr: 0.001000, mv_avg_loss: 35.935246, loss: 26.807743
18:43:47.695429 Step[100/6995], lr: 0.001000, mv_avg_loss: 34.509315, loss: 23.425491
18:43:52.460944 Step[200/6995], lr: 0.001000, mv_avg_loss: 38.452297, loss: 32.149330
18:43:57.261733 Step[300/6995], lr: 0.001000, mv_avg_loss: 41.064720, loss: 35.353123
18:44:02.023589 Step[400/6995], lr: 0.001000, mv_avg_loss: 43.647808, loss: 25.835808
18:44:06.775835 Step[500/6995], lr: 0.001000, mv_avg_loss: 34.930862, loss: 37.385376
18:44:11.529031 Step[600/6995], lr: 0.001000, mv_avg_loss: 43.163074, loss: 39.267700
18:44:16.295990 Step[700/6995], lr: 0.001000, mv_avg_loss: 37.379261, loss: 42.597801
18:44:21.069490 Step[800/6995], lr: 0.001000, mv_avg_loss: 38.289646, loss: 33.713287
18:44:25.848311 Step[900/6995], lr: 0.001000, mv_avg_loss: 41.499462, loss: 56.198330
18:44:30.605990 Step[1000/6995], lr: 0.001000, mv_avg_loss: 42.873020, loss: 45.526634
18:44:35.358372 Step[1100/6995], lr: 0.001000, mv_avg_loss: 37.974888, loss: 45.258038
18:44:40.124603 Step[1200/6995], lr: 0.001000, mv_avg_loss: 41.260147, loss: 33.563160
18:44:44.908669 Step[1300/6995], lr: 0.001000, mv_avg_loss: 40.980808, loss: 36.765759
18:44:49.672753 Step[1400/6995], lr: 0.001000, mv_avg_loss: 36.839451, loss: 30.531837
18:44:54.457076 Step[1500/6995], lr: 0.001000, mv_avg_loss: 37.330406, loss: 46.170616
18:44:59.247776 Step[1600/6995], lr: 0.001000, mv_avg_loss: 41.868370, loss: 36.738365
18:45:04.018888 Step[1700/6995], lr: 0.001000, mv_avg_loss: 39.709370, loss: 33.842411
18:45:08.789449 Step[1800/6995], lr: 0.001000, mv_avg_loss: 37.519661, loss: 28.673847
18:45:13.548037 Step[1900/6995], lr: 0.001000, mv_avg_loss: 39.600498, loss: 36.989716
18:45:18.309256 Step[2000/6995], lr: 0.001000, mv_avg_loss: 38.211830, loss: 61.756817
18:45:23.120364 Step[2100/6995], lr: 0.001000, mv_avg_loss: 44.520802, loss: 39.721054
18:45:27.901081 Step[2200/6995], lr: 0.001000, mv_avg_loss: 38.570766, loss: 29.269222
18:45:32.693356 Step[2300/6995], lr: 0.001000, mv_avg_loss: 38.439266, loss: 35.875278
18:45:37.452122 Step[2400/6995], lr: 0.001000, mv_avg_loss: 45.748192, loss: 41.070015
18:45:42.212554 Step[2500/6995], lr: 0.001000, mv_avg_loss: 35.375252, loss: 49.056728
18:45:46.985438 Step[2600/6995], lr: 0.001000, mv_avg_loss: 40.442432, loss: 55.779816
18:45:51.749627 Step[2700/6995], lr: 0.001000, mv_avg_loss: 40.517254, loss: 44.522900
18:45:56.491166 Step[2800/6995], lr: 0.001000, mv_avg_loss: 40.477749, loss: 34.897434
18:46:01.243047 Step[2900/6995], lr: 0.001000, mv_avg_loss: 37.342117, loss: 20.927410
18:46:05.999973 Step[3000/6995], lr: 0.001000, mv_avg_loss: 36.750183, loss: 33.294567
18:46:10.760413 Step[3100/6995], lr: 0.001000, mv_avg_loss: 39.335278, loss: 38.165573
18:46:15.515680 Step[3200/6995], lr: 0.001000, mv_avg_loss: 40.786346, loss: 25.178280
18:46:20.290135 Step[3300/6995], lr: 0.001000, mv_avg_loss: 41.538685, loss: 41.676605
18:46:25.042376 Step[3400/6995], lr: 0.001000, mv_avg_loss: 34.885201, loss: 27.441412
18:46:29.807504 Step[3500/6995], lr: 0.001000, mv_avg_loss: 39.831387, loss: 53.995388
18:46:34.573800 Step[3600/6995], lr: 0.001000, mv_avg_loss: 44.913597, loss: 46.103363
18:46:39.315651 Step[3700/6995], lr: 0.001000, mv_avg_loss: 51.048534, loss: 56.860527
18:46:44.073636 Step[3800/6995], lr: 0.001000, mv_avg_loss: 39.197796, loss: 37.567993
18:46:48.831362 Step[3900/6995], lr: 0.001000, mv_avg_loss: 37.425011, loss: 21.513115
18:46:53.573498 Step[4000/6995], lr: 0.001000, mv_avg_loss: 39.188057, loss: 34.938622
18:46:58.342030 Step[4100/6995], lr: 0.001000, mv_avg_loss: 38.445469, loss: 41.580132
18:47:03.082051 Step[4200/6995], lr: 0.001000, mv_avg_loss: 36.156487, loss: 21.959240
18:47:07.812147 Step[4300/6995], lr: 0.001000, mv_avg_loss: 36.951183, loss: 28.750103
18:47:12.550130 Step[4400/6995], lr: 0.001000, mv_avg_loss: 38.284626, loss: 55.800301
18:47:17.294463 Step[4500/6995], lr: 0.001000, mv_avg_loss: 40.088505, loss: 43.916321
18:47:22.039789 Step[4600/6995], lr: 0.001000, mv_avg_loss: 42.647480, loss: 24.420990
18:47:26.794695 Step[4700/6995], lr: 0.001000, mv_avg_loss: 38.062523, loss: 58.850822
18:47:31.553405 Step[4800/6995], lr: 0.001000, mv_avg_loss: 40.177498, loss: 24.785858
18:47:36.313611 Step[4900/6995], lr: 0.001000, mv_avg_loss: 36.047962, loss: 32.100471
18:47:41.061568 Step[5000/6995], lr: 0.001000, mv_avg_loss: 38.229988, loss: 82.021790
18:47:45.813784 Step[5100/6995], lr: 0.001000, mv_avg_loss: 38.576424, loss: 27.004074
18:47:50.552747 Step[5200/6995], lr: 0.001000, mv_avg_loss: 38.058590, loss: 38.883766
18:47:55.293773 Step[5300/6995], lr: 0.001000, mv_avg_loss: 39.328995, loss: 28.856121
18:48:00.043299 Step[5400/6995], lr: 0.001000, mv_avg_loss: 37.031185, loss: 28.463156
18:48:04.785042 Step[5500/6995], lr: 0.001000, mv_avg_loss: 44.280315, loss: 50.691490
18:48:09.556145 Step[5600/6995], lr: 0.001000, mv_avg_loss: 47.135780, loss: 53.120770
18:48:14.299999 Step[5700/6995], lr: 0.001000, mv_avg_loss: 42.951756, loss: 57.486908
18:48:19.024757 Step[5800/6995], lr: 0.001000, mv_avg_loss: 41.160561, loss: 23.110693
18:48:23.751632 Step[5900/6995], lr: 0.001000, mv_avg_loss: 38.089123, loss: 32.081917
18:48:28.470395 Step[6000/6995], lr: 0.001000, mv_avg_loss: 40.180103, loss: 60.994858
18:48:33.204863 Step[6100/6995], lr: 0.001000, mv_avg_loss: 37.535851, loss: 32.823761
18:48:37.950606 Step[6200/6995], lr: 0.001000, mv_avg_loss: 37.226028, loss: 37.271240
18:48:42.694937 Step[6300/6995], lr: 0.001000, mv_avg_loss: 40.888420, loss: 26.748541
18:48:47.422994 Step[6400/6995], lr: 0.001000, mv_avg_loss: 39.096924, loss: 26.443523
18:48:52.152977 Step[6500/6995], lr: 0.001000, mv_avg_loss: 38.455051, loss: 34.974464
18:48:56.883371 Step[6600/6995], lr: 0.001000, mv_avg_loss: 35.404720, loss: 26.216614
18:49:01.614214 Step[6700/6995], lr: 0.001000, mv_avg_loss: 37.020313, loss: 32.895512
18:49:06.346049 Step[6800/6995], lr: 0.001000, mv_avg_loss: 40.404434, loss: 27.926258
18:49:11.083576 Step[6900/6995], lr: 0.001000, mv_avg_loss: 34.953754, loss: 21.359228
saving model for epoch 19
Testing for epoch: 19
Average test PNSR is 25.000337 for 500 images
Start to train epoch 20
18:49:36.859170 Step[0/6995], lr: 0.001000, mv_avg_loss: 43.065933, loss: 42.997864
18:49:41.607852 Step[100/6995], lr: 0.001000, mv_avg_loss: 36.385136, loss: 35.394638
18:49:46.323881 Step[200/6995], lr: 0.001000, mv_avg_loss: 35.512711, loss: 24.919529
18:49:51.045548 Step[300/6995], lr: 0.001000, mv_avg_loss: 39.384907, loss: 30.822571
18:49:55.761356 Step[400/6995], lr: 0.001000, mv_avg_loss: 39.916679, loss: 31.832470
18:50:00.468234 Step[500/6995], lr: 0.001000, mv_avg_loss: 36.878635, loss: 57.953300
18:50:05.187748 Step[600/6995], lr: 0.001000, mv_avg_loss: 34.579128, loss: 30.274323
18:50:09.911780 Step[700/6995], lr: 0.001000, mv_avg_loss: 39.036457, loss: 40.811234
18:50:14.663197 Step[800/6995], lr: 0.001000, mv_avg_loss: 38.674831, loss: 67.493813
18:50:19.376320 Step[900/6995], lr: 0.001000, mv_avg_loss: 40.615818, loss: 29.907110
18:50:24.109734 Step[1000/6995], lr: 0.001000, mv_avg_loss: 34.652378, loss: 32.847641
18:50:28.830837 Step[1100/6995], lr: 0.001000, mv_avg_loss: 39.854950, loss: 37.649990
18:50:33.538871 Step[1200/6995], lr: 0.001000, mv_avg_loss: 34.680042, loss: 17.695446
18:50:38.250236 Step[1300/6995], lr: 0.001000, mv_avg_loss: 37.418514, loss: 68.618225
18:50:42.970874 Step[1400/6995], lr: 0.001000, mv_avg_loss: 40.648247, loss: 41.767998
18:50:47.684308 Step[1500/6995], lr: 0.001000, mv_avg_loss: 42.387772, loss: 36.467171
18:50:52.393133 Step[1600/6995], lr: 0.001000, mv_avg_loss: 39.044029, loss: 27.872946
18:50:57.115575 Step[1700/6995], lr: 0.001000, mv_avg_loss: 34.736004, loss: 23.337204
18:51:01.843778 Step[1800/6995], lr: 0.001000, mv_avg_loss: 35.434914, loss: 34.418610
18:51:06.566004 Step[1900/6995], lr: 0.001000, mv_avg_loss: 35.984608, loss: 29.567032
18:51:11.273827 Step[2000/6995], lr: 0.001000, mv_avg_loss: 36.705364, loss: 23.097759
18:51:16.004807 Step[2100/6995], lr: 0.001000, mv_avg_loss: 36.963692, loss: 33.033707
18:51:20.726304 Step[2200/6995], lr: 0.001000, mv_avg_loss: 36.693748, loss: 29.330177
18:51:25.455342 Step[2300/6995], lr: 0.001000, mv_avg_loss: 43.615952, loss: 76.741470
18:51:30.211302 Step[2400/6995], lr: 0.001000, mv_avg_loss: 38.733604, loss: 41.597961
18:51:34.949599 Step[2500/6995], lr: 0.001000, mv_avg_loss: 41.600304, loss: 28.892141
18:51:39.678054 Step[2600/6995], lr: 0.001000, mv_avg_loss: 37.528435, loss: 38.600056
18:51:44.417722 Step[2700/6995], lr: 0.001000, mv_avg_loss: 38.438381, loss: 32.781155
18:51:49.164659 Step[2800/6995], lr: 0.001000, mv_avg_loss: 40.253555, loss: 49.805756
18:51:53.930439 Step[2900/6995], lr: 0.001000, mv_avg_loss: 35.195713, loss: 29.429592
18:51:58.665672 Step[3000/6995], lr: 0.001000, mv_avg_loss: 37.273014, loss: 45.999390
18:52:03.403103 Step[3100/6995], lr: 0.001000, mv_avg_loss: 38.441986, loss: 31.170547
18:52:08.149871 Step[3200/6995], lr: 0.001000, mv_avg_loss: 34.096523, loss: 26.828899
18:52:12.891342 Step[3300/6995], lr: 0.001000, mv_avg_loss: 36.030941, loss: 38.971050
18:52:17.629788 Step[3400/6995], lr: 0.001000, mv_avg_loss: 40.329029, loss: 28.770521
18:52:22.376790 Step[3500/6995], lr: 0.001000, mv_avg_loss: 38.659252, loss: 31.299767
18:52:27.139728 Step[3600/6995], lr: 0.001000, mv_avg_loss: 39.129078, loss: 31.329132
18:52:31.874794 Step[3700/6995], lr: 0.001000, mv_avg_loss: 38.495079, loss: 26.965519
18:52:36.619851 Step[3800/6995], lr: 0.001000, mv_avg_loss: 37.728657, loss: 30.356159
18:52:41.346242 Step[3900/6995], lr: 0.001000, mv_avg_loss: 33.410175, loss: 40.472473
18:52:46.083733 Step[4000/6995], lr: 0.001000, mv_avg_loss: 42.313286, loss: 81.170677
18:52:50.816372 Step[4100/6995], lr: 0.001000, mv_avg_loss: 38.167912, loss: 48.090302
18:52:55.554697 Step[4200/6995], lr: 0.001000, mv_avg_loss: 36.622345, loss: 22.365494
18:53:00.292314 Step[4300/6995], lr: 0.001000, mv_avg_loss: 39.552074, loss: 45.174194
18:53:05.042267 Step[4400/6995], lr: 0.001000, mv_avg_loss: 36.054562, loss: 21.622845
18:53:09.778612 Step[4500/6995], lr: 0.001000, mv_avg_loss: 35.923317, loss: 35.073505
18:53:14.524469 Step[4600/6995], lr: 0.001000, mv_avg_loss: 35.299156, loss: 30.787510
18:53:19.263591 Step[4700/6995], lr: 0.001000, mv_avg_loss: 32.771057, loss: 24.999828
18:53:24.004074 Step[4800/6995], lr: 0.001000, mv_avg_loss: 37.172604, loss: 55.183056
18:53:28.740859 Step[4900/6995], lr: 0.001000, mv_avg_loss: 39.590454, loss: 20.673223
18:53:33.478691 Step[5000/6995], lr: 0.001000, mv_avg_loss: 37.943146, loss: 22.590071
18:53:38.244355 Step[5100/6995], lr: 0.001000, mv_avg_loss: 33.912758, loss: 42.777020
18:53:42.985393 Step[5200/6995], lr: 0.001000, mv_avg_loss: 38.961277, loss: 59.208839
18:53:47.754772 Step[5300/6995], lr: 0.001000, mv_avg_loss: 37.954948, loss: 41.597164
18:53:52.495656 Step[5400/6995], lr: 0.001000, mv_avg_loss: 38.910423, loss: 28.443054
18:53:57.231763 Step[5500/6995], lr: 0.001000, mv_avg_loss: 36.491669, loss: 36.602898
18:54:01.984949 Step[5600/6995], lr: 0.001000, mv_avg_loss: 35.209087, loss: 46.178253
18:54:06.742623 Step[5700/6995], lr: 0.001000, mv_avg_loss: 32.173668, loss: 31.867107
18:54:11.496359 Step[5800/6995], lr: 0.001000, mv_avg_loss: 36.513603, loss: 35.577583
18:54:16.243068 Step[5900/6995], lr: 0.001000, mv_avg_loss: 38.115376, loss: 46.199749
18:54:21.001112 Step[6000/6995], lr: 0.001000, mv_avg_loss: 39.955898, loss: 22.617584
18:54:25.751644 Step[6100/6995], lr: 0.001000, mv_avg_loss: 35.145393, loss: 37.765812
18:54:30.489928 Step[6200/6995], lr: 0.001000, mv_avg_loss: 42.792831, loss: 37.586796
18:54:35.239343 Step[6300/6995], lr: 0.001000, mv_avg_loss: 38.662827, loss: 23.604574
18:54:39.982231 Step[6400/6995], lr: 0.001000, mv_avg_loss: 35.728024, loss: 32.160126
18:54:44.729730 Step[6500/6995], lr: 0.001000, mv_avg_loss: 36.547485, loss: 22.527128
18:54:49.478985 Step[6600/6995], lr: 0.001000, mv_avg_loss: 37.168491, loss: 29.329296
18:54:54.230756 Step[6700/6995], lr: 0.001000, mv_avg_loss: 41.123253, loss: 95.343857
18:54:58.975666 Step[6800/6995], lr: 0.001000, mv_avg_loss: 37.929726, loss: 27.341299
18:55:03.732440 Step[6900/6995], lr: 0.001000, mv_avg_loss: 39.687065, loss: 38.280769
Testing for epoch: 20
Average test PNSR is 25.221393 for 500 images
Start to train epoch 21
18:55:31.025902 Step[0/6995], lr: 0.001000, mv_avg_loss: 38.156395, loss: 35.772728
18:55:35.749307 Step[100/6995], lr: 0.001000, mv_avg_loss: 33.362946, loss: 21.385937
18:55:40.497980 Step[200/6995], lr: 0.001000, mv_avg_loss: 33.225746, loss: 25.129141
18:55:45.228105 Step[300/6995], lr: 0.001000, mv_avg_loss: 33.386681, loss: 41.742168
18:55:49.967129 Step[400/6995], lr: 0.001000, mv_avg_loss: 35.018440, loss: 59.988304
18:55:54.705241 Step[500/6995], lr: 0.001000, mv_avg_loss: 33.925262, loss: 75.454643
18:55:59.437576 Step[600/6995], lr: 0.001000, mv_avg_loss: 35.826607, loss: 46.116272
18:56:04.190183 Step[700/6995], lr: 0.001000, mv_avg_loss: 35.348557, loss: 31.285313
18:56:08.935086 Step[800/6995], lr: 0.001000, mv_avg_loss: 32.562267, loss: 53.916405
18:56:13.689300 Step[900/6995], lr: 0.001000, mv_avg_loss: 34.809383, loss: 29.393688
18:56:18.426589 Step[1000/6995], lr: 0.001000, mv_avg_loss: 37.083282, loss: 37.042301
18:56:23.178074 Step[1100/6995], lr: 0.001000, mv_avg_loss: 33.571167, loss: 34.845829
18:56:27.923707 Step[1200/6995], lr: 0.001000, mv_avg_loss: 37.181496, loss: 22.487682
18:56:32.674313 Step[1300/6995], lr: 0.001000, mv_avg_loss: 33.985252, loss: 18.745411
18:56:37.444880 Step[1400/6995], lr: 0.001000, mv_avg_loss: 37.451206, loss: 49.831463
18:56:42.200222 Step[1500/6995], lr: 0.001000, mv_avg_loss: 34.042210, loss: 42.528152
18:56:46.962257 Step[1600/6995], lr: 0.001000, mv_avg_loss: 39.083855, loss: 43.175320
18:56:51.702223 Step[1700/6995], lr: 0.001000, mv_avg_loss: 39.523140, loss: 26.687702
18:56:56.453462 Step[1800/6995], lr: 0.001000, mv_avg_loss: 33.965347, loss: 24.192455
18:57:01.202038 Step[1900/6995], lr: 0.001000, mv_avg_loss: 30.778952, loss: 26.637106
18:57:05.957500 Step[2000/6995], lr: 0.001000, mv_avg_loss: 33.983650, loss: 46.039391
18:57:10.724993 Step[2100/6995], lr: 0.001000, mv_avg_loss: 41.911812, loss: 61.950695
18:57:15.476106 Step[2200/6995], lr: 0.001000, mv_avg_loss: 36.388393, loss: 37.271103
18:57:20.218990 Step[2300/6995], lr: 0.001000, mv_avg_loss: 36.348091, loss: 40.997398
18:57:24.970909 Step[2400/6995], lr: 0.001000, mv_avg_loss: 37.042816, loss: 25.135771
18:57:29.715949 Step[2500/6995], lr: 0.001000, mv_avg_loss: 35.554478, loss: 34.411926
18:57:34.458686 Step[2600/6995], lr: 0.001000, mv_avg_loss: 33.080391, loss: 22.240967
18:57:39.195892 Step[2700/6995], lr: 0.001000, mv_avg_loss: 39.801533, loss: 41.342220
18:57:43.950974 Step[2800/6995], lr: 0.001000, mv_avg_loss: 37.106472, loss: 36.215836
18:57:48.700014 Step[2900/6995], lr: 0.001000, mv_avg_loss: 35.743309, loss: 44.150982
18:57:53.441687 Step[3000/6995], lr: 0.001000, mv_avg_loss: 40.608818, loss: 37.360939
18:57:58.197641 Step[3100/6995], lr: 0.001000, mv_avg_loss: 39.614159, loss: 28.570906
18:58:02.975726 Step[3200/6995], lr: 0.001000, mv_avg_loss: 37.892532, loss: 37.432571
18:58:07.719326 Step[3300/6995], lr: 0.001000, mv_avg_loss: 36.287727, loss: 26.266088
18:58:12.460698 Step[3400/6995], lr: 0.001000, mv_avg_loss: 34.968971, loss: 23.366135
18:58:17.211749 Step[3500/6995], lr: 0.001000, mv_avg_loss: 40.005013, loss: 37.874935
18:58:21.986628 Step[3600/6995], lr: 0.001000, mv_avg_loss: 40.568626, loss: 30.204300
18:58:26.746956 Step[3700/6995], lr: 0.001000, mv_avg_loss: 36.898449, loss: 30.150879
18:58:31.487904 Step[3800/6995], lr: 0.001000, mv_avg_loss: 36.500828, loss: 22.233923
18:58:36.226053 Step[3900/6995], lr: 0.001000, mv_avg_loss: 39.082546, loss: 30.874771
18:58:40.964568 Step[4000/6995], lr: 0.001000, mv_avg_loss: 34.635803, loss: 23.675640
18:58:45.698095 Step[4100/6995], lr: 0.001000, mv_avg_loss: 38.306473, loss: 46.705990
18:58:50.445096 Step[4200/6995], lr: 0.001000, mv_avg_loss: 37.963314, loss: 25.199326
18:58:55.193152 Step[4300/6995], lr: 0.001000, mv_avg_loss: 36.628128, loss: 51.855423
18:58:59.948460 Step[4400/6995], lr: 0.001000, mv_avg_loss: 36.813660, loss: 30.477291
18:59:04.697927 Step[4500/6995], lr: 0.001000, mv_avg_loss: 38.354942, loss: 30.935192
18:59:09.459379 Step[4600/6995], lr: 0.001000, mv_avg_loss: 35.891109, loss: 45.572258
18:59:14.191934 Step[4700/6995], lr: 0.001000, mv_avg_loss: 35.402977, loss: 20.344452
18:59:18.944928 Step[4800/6995], lr: 0.001000, mv_avg_loss: 38.880527, loss: 26.756863
18:59:23.706250 Step[4900/6995], lr: 0.001000, mv_avg_loss: 35.622753, loss: 26.097179
18:59:28.450965 Step[5000/6995], lr: 0.001000, mv_avg_loss: 36.083130, loss: 39.462158
18:59:33.188672 Step[5100/6995], lr: 0.001000, mv_avg_loss: 38.453892, loss: 26.820402
18:59:37.935188 Step[5200/6995], lr: 0.001000, mv_avg_loss: 38.884697, loss: 40.321808
18:59:42.668448 Step[5300/6995], lr: 0.001000, mv_avg_loss: 36.912964, loss: 30.101248
18:59:47.413820 Step[5400/6995], lr: 0.001000, mv_avg_loss: 32.426079, loss: 47.051384
18:59:52.149797 Step[5500/6995], lr: 0.001000, mv_avg_loss: 30.665590, loss: 53.580505
18:59:56.894807 Step[5600/6995], lr: 0.001000, mv_avg_loss: 36.928925, loss: 30.438345
19:00:01.648991 Step[5700/6995], lr: 0.001000, mv_avg_loss: 33.715294, loss: 19.809486
19:00:06.419052 Step[5800/6995], lr: 0.001000, mv_avg_loss: 34.404861, loss: 24.904419
19:00:11.160834 Step[5900/6995], lr: 0.001000, mv_avg_loss: 30.513168, loss: 36.963467
19:00:15.905515 Step[6000/6995], lr: 0.001000, mv_avg_loss: 35.211201, loss: 45.434761
19:00:20.651737 Step[6100/6995], lr: 0.001000, mv_avg_loss: 32.394451, loss: 28.762953
19:00:25.399874 Step[6200/6995], lr: 0.001000, mv_avg_loss: 33.671497, loss: 24.349945
19:00:30.155770 Step[6300/6995], lr: 0.001000, mv_avg_loss: 31.600813, loss: 48.813942
19:00:34.906225 Step[6400/6995], lr: 0.001000, mv_avg_loss: 34.928894, loss: 86.229767
19:00:39.643337 Step[6500/6995], lr: 0.001000, mv_avg_loss: 42.520409, loss: 31.172390
19:00:44.368134 Step[6600/6995], lr: 0.001000, mv_avg_loss: 40.978355, loss: 39.713264
19:00:49.102633 Step[6700/6995], lr: 0.001000, mv_avg_loss: 38.192146, loss: 38.344902
19:00:53.831706 Step[6800/6995], lr: 0.001000, mv_avg_loss: 38.866814, loss: 51.804314
19:00:58.558198 Step[6900/6995], lr: 0.001000, mv_avg_loss: 40.581730, loss: 35.908512
Testing for epoch: 21
Average test PNSR is 24.941002 for 500 images
Start to train epoch 22
19:01:25.276482 Step[0/6995], lr: 0.001000, mv_avg_loss: 36.533516, loss: 42.303997
19:01:29.973125 Step[100/6995], lr: 0.001000, mv_avg_loss: 34.064171, loss: 34.401745
19:01:34.672893 Step[200/6995], lr: 0.001000, mv_avg_loss: 33.789505, loss: 45.386532
19:01:39.394475 Step[300/6995], lr: 0.001000, mv_avg_loss: 33.227474, loss: 29.819189
19:01:44.104162 Step[400/6995], lr: 0.001000, mv_avg_loss: 31.391010, loss: 25.822475
19:01:48.821207 Step[500/6995], lr: 0.001000, mv_avg_loss: 34.163948, loss: 28.362164
19:01:53.533292 Step[600/6995], lr: 0.001000, mv_avg_loss: 38.429253, loss: 24.640045
19:01:58.254831 Step[700/6995], lr: 0.001000, mv_avg_loss: 37.074791, loss: 37.793129
19:02:02.979334 Step[800/6995], lr: 0.001000, mv_avg_loss: 36.333645, loss: 32.302979
19:02:07.684929 Step[900/6995], lr: 0.001000, mv_avg_loss: 41.217758, loss: 44.083797
19:02:12.399563 Step[1000/6995], lr: 0.001000, mv_avg_loss: 34.465176, loss: 25.507568
19:02:17.265680 Step[1100/6995], lr: 0.001000, mv_avg_loss: 42.859581, loss: 30.411306
19:02:22.031564 Step[1200/6995], lr: 0.001000, mv_avg_loss: 45.876667, loss: 21.871128
19:02:26.761362 Step[1300/6995], lr: 0.001000, mv_avg_loss: 39.734985, loss: 36.328247
19:02:31.508865 Step[1400/6995], lr: 0.001000, mv_avg_loss: 39.284779, loss: 27.082495
19:02:36.258936 Step[1500/6995], lr: 0.001000, mv_avg_loss: 34.045681, loss: 18.837215
19:02:41.016085 Step[1600/6995], lr: 0.001000, mv_avg_loss: 33.823318, loss: 30.936117
19:02:45.746466 Step[1700/6995], lr: 0.001000, mv_avg_loss: 31.699022, loss: 44.702209
19:02:50.503128 Step[1800/6995], lr: 0.001000, mv_avg_loss: 37.222111, loss: 29.391325
19:02:55.219763 Step[1900/6995], lr: 0.001000, mv_avg_loss: 33.339546, loss: 27.214552
19:02:59.946112 Step[2000/6995], lr: 0.001000, mv_avg_loss: 29.699656, loss: 24.369083
19:03:04.669406 Step[2100/6995], lr: 0.001000, mv_avg_loss: 35.685944, loss: 70.732651
19:03:09.392648 Step[2200/6995], lr: 0.001000, mv_avg_loss: 34.021770, loss: 29.510269
19:03:14.124545 Step[2300/6995], lr: 0.001000, mv_avg_loss: 38.935402, loss: 54.150681
19:03:18.832784 Step[2400/6995], lr: 0.001000, mv_avg_loss: 41.362671, loss: 38.292416
19:03:23.559703 Step[2500/6995], lr: 0.001000, mv_avg_loss: 37.338432, loss: 21.581846
19:03:28.287906 Step[2600/6995], lr: 0.001000, mv_avg_loss: 33.968105, loss: 50.448997
19:03:33.027491 Step[2700/6995], lr: 0.001000, mv_avg_loss: 33.362183, loss: 35.785774
19:03:37.752932 Step[2800/6995], lr: 0.001000, mv_avg_loss: 47.291313, loss: 39.763283
19:03:42.488010 Step[2900/6995], lr: 0.001000, mv_avg_loss: 35.223316, loss: 36.908028
19:03:47.223704 Step[3000/6995], lr: 0.001000, mv_avg_loss: 34.249859, loss: 54.748550
19:03:51.959039 Step[3100/6995], lr: 0.001000, mv_avg_loss: 32.842007, loss: 17.751926
19:03:56.707749 Step[3200/6995], lr: 0.001000, mv_avg_loss: 32.927986, loss: 41.101959
19:04:01.447145 Step[3300/6995], lr: 0.001000, mv_avg_loss: 30.196362, loss: 24.972692
19:04:06.183418 Step[3400/6995], lr: 0.001000, mv_avg_loss: 34.733589, loss: 30.088123
19:04:10.914447 Step[3500/6995], lr: 0.001000, mv_avg_loss: 36.026043, loss: 31.913227
19:04:15.657457 Step[3600/6995], lr: 0.001000, mv_avg_loss: 36.143936, loss: 29.647469
19:04:20.381311 Step[3700/6995], lr: 0.001000, mv_avg_loss: 33.080471, loss: 34.672180
19:04:25.120894 Step[3800/6995], lr: 0.001000, mv_avg_loss: 32.327103, loss: 72.112198
19:04:29.859622 Step[3900/6995], lr: 0.001000, mv_avg_loss: 34.439461, loss: 29.205673
19:04:34.587591 Step[4000/6995], lr: 0.001000, mv_avg_loss: 35.300735, loss: 44.686882
19:04:39.314318 Step[4100/6995], lr: 0.001000, mv_avg_loss: 32.588062, loss: 34.795666
19:04:44.028082 Step[4200/6995], lr: 0.001000, mv_avg_loss: 35.453316, loss: 31.073141
19:04:48.757272 Step[4300/6995], lr: 0.001000, mv_avg_loss: 37.142506, loss: 41.216061
19:04:53.472949 Step[4400/6995], lr: 0.001000, mv_avg_loss: 34.359573, loss: 19.902534
19:04:58.192375 Step[4500/6995], lr: 0.001000, mv_avg_loss: 37.614704, loss: 44.727581
19:05:02.934060 Step[4600/6995], lr: 0.001000, mv_avg_loss: 33.584332, loss: 24.314070
19:05:07.656211 Step[4700/6995], lr: 0.001000, mv_avg_loss: 37.205967, loss: 35.993458
19:05:12.376855 Step[4800/6995], lr: 0.001000, mv_avg_loss: 36.484138, loss: 46.748501
19:05:17.107597 Step[4900/6995], lr: 0.001000, mv_avg_loss: 37.980343, loss: 54.431561
19:05:21.834160 Step[5000/6995], lr: 0.001000, mv_avg_loss: 34.536266, loss: 51.778587
19:05:26.571960 Step[5100/6995], lr: 0.001000, mv_avg_loss: 34.587425, loss: 28.455288
19:05:31.308191 Step[5200/6995], lr: 0.001000, mv_avg_loss: 38.464317, loss: 47.734276
19:05:36.038504 Step[5300/6995], lr: 0.001000, mv_avg_loss: 32.429913, loss: 40.043243
19:05:40.773306 Step[5400/6995], lr: 0.001000, mv_avg_loss: 38.139420, loss: 31.605335
19:05:45.533492 Step[5500/6995], lr: 0.001000, mv_avg_loss: 37.391476, loss: 47.914528
19:05:50.284950 Step[5600/6995], lr: 0.001000, mv_avg_loss: 33.083889, loss: 33.101299
19:05:55.036208 Step[5700/6995], lr: 0.001000, mv_avg_loss: 36.895550, loss: 23.162489
19:05:59.777732 Step[5800/6995], lr: 0.001000, mv_avg_loss: 33.499214, loss: 36.814304
19:06:04.527529 Step[5900/6995], lr: 0.001000, mv_avg_loss: 31.537924, loss: 67.317734
19:06:09.269880 Step[6000/6995], lr: 0.001000, mv_avg_loss: 34.371693, loss: 34.458893
19:06:14.012354 Step[6100/6995], lr: 0.001000, mv_avg_loss: 31.887865, loss: 22.931625
19:06:18.776356 Step[6200/6995], lr: 0.001000, mv_avg_loss: 34.461578, loss: 33.324318
19:06:23.524604 Step[6300/6995], lr: 0.001000, mv_avg_loss: 33.610767, loss: 26.544271
19:06:28.275558 Step[6400/6995], lr: 0.001000, mv_avg_loss: 40.332050, loss: 53.635239
19:06:33.021352 Step[6500/6995], lr: 0.001000, mv_avg_loss: 39.914356, loss: 38.751427
19:06:37.753008 Step[6600/6995], lr: 0.001000, mv_avg_loss: 41.603687, loss: 45.253094
19:06:42.501670 Step[6700/6995], lr: 0.001000, mv_avg_loss: 33.747261, loss: 38.370872
19:06:47.252762 Step[6800/6995], lr: 0.001000, mv_avg_loss: 35.380291, loss: 27.951294
19:06:52.001159 Step[6900/6995], lr: 0.001000, mv_avg_loss: 30.881596, loss: 37.303104
Testing for epoch: 22
Average test PNSR is 24.964882 for 500 images
Start to train epoch 23
19:07:19.325027 Step[0/6995], lr: 0.001000, mv_avg_loss: 36.521545, loss: 24.286160
19:07:24.052370 Step[100/6995], lr: 0.001000, mv_avg_loss: 32.675167, loss: 18.494087
19:07:28.768420 Step[200/6995], lr: 0.001000, mv_avg_loss: 33.561470, loss: 43.513084
19:07:33.489401 Step[300/6995], lr: 0.001000, mv_avg_loss: 34.008919, loss: 37.366730
19:07:38.210449 Step[400/6995], lr: 0.001000, mv_avg_loss: 34.526211, loss: 49.646454
19:07:42.898768 Step[500/6995], lr: 0.001000, mv_avg_loss: 30.452606, loss: 20.928532
19:07:47.588448 Step[600/6995], lr: 0.001000, mv_avg_loss: 34.750008, loss: 36.165359
19:07:52.345031 Step[700/6995], lr: 0.001000, mv_avg_loss: 36.637806, loss: 45.647118
19:07:57.104161 Step[800/6995], lr: 0.001000, mv_avg_loss: 35.069309, loss: 32.403564
19:08:01.866369 Step[900/6995], lr: 0.001000, mv_avg_loss: 32.282269, loss: 30.591360
19:08:06.621085 Step[1000/6995], lr: 0.001000, mv_avg_loss: 34.093449, loss: 38.171043
19:08:11.378222 Step[1100/6995], lr: 0.001000, mv_avg_loss: 34.219723, loss: 33.689865
19:08:16.138370 Step[1200/6995], lr: 0.001000, mv_avg_loss: 33.783321, loss: 24.527756
19:08:20.904511 Step[1300/6995], lr: 0.001000, mv_avg_loss: 37.323124, loss: 28.009537
19:08:25.696270 Step[1400/6995], lr: 0.001000, mv_avg_loss: 37.073956, loss: 34.669601
19:08:30.474952 Step[1500/6995], lr: 0.001000, mv_avg_loss: 36.321838, loss: 27.021832
19:08:35.261203 Step[1600/6995], lr: 0.001000, mv_avg_loss: 32.688702, loss: 34.017181
19:08:40.033689 Step[1700/6995], lr: 0.001000, mv_avg_loss: 33.987022, loss: 46.897533
19:08:44.815619 Step[1800/6995], lr: 0.001000, mv_avg_loss: 33.929848, loss: 33.479401
19:08:49.582290 Step[1900/6995], lr: 0.001000, mv_avg_loss: 41.488964, loss: 32.378075
19:08:54.367629 Step[2000/6995], lr: 0.001000, mv_avg_loss: 31.068666, loss: 30.473644
19:08:59.165599 Step[2100/6995], lr: 0.001000, mv_avg_loss: 36.726936, loss: 46.094452
19:09:03.925798 Step[2200/6995], lr: 0.001000, mv_avg_loss: 33.755314, loss: 24.009979
19:09:08.715440 Step[2300/6995], lr: 0.001000, mv_avg_loss: 35.217403, loss: 35.974079
19:09:13.513023 Step[2400/6995], lr: 0.001000, mv_avg_loss: 34.650063, loss: 29.634090
19:09:18.312315 Step[2500/6995], lr: 0.001000, mv_avg_loss: 34.687351, loss: 45.974091
19:09:23.097901 Step[2600/6995], lr: 0.001000, mv_avg_loss: 34.599388, loss: 43.953308
19:09:27.878308 Step[2700/6995], lr: 0.001000, mv_avg_loss: 30.479954, loss: 34.474617
19:09:32.669342 Step[2800/6995], lr: 0.001000, mv_avg_loss: 36.340511, loss: 34.837219
19:09:37.447047 Step[2900/6995], lr: 0.001000, mv_avg_loss: 34.187996, loss: 37.714493
19:09:42.240573 Step[3000/6995], lr: 0.001000, mv_avg_loss: 32.492519, loss: 41.786232
19:09:47.036428 Step[3100/6995], lr: 0.001000, mv_avg_loss: 34.838848, loss: 28.940197
19:09:51.828256 Step[3200/6995], lr: 0.001000, mv_avg_loss: 32.257511, loss: 30.350548
19:09:56.633661 Step[3300/6995], lr: 0.001000, mv_avg_loss: 31.253645, loss: 22.924129
19:10:01.426744 Step[3400/6995], lr: 0.001000, mv_avg_loss: 31.088837, loss: 26.548468
19:10:06.220499 Step[3500/6995], lr: 0.001000, mv_avg_loss: 41.519634, loss: 26.598431
19:10:11.009716 Step[3600/6995], lr: 0.001000, mv_avg_loss: 30.080851, loss: 22.205471
19:10:15.802746 Step[3700/6995], lr: 0.001000, mv_avg_loss: 35.244946, loss: 32.115990
19:10:20.594795 Step[3800/6995], lr: 0.001000, mv_avg_loss: 40.079960, loss: 46.786125
19:10:25.389060 Step[3900/6995], lr: 0.001000, mv_avg_loss: 41.055225, loss: 29.235281
19:10:30.191373 Step[4000/6995], lr: 0.001000, mv_avg_loss: 34.984581, loss: 42.530296
19:10:34.984433 Step[4100/6995], lr: 0.001000, mv_avg_loss: 31.601696, loss: 29.827705
19:10:39.799490 Step[4200/6995], lr: 0.001000, mv_avg_loss: 34.290977, loss: 44.589584
19:10:44.590467 Step[4300/6995], lr: 0.001000, mv_avg_loss: 31.858168, loss: 41.257263
19:10:49.370091 Step[4400/6995], lr: 0.001000, mv_avg_loss: 36.352627, loss: 34.519772
19:10:54.168577 Step[4500/6995], lr: 0.001000, mv_avg_loss: 37.968796, loss: 36.167389
19:10:58.977270 Step[4600/6995], lr: 0.001000, mv_avg_loss: 32.801239, loss: 25.257887
19:11:03.772492 Step[4700/6995], lr: 0.001000, mv_avg_loss: 33.736294, loss: 25.576641
19:11:08.565446 Step[4800/6995], lr: 0.001000, mv_avg_loss: 30.592630, loss: 23.337967
19:11:13.374085 Step[4900/6995], lr: 0.001000, mv_avg_loss: 32.446430, loss: 30.060888
19:11:18.152186 Step[5000/6995], lr: 0.001000, mv_avg_loss: 34.594971, loss: 22.474514
19:11:22.931671 Step[5100/6995], lr: 0.001000, mv_avg_loss: 34.060547, loss: 27.044912
19:11:27.721467 Step[5200/6995], lr: 0.001000, mv_avg_loss: 35.570400, loss: 38.781578
19:11:32.520840 Step[5300/6995], lr: 0.001000, mv_avg_loss: 40.351761, loss: 66.047134
19:11:37.296383 Step[5400/6995], lr: 0.001000, mv_avg_loss: 34.365341, loss: 21.351934
19:11:42.089630 Step[5500/6995], lr: 0.001000, mv_avg_loss: 30.657578, loss: 41.272301
19:11:46.888432 Step[5600/6995], lr: 0.001000, mv_avg_loss: 42.496040, loss: 51.080292
19:11:51.688147 Step[5700/6995], lr: 0.001000, mv_avg_loss: 41.558434, loss: 60.834698
19:11:56.457110 Step[5800/6995], lr: 0.001000, mv_avg_loss: 34.523239, loss: 38.198833
19:12:01.229817 Step[5900/6995], lr: 0.001000, mv_avg_loss: 34.754673, loss: 26.762915
19:12:06.015983 Step[6000/6995], lr: 0.001000, mv_avg_loss: 34.900925, loss: 21.329870
19:12:10.788268 Step[6100/6995], lr: 0.001000, mv_avg_loss: 33.139378, loss: 28.319607
19:12:15.560864 Step[6200/6995], lr: 0.001000, mv_avg_loss: 36.654751, loss: 21.802307
19:12:20.332737 Step[6300/6995], lr: 0.001000, mv_avg_loss: 37.190922, loss: 43.755424
19:12:25.105379 Step[6400/6995], lr: 0.001000, mv_avg_loss: 32.580933, loss: 52.339153
19:12:29.868375 Step[6500/6995], lr: 0.001000, mv_avg_loss: 33.771477, loss: 54.587841
19:12:34.632120 Step[6600/6995], lr: 0.001000, mv_avg_loss: 31.613512, loss: 25.389160
19:12:39.407999 Step[6700/6995], lr: 0.001000, mv_avg_loss: 34.021255, loss: 32.167152
19:12:44.193093 Step[6800/6995], lr: 0.001000, mv_avg_loss: 30.308994, loss: 20.519720
19:12:48.980873 Step[6900/6995], lr: 0.001000, mv_avg_loss: 32.350628, loss: 37.666267
Testing for epoch: 23
Average test PNSR is 25.141493 for 500 images
Start to train epoch 24
19:13:14.925726 Step[0/6995], lr: 0.001000, mv_avg_loss: 31.484299, loss: 24.527969
19:13:19.674694 Step[100/6995], lr: 0.001000, mv_avg_loss: 31.776707, loss: 36.650890
19:13:24.429096 Step[200/6995], lr: 0.001000, mv_avg_loss: 31.015011, loss: 29.615250
19:13:29.177613 Step[300/6995], lr: 0.001000, mv_avg_loss: 28.538902, loss: 17.349800
19:13:33.958795 Step[400/6995], lr: 0.001000, mv_avg_loss: 33.169960, loss: 29.587933
19:13:38.714831 Step[500/6995], lr: 0.001000, mv_avg_loss: 32.389988, loss: 43.542336
19:13:43.467899 Step[600/6995], lr: 0.001000, mv_avg_loss: 38.880039, loss: 31.276024
19:13:48.225250 Step[700/6995], lr: 0.001000, mv_avg_loss: 37.102798, loss: 23.021553
19:13:52.982112 Step[800/6995], lr: 0.001000, mv_avg_loss: 36.444454, loss: 27.161758
19:13:57.736273 Step[900/6995], lr: 0.001000, mv_avg_loss: 35.009487, loss: 39.664932
19:14:02.499175 Step[1000/6995], lr: 0.001000, mv_avg_loss: 33.073322, loss: 31.082792
19:14:07.279711 Step[1100/6995], lr: 0.001000, mv_avg_loss: 33.250816, loss: 26.929897
19:14:12.041059 Step[1200/6995], lr: 0.001000, mv_avg_loss: 33.054817, loss: 45.614330
19:14:16.802585 Step[1300/6995], lr: 0.001000, mv_avg_loss: 34.880222, loss: 39.931610
19:14:21.572894 Step[1400/6995], lr: 0.001000, mv_avg_loss: 37.025230, loss: 29.577667
19:14:26.326690 Step[1500/6995], lr: 0.001000, mv_avg_loss: 32.860397, loss: 21.630295
19:14:31.074726 Step[1600/6995], lr: 0.001000, mv_avg_loss: 33.014786, loss: 47.552208
19:14:35.823305 Step[1700/6995], lr: 0.001000, mv_avg_loss: 34.747025, loss: 38.332848
19:14:40.565771 Step[1800/6995], lr: 0.001000, mv_avg_loss: 35.024544, loss: 35.423233
19:14:45.339709 Step[1900/6995], lr: 0.001000, mv_avg_loss: 33.907948, loss: 30.370764
19:14:50.087886 Step[2000/6995], lr: 0.001000, mv_avg_loss: 31.694250, loss: 51.807564
19:14:54.854936 Step[2100/6995], lr: 0.001000, mv_avg_loss: 31.096395, loss: 30.642216
19:14:59.656702 Step[2200/6995], lr: 0.001000, mv_avg_loss: 30.254049, loss: 29.881966
19:15:04.428584 Step[2300/6995], lr: 0.001000, mv_avg_loss: 33.455521, loss: 23.111151
19:15:09.194882 Step[2400/6995], lr: 0.001000, mv_avg_loss: 32.668785, loss: 26.206686
19:15:13.973909 Step[2500/6995], lr: 0.001000, mv_avg_loss: 32.614933, loss: 70.957298
19:15:18.719759 Step[2600/6995], lr: 0.001000, mv_avg_loss: 35.835140, loss: 35.851982
19:15:23.472143 Step[2700/6995], lr: 0.001000, mv_avg_loss: 35.242535, loss: 22.634390
19:15:28.219793 Step[2800/6995], lr: 0.001000, mv_avg_loss: 37.353115, loss: 28.643803
19:15:32.964664 Step[2900/6995], lr: 0.001000, mv_avg_loss: 37.408710, loss: 72.196281
19:15:37.723714 Step[3000/6995], lr: 0.001000, mv_avg_loss: 39.163269, loss: 34.477936
19:15:42.497229 Step[3100/6995], lr: 0.001000, mv_avg_loss: 35.193661, loss: 36.148804
19:15:47.261179 Step[3200/6995], lr: 0.001000, mv_avg_loss: 32.090904, loss: 22.956535
19:15:52.014686 Step[3300/6995], lr: 0.001000, mv_avg_loss: 34.849297, loss: 26.705229
19:15:56.790403 Step[3400/6995], lr: 0.001000, mv_avg_loss: 31.722408, loss: 31.343487
19:16:01.552856 Step[3500/6995], lr: 0.001000, mv_avg_loss: 31.285147, loss: 17.250565
19:16:06.307587 Step[3600/6995], lr: 0.001000, mv_avg_loss: 30.716013, loss: 28.166378
19:16:11.065093 Step[3700/6995], lr: 0.001000, mv_avg_loss: 31.699059, loss: 15.475090
19:16:15.819952 Step[3800/6995], lr: 0.001000, mv_avg_loss: 34.251270, loss: 27.395454
19:16:20.556406 Step[3900/6995], lr: 0.001000, mv_avg_loss: 29.455307, loss: 20.387672
19:16:25.303462 Step[4000/6995], lr: 0.001000, mv_avg_loss: 32.490574, loss: 21.185158
19:16:30.069388 Step[4100/6995], lr: 0.001000, mv_avg_loss: 31.442837, loss: 63.302704
19:16:34.852599 Step[4200/6995], lr: 0.001000, mv_avg_loss: 35.280758, loss: 50.347713
19:16:39.608407 Step[4300/6995], lr: 0.001000, mv_avg_loss: 30.766218, loss: 32.662621
19:16:44.350576 Step[4400/6995], lr: 0.001000, mv_avg_loss: 33.223824, loss: 20.786312
19:16:49.092505 Step[4500/6995], lr: 0.001000, mv_avg_loss: 30.369717, loss: 21.882040
19:16:53.864355 Step[4600/6995], lr: 0.001000, mv_avg_loss: 31.693827, loss: 34.293159
19:16:58.608490 Step[4700/6995], lr: 0.001000, mv_avg_loss: 35.951649, loss: 26.773880
19:17:03.359878 Step[4800/6995], lr: 0.001000, mv_avg_loss: 34.289520, loss: 43.096092
19:17:08.109949 Step[4900/6995], lr: 0.001000, mv_avg_loss: 30.316490, loss: 32.679321
19:17:12.855720 Step[5000/6995], lr: 0.001000, mv_avg_loss: 33.494862, loss: 23.457390
19:17:17.590444 Step[5100/6995], lr: 0.001000, mv_avg_loss: 34.280724, loss: 39.814262
19:17:22.355799 Step[5200/6995], lr: 0.001000, mv_avg_loss: 34.780922, loss: 29.305244
19:17:27.086037 Step[5300/6995], lr: 0.001000, mv_avg_loss: 32.290577, loss: 66.810387
19:17:31.836425 Step[5400/6995], lr: 0.001000, mv_avg_loss: 31.151760, loss: 24.008514
19:17:36.579060 Step[5500/6995], lr: 0.001000, mv_avg_loss: 31.887672, loss: 24.540686
19:17:41.320032 Step[5600/6995], lr: 0.001000, mv_avg_loss: 31.601627, loss: 29.296026
19:17:46.056026 Step[5700/6995], lr: 0.001000, mv_avg_loss: 38.226410, loss: 37.256851
19:17:50.795652 Step[5800/6995], lr: 0.001000, mv_avg_loss: 33.047981, loss: 27.978363
19:17:55.526286 Step[5900/6995], lr: 0.001000, mv_avg_loss: 32.377720, loss: 62.030609
19:18:00.270257 Step[6000/6995], lr: 0.001000, mv_avg_loss: 37.220352, loss: 54.603741
19:18:05.005435 Step[6100/6995], lr: 0.001000, mv_avg_loss: 35.295189, loss: 38.291817
19:18:09.740396 Step[6200/6995], lr: 0.001000, mv_avg_loss: 29.576189, loss: 45.563473
19:18:14.483965 Step[6300/6995], lr: 0.001000, mv_avg_loss: 30.627401, loss: 22.740562
19:18:19.220666 Step[6400/6995], lr: 0.001000, mv_avg_loss: 34.666973, loss: 35.726501
19:18:23.952998 Step[6500/6995], lr: 0.001000, mv_avg_loss: 34.756672, loss: 29.619438
19:18:28.712109 Step[6600/6995], lr: 0.001000, mv_avg_loss: 31.230408, loss: 33.388653
19:18:33.448204 Step[6700/6995], lr: 0.001000, mv_avg_loss: 30.676100, loss: 43.309776
19:18:38.189634 Step[6800/6995], lr: 0.001000, mv_avg_loss: 32.701828, loss: 47.511471
19:18:42.927053 Step[6900/6995], lr: 0.001000, mv_avg_loss: 31.507511, loss: 36.686653
saving model for epoch 24
Testing for epoch: 24
Average test PNSR is 25.456041 for 500 images
Start to train epoch 25
19:19:08.346371 Step[0/6995], lr: 0.001000, mv_avg_loss: 30.592508, loss: 21.839874
19:19:13.060672 Step[100/6995], lr: 0.001000, mv_avg_loss: 32.126930, loss: 40.317120
19:19:17.808543 Step[200/6995], lr: 0.001000, mv_avg_loss: 32.300667, loss: 26.216492
19:19:22.519058 Step[300/6995], lr: 0.001000, mv_avg_loss: 29.828243, loss: 31.597698
19:19:27.228156 Step[400/6995], lr: 0.001000, mv_avg_loss: 36.200428, loss: 18.349653
19:19:31.945952 Step[500/6995], lr: 0.001000, mv_avg_loss: 31.154316, loss: 28.531530
19:19:36.677275 Step[600/6995], lr: 0.001000, mv_avg_loss: 30.928207, loss: 20.486364
19:19:41.411642 Step[700/6995], lr: 0.001000, mv_avg_loss: 37.506233, loss: 30.633827
19:19:46.217581 Step[800/6995], lr: 0.001000, mv_avg_loss: 31.076023, loss: 26.475882
19:19:50.960172 Step[900/6995], lr: 0.001000, mv_avg_loss: 35.555584, loss: 30.365185
19:19:55.727073 Step[1000/6995], lr: 0.001000, mv_avg_loss: 35.012299, loss: 21.635887
19:20:00.474689 Step[1100/6995], lr: 0.001000, mv_avg_loss: 31.545143, loss: 31.188002
19:20:05.234529 Step[1200/6995], lr: 0.001000, mv_avg_loss: 32.698830, loss: 58.673321
19:20:10.001681 Step[1300/6995], lr: 0.001000, mv_avg_loss: 31.082409, loss: 19.662916
19:20:14.763589 Step[1400/6995], lr: 0.001000, mv_avg_loss: 30.327576, loss: 48.855740
19:20:19.546723 Step[1500/6995], lr: 0.001000, mv_avg_loss: 31.252621, loss: 43.765732
19:20:24.327975 Step[1600/6995], lr: 0.001000, mv_avg_loss: 34.688389, loss: 30.992069
19:20:29.102151 Step[1700/6995], lr: 0.001000, mv_avg_loss: 32.798126, loss: 33.879776
19:20:33.879980 Step[1800/6995], lr: 0.001000, mv_avg_loss: 34.355122, loss: 21.817776
19:20:38.660336 Step[1900/6995], lr: 0.001000, mv_avg_loss: 37.356976, loss: 40.241016
19:20:43.443338 Step[2000/6995], lr: 0.001000, mv_avg_loss: 32.321739, loss: 33.371193
19:20:48.212562 Step[2100/6995], lr: 0.001000, mv_avg_loss: 35.296570, loss: 15.961386
19:20:52.974909 Step[2200/6995], lr: 0.001000, mv_avg_loss: 35.187794, loss: 25.504181
19:20:57.750835 Step[2300/6995], lr: 0.001000, mv_avg_loss: 32.073544, loss: 30.053360
19:21:02.538823 Step[2400/6995], lr: 0.001000, mv_avg_loss: 29.650953, loss: 29.977947
19:21:07.325177 Step[2500/6995], lr: 0.001000, mv_avg_loss: 33.553005, loss: 31.221205
19:21:12.108588 Step[2600/6995], lr: 0.001000, mv_avg_loss: 32.363514, loss: 40.408413
19:21:16.871428 Step[2700/6995], lr: 0.001000, mv_avg_loss: 36.916817, loss: 33.882908
19:21:21.654601 Step[2800/6995], lr: 0.001000, mv_avg_loss: 27.274670, loss: 20.662998
19:21:26.438474 Step[2900/6995], lr: 0.001000, mv_avg_loss: 30.293169, loss: 20.302055
19:21:31.218692 Step[3000/6995], lr: 0.001000, mv_avg_loss: 32.418907, loss: 31.430222
19:21:35.993690 Step[3100/6995], lr: 0.001000, mv_avg_loss: 36.585697, loss: 26.642532
19:21:40.787753 Step[3200/6995], lr: 0.001000, mv_avg_loss: 33.387089, loss: 50.800426
19:21:45.577647 Step[3300/6995], lr: 0.001000, mv_avg_loss: 30.063784, loss: 24.381989
19:21:50.349603 Step[3400/6995], lr: 0.001000, mv_avg_loss: 33.202278, loss: 24.997433
19:21:55.126445 Step[3500/6995], lr: 0.001000, mv_avg_loss: 29.816120, loss: 30.753586
19:21:59.912206 Step[3600/6995], lr: 0.001000, mv_avg_loss: 31.246708, loss: 22.596722
19:22:04.704068 Step[3700/6995], lr: 0.001000, mv_avg_loss: 29.776964, loss: 24.065388
19:22:09.498805 Step[3800/6995], lr: 0.001000, mv_avg_loss: 35.184937, loss: 53.398445
19:22:14.285824 Step[3900/6995], lr: 0.001000, mv_avg_loss: 34.463711, loss: 41.583275
19:22:19.078428 Step[4000/6995], lr: 0.001000, mv_avg_loss: 33.756496, loss: 26.203411
19:22:23.883914 Step[4100/6995], lr: 0.001000, mv_avg_loss: 31.468700, loss: 32.394402
19:22:28.662704 Step[4200/6995], lr: 0.001000, mv_avg_loss: 31.750299, loss: 24.371679
19:22:33.444048 Step[4300/6995], lr: 0.001000, mv_avg_loss: 28.845995, loss: 36.037426
19:22:38.222428 Step[4400/6995], lr: 0.001000, mv_avg_loss: 31.869064, loss: 30.024235
19:22:43.016932 Step[4500/6995], lr: 0.001000, mv_avg_loss: 31.850536, loss: 25.399475
19:22:47.818022 Step[4600/6995], lr: 0.001000, mv_avg_loss: 32.599121, loss: 27.627697
19:22:52.608077 Step[4700/6995], lr: 0.001000, mv_avg_loss: 32.108177, loss: 17.180397
19:22:57.407525 Step[4800/6995], lr: 0.001000, mv_avg_loss: 37.536480, loss: 38.003937
19:23:02.200076 Step[4900/6995], lr: 0.001000, mv_avg_loss: 32.049435, loss: 36.896858
19:23:06.999247 Step[5000/6995], lr: 0.001000, mv_avg_loss: 30.057201, loss: 35.161568
19:23:11.801502 Step[5100/6995], lr: 0.001000, mv_avg_loss: 33.026134, loss: 41.299774
19:23:16.633886 Step[5200/6995], lr: 0.001000, mv_avg_loss: 31.034550, loss: 21.027321
19:23:21.437024 Step[5300/6995], lr: 0.001000, mv_avg_loss: 33.679211, loss: 31.165676
19:23:26.232388 Step[5400/6995], lr: 0.001000, mv_avg_loss: 31.414082, loss: 38.362617
19:23:31.030010 Step[5500/6995], lr: 0.001000, mv_avg_loss: 31.526892, loss: 17.904343
19:23:35.820098 Step[5600/6995], lr: 0.001000, mv_avg_loss: 28.558388, loss: 39.892807
19:23:40.603657 Step[5700/6995], lr: 0.001000, mv_avg_loss: 36.960850, loss: 26.523933
19:23:45.384196 Step[5800/6995], lr: 0.001000, mv_avg_loss: 30.791744, loss: 29.123299
19:23:50.161880 Step[5900/6995], lr: 0.001000, mv_avg_loss: 35.410389, loss: 27.081047
19:23:54.950802 Step[6000/6995], lr: 0.001000, mv_avg_loss: 33.826023, loss: 37.627853
19:23:59.741386 Step[6100/6995], lr: 0.001000, mv_avg_loss: 30.838459, loss: 25.522808
19:24:04.545713 Step[6200/6995], lr: 0.001000, mv_avg_loss: 32.499321, loss: 36.063503
19:24:09.339717 Step[6300/6995], lr: 0.001000, mv_avg_loss: 30.802265, loss: 31.844513
19:24:14.136828 Step[6400/6995], lr: 0.001000, mv_avg_loss: 30.951925, loss: 22.833771
19:24:18.930200 Step[6500/6995], lr: 0.001000, mv_avg_loss: 30.118221, loss: 33.695248
19:24:23.717476 Step[6600/6995], lr: 0.001000, mv_avg_loss: 30.301266, loss: 20.185526
19:24:28.517829 Step[6700/6995], lr: 0.001000, mv_avg_loss: 31.081112, loss: 23.487997
19:24:33.316450 Step[6800/6995], lr: 0.001000, mv_avg_loss: 33.607857, loss: 31.890997
19:24:38.111979 Step[6900/6995], lr: 0.001000, mv_avg_loss: 32.403023, loss: 29.271421
Testing for epoch: 25
Average test PNSR is 25.351158 for 500 images
Start to train epoch 26
19:25:04.689808 Step[0/6995], lr: 0.001000, mv_avg_loss: 35.953239, loss: 36.484131
19:25:09.480658 Step[100/6995], lr: 0.001000, mv_avg_loss: 27.965349, loss: 19.227325
19:25:14.283523 Step[200/6995], lr: 0.001000, mv_avg_loss: 32.843803, loss: 29.045752
19:25:19.070225 Step[300/6995], lr: 0.001000, mv_avg_loss: 35.006142, loss: 22.442287
19:25:23.882015 Step[400/6995], lr: 0.001000, mv_avg_loss: 30.037931, loss: 26.215336
19:25:28.691633 Step[500/6995], lr: 0.001000, mv_avg_loss: 31.632355, loss: 37.816017
19:25:33.485197 Step[600/6995], lr: 0.001000, mv_avg_loss: 30.557220, loss: 39.860023
19:25:38.291310 Step[700/6995], lr: 0.001000, mv_avg_loss: 30.949255, loss: 27.321648
19:25:43.079129 Step[800/6995], lr: 0.001000, mv_avg_loss: 28.024994, loss: 24.187893
19:25:47.881214 Step[900/6995], lr: 0.001000, mv_avg_loss: 31.179359, loss: 42.579369
19:25:52.684151 Step[1000/6995], lr: 0.001000, mv_avg_loss: 27.953691, loss: 25.625599
19:25:57.481695 Step[1100/6995], lr: 0.001000, mv_avg_loss: 27.001318, loss: 24.117245
19:26:02.282863 Step[1200/6995], lr: 0.001000, mv_avg_loss: 31.677343, loss: 27.987951
19:26:07.090055 Step[1300/6995], lr: 0.001000, mv_avg_loss: 30.211239, loss: 31.076595
19:26:11.908905 Step[1400/6995], lr: 0.001000, mv_avg_loss: 30.611067, loss: 35.335781
19:26:16.713729 Step[1500/6995], lr: 0.001000, mv_avg_loss: 29.995878, loss: 44.745544
19:26:21.524727 Step[1600/6995], lr: 0.001000, mv_avg_loss: 37.508968, loss: 78.121628
19:26:26.319555 Step[1700/6995], lr: 0.001000, mv_avg_loss: 31.965624, loss: 24.117422
19:26:31.117187 Step[1800/6995], lr: 0.001000, mv_avg_loss: 33.869205, loss: 21.770468
19:26:35.925793 Step[1900/6995], lr: 0.001000, mv_avg_loss: 36.173992, loss: 34.153248
19:26:40.732041 Step[2000/6995], lr: 0.001000, mv_avg_loss: 30.799801, loss: 22.538124
19:26:45.540068 Step[2100/6995], lr: 0.001000, mv_avg_loss: 32.670666, loss: 26.585180
19:26:50.326089 Step[2200/6995], lr: 0.001000, mv_avg_loss: 30.870041, loss: 26.457952
19:26:55.120074 Step[2300/6995], lr: 0.001000, mv_avg_loss: 30.329926, loss: 28.657303
19:26:59.920036 Step[2400/6995], lr: 0.001000, mv_avg_loss: 28.220381, loss: 39.525799
19:27:04.710400 Step[2500/6995], lr: 0.001000, mv_avg_loss: 35.918751, loss: 81.142700
19:27:09.515591 Step[2600/6995], lr: 0.001000, mv_avg_loss: 34.306408, loss: 34.443981
19:27:14.315492 Step[2700/6995], lr: 0.001000, mv_avg_loss: 31.662750, loss: 26.781506
19:27:19.119782 Step[2800/6995], lr: 0.001000, mv_avg_loss: 29.922306, loss: 31.821112
19:27:23.909694 Step[2900/6995], lr: 0.001000, mv_avg_loss: 32.261620, loss: 26.694220
19:27:28.709573 Step[3000/6995], lr: 0.001000, mv_avg_loss: 31.271931, loss: 49.844955
19:27:33.520459 Step[3100/6995], lr: 0.001000, mv_avg_loss: 32.358395, loss: 23.003857
19:27:38.360071 Step[3200/6995], lr: 0.001000, mv_avg_loss: 36.572216, loss: 28.592026
19:27:43.171399 Step[3300/6995], lr: 0.001000, mv_avg_loss: 30.272148, loss: 19.744423
19:27:47.966932 Step[3400/6995], lr: 0.001000, mv_avg_loss: 30.964239, loss: 61.479351
19:27:52.773305 Step[3500/6995], lr: 0.001000, mv_avg_loss: 30.609032, loss: 30.884228
19:27:57.586222 Step[3600/6995], lr: 0.001000, mv_avg_loss: 36.456062, loss: 52.445297
19:28:02.380628 Step[3700/6995], lr: 0.001000, mv_avg_loss: 35.205807, loss: 53.170822
19:28:07.186971 Step[3800/6995], lr: 0.001000, mv_avg_loss: 40.793777, loss: 59.539948
19:28:11.998093 Step[3900/6995], lr: 0.001000, mv_avg_loss: 33.291019, loss: 23.038160
19:28:16.805614 Step[4000/6995], lr: 0.001000, mv_avg_loss: 33.286724, loss: 42.728786
19:28:21.598592 Step[4100/6995], lr: 0.001000, mv_avg_loss: 30.871445, loss: 32.686672
19:28:26.404711 Step[4200/6995], lr: 0.001000, mv_avg_loss: 30.903471, loss: 32.143749
19:28:31.211590 Step[4300/6995], lr: 0.001000, mv_avg_loss: 28.956285, loss: 29.396606
19:28:36.012185 Step[4400/6995], lr: 0.001000, mv_avg_loss: 29.989267, loss: 30.267561
19:28:40.818235 Step[4500/6995], lr: 0.001000, mv_avg_loss: 27.913445, loss: 24.699234
19:28:45.615963 Step[4600/6995], lr: 0.001000, mv_avg_loss: 35.309795, loss: 18.286596
19:28:50.412227 Step[4700/6995], lr: 0.001000, mv_avg_loss: 30.731497, loss: 20.664879
19:28:55.221899 Step[4800/6995], lr: 0.001000, mv_avg_loss: 28.722574, loss: 20.867439
19:29:00.018398 Step[4900/6995], lr: 0.001000, mv_avg_loss: 31.576611, loss: 43.540344
19:29:04.812498 Step[5000/6995], lr: 0.001000, mv_avg_loss: 31.232540, loss: 25.599411
19:29:09.615676 Step[5100/6995], lr: 0.001000, mv_avg_loss: 27.698660, loss: 22.233082
19:29:14.412180 Step[5200/6995], lr: 0.001000, mv_avg_loss: 30.485464, loss: 34.855927
19:29:19.203281 Step[5300/6995], lr: 0.001000, mv_avg_loss: 32.677227, loss: 32.003693
19:29:23.991998 Step[5400/6995], lr: 0.001000, mv_avg_loss: 30.722311, loss: 29.097691
19:29:28.775140 Step[5500/6995], lr: 0.001000, mv_avg_loss: 28.480757, loss: 19.592558
19:29:33.568120 Step[5600/6995], lr: 0.001000, mv_avg_loss: 28.441658, loss: 23.675390
19:29:38.368921 Step[5700/6995], lr: 0.001000, mv_avg_loss: 32.725853, loss: 37.842991
19:29:43.165631 Step[5800/6995], lr: 0.001000, mv_avg_loss: 34.066212, loss: 23.847052
19:29:47.971952 Step[5900/6995], lr: 0.001000, mv_avg_loss: 32.625320, loss: 55.000763
19:29:52.760433 Step[6000/6995], lr: 0.001000, mv_avg_loss: 28.248631, loss: 28.716154
19:29:57.568415 Step[6100/6995], lr: 0.001000, mv_avg_loss: 32.845707, loss: 58.341900
19:30:02.359842 Step[6200/6995], lr: 0.001000, mv_avg_loss: 28.725460, loss: 30.861290
19:30:07.155138 Step[6300/6995], lr: 0.001000, mv_avg_loss: 34.153252, loss: 20.194666
19:30:11.946276 Step[6400/6995], lr: 0.001000, mv_avg_loss: 32.052147, loss: 33.554588
19:30:16.731577 Step[6500/6995], lr: 0.001000, mv_avg_loss: 33.550209, loss: 29.620998
19:30:21.522303 Step[6600/6995], lr: 0.001000, mv_avg_loss: 34.140957, loss: 29.920019
19:30:26.316870 Step[6700/6995], lr: 0.001000, mv_avg_loss: 42.987869, loss: 53.701965
19:30:31.118654 Step[6800/6995], lr: 0.001000, mv_avg_loss: 33.052959, loss: 27.015236
19:30:35.903235 Step[6900/6995], lr: 0.001000, mv_avg_loss: 30.987753, loss: 38.749672
Testing for epoch: 26
Average test PNSR is 25.186162 for 500 images
Start to train epoch 27
19:31:02.474143 Step[0/6995], lr: 0.001000, mv_avg_loss: 29.451694, loss: 29.656677
19:31:07.238047 Step[100/6995], lr: 0.001000, mv_avg_loss: 28.472847, loss: 31.295761
19:31:12.009093 Step[200/6995], lr: 0.001000, mv_avg_loss: 26.284054, loss: 41.742077
19:31:16.776957 Step[300/6995], lr: 0.001000, mv_avg_loss: 29.619774, loss: 17.973099
19:31:21.554864 Step[400/6995], lr: 0.001000, mv_avg_loss: 29.323009, loss: 26.488401
19:31:26.347397 Step[500/6995], lr: 0.001000, mv_avg_loss: 28.454618, loss: 33.592403
19:31:31.109800 Step[600/6995], lr: 0.001000, mv_avg_loss: 30.517513, loss: 33.841240
19:31:35.890733 Step[700/6995], lr: 0.001000, mv_avg_loss: 28.402452, loss: 13.785002
19:31:40.666990 Step[800/6995], lr: 0.001000, mv_avg_loss: 37.195087, loss: 29.947920
19:31:45.443924 Step[900/6995], lr: 0.001000, mv_avg_loss: 28.633507, loss: 35.327538
19:31:50.215593 Step[1000/6995], lr: 0.001000, mv_avg_loss: 28.593166, loss: 26.696674
19:31:54.994414 Step[1100/6995], lr: 0.001000, mv_avg_loss: 31.851789, loss: 15.427149
19:31:59.797454 Step[1200/6995], lr: 0.001000, mv_avg_loss: 29.612711, loss: 44.615631
19:32:04.560963 Step[1300/6995], lr: 0.001000, mv_avg_loss: 30.923159, loss: 22.537628
19:32:09.337271 Step[1400/6995], lr: 0.001000, mv_avg_loss: 34.142372, loss: 18.813816
19:32:14.122879 Step[1500/6995], lr: 0.001000, mv_avg_loss: 28.885412, loss: 24.746746
19:32:18.900092 Step[1600/6995], lr: 0.001000, mv_avg_loss: 29.687613, loss: 37.015114
19:32:23.680748 Step[1700/6995], lr: 0.001000, mv_avg_loss: 36.461082, loss: 28.303854
19:32:28.461909 Step[1800/6995], lr: 0.001000, mv_avg_loss: 29.008774, loss: 27.034529
19:32:33.221093 Step[1900/6995], lr: 0.001000, mv_avg_loss: 30.257864, loss: 23.730911
19:32:37.998337 Step[2000/6995], lr: 0.001000, mv_avg_loss: 33.005047, loss: 30.099081
19:32:42.782450 Step[2100/6995], lr: 0.001000, mv_avg_loss: 29.466627, loss: 25.569656
19:32:47.552772 Step[2200/6995], lr: 0.001000, mv_avg_loss: 28.086674, loss: 43.117371
19:32:52.315143 Step[2300/6995], lr: 0.001000, mv_avg_loss: 28.994020, loss: 36.870171
19:32:57.110044 Step[2400/6995], lr: 0.001000, mv_avg_loss: 30.813572, loss: 18.727463
19:33:01.882627 Step[2500/6995], lr: 0.001000, mv_avg_loss: 29.704403, loss: 25.993683
19:33:06.657616 Step[2600/6995], lr: 0.001000, mv_avg_loss: 32.444252, loss: 32.670708
19:33:11.435026 Step[2700/6995], lr: 0.001000, mv_avg_loss: 29.254974, loss: 26.610540
19:33:16.215047 Step[2800/6995], lr: 0.001000, mv_avg_loss: 27.873386, loss: 27.167274
19:33:20.987615 Step[2900/6995], lr: 0.001000, mv_avg_loss: 31.893053, loss: 36.061188
19:33:25.764811 Step[3000/6995], lr: 0.001000, mv_avg_loss: 37.753353, loss: 24.368065
19:33:30.540784 Step[3100/6995], lr: 0.001000, mv_avg_loss: 31.177614, loss: 25.752598
19:33:35.330562 Step[3200/6995], lr: 0.001000, mv_avg_loss: 31.699995, loss: 49.967663
19:33:40.101835 Step[3300/6995], lr: 0.001000, mv_avg_loss: 35.407352, loss: 23.969725
19:33:44.877829 Step[3400/6995], lr: 0.001000, mv_avg_loss: 28.990524, loss: 34.740215
19:33:49.661287 Step[3500/6995], lr: 0.001000, mv_avg_loss: 27.184982, loss: 19.641201
19:33:54.439549 Step[3600/6995], lr: 0.001000, mv_avg_loss: 30.367172, loss: 21.581955
19:33:59.212734 Step[3700/6995], lr: 0.001000, mv_avg_loss: 30.779900, loss: 18.010380
19:34:03.981817 Step[3800/6995], lr: 0.001000, mv_avg_loss: 31.835468, loss: 62.203552
19:34:08.772267 Step[3900/6995], lr: 0.001000, mv_avg_loss: 33.953617, loss: 25.286983
19:34:13.553244 Step[4000/6995], lr: 0.001000, mv_avg_loss: 32.174110, loss: 30.021187
19:34:18.335282 Step[4100/6995], lr: 0.001000, mv_avg_loss: 30.675016, loss: 19.322601
19:34:23.106568 Step[4200/6995], lr: 0.001000, mv_avg_loss: 32.518604, loss: 44.272896
19:34:27.887697 Step[4300/6995], lr: 0.001000, mv_avg_loss: 33.206703, loss: 31.947842
19:34:32.662370 Step[4400/6995], lr: 0.001000, mv_avg_loss: 28.013790, loss: 24.282700
19:34:37.442097 Step[4500/6995], lr: 0.001000, mv_avg_loss: 28.938707, loss: 22.074156
19:34:42.231457 Step[4600/6995], lr: 0.001000, mv_avg_loss: 28.397488, loss: 30.692778
19:34:47.007652 Step[4700/6995], lr: 0.001000, mv_avg_loss: 29.474899, loss: 39.423431
19:34:51.782836 Step[4800/6995], lr: 0.001000, mv_avg_loss: 28.016577, loss: 29.013184
19:34:56.560610 Step[4900/6995], lr: 0.001000, mv_avg_loss: 29.898394, loss: 32.739941
19:35:01.339484 Step[5000/6995], lr: 0.001000, mv_avg_loss: 32.933502, loss: 21.736439
19:35:06.123404 Step[5100/6995], lr: 0.001000, mv_avg_loss: 32.181038, loss: 25.268887
19:35:11.023183 Step[5200/6995], lr: 0.001000, mv_avg_loss: 31.179977, loss: 28.477745
19:35:15.812028 Step[5300/6995], lr: 0.001000, mv_avg_loss: 33.268250, loss: 32.953053
19:35:20.589432 Step[5400/6995], lr: 0.001000, mv_avg_loss: 27.391762, loss: 22.492306
19:35:25.365056 Step[5500/6995], lr: 0.001000, mv_avg_loss: 35.142117, loss: 32.637970
19:35:30.144189 Step[5600/6995], lr: 0.001000, mv_avg_loss: 28.593517, loss: 25.021627
19:35:34.923110 Step[5700/6995], lr: 0.001000, mv_avg_loss: 28.934269, loss: 16.004087
19:35:39.696661 Step[5800/6995], lr: 0.001000, mv_avg_loss: 33.269268, loss: 27.304241
19:35:44.487505 Step[5900/6995], lr: 0.001000, mv_avg_loss: 33.050587, loss: 41.223747
19:35:49.268691 Step[6000/6995], lr: 0.001000, mv_avg_loss: 28.998711, loss: 21.004585
19:35:54.041391 Step[6100/6995], lr: 0.001000, mv_avg_loss: 34.688423, loss: 35.924160
19:35:58.872621 Step[6200/6995], lr: 0.001000, mv_avg_loss: 30.572241, loss: 37.025543
19:36:03.653458 Step[6300/6995], lr: 0.001000, mv_avg_loss: 31.238903, loss: 27.239040
19:36:08.435122 Step[6400/6995], lr: 0.001000, mv_avg_loss: 29.949230, loss: 19.948631
19:36:13.226621 Step[6500/6995], lr: 0.001000, mv_avg_loss: 29.421801, loss: 22.699057
19:36:18.003954 Step[6600/6995], lr: 0.001000, mv_avg_loss: 30.063334, loss: 23.200397
19:36:22.791821 Step[6700/6995], lr: 0.001000, mv_avg_loss: 31.228762, loss: 29.337400
19:36:27.589146 Step[6800/6995], lr: 0.001000, mv_avg_loss: 28.628105, loss: 68.699783
19:36:32.381661 Step[6900/6995], lr: 0.001000, mv_avg_loss: 29.341787, loss: 27.890881
Testing for epoch: 27
Average test PNSR is 25.283380 for 500 images
Start to train epoch 28
19:36:59.062857 Step[0/6995], lr: 0.001000, mv_avg_loss: 33.225513, loss: 23.253164
19:37:03.845101 Step[100/6995], lr: 0.001000, mv_avg_loss: 28.212065, loss: 25.659903
19:37:08.626405 Step[200/6995], lr: 0.001000, mv_avg_loss: 27.687571, loss: 26.736616
19:37:13.399154 Step[300/6995], lr: 0.001000, mv_avg_loss: 30.873051, loss: 32.354183
19:37:18.176142 Step[400/6995], lr: 0.001000, mv_avg_loss: 28.619331, loss: 39.979172
19:37:22.965954 Step[500/6995], lr: 0.001000, mv_avg_loss: 28.811684, loss: 26.735077
19:37:27.752101 Step[600/6995], lr: 0.001000, mv_avg_loss: 27.579777, loss: 40.724911
19:37:32.533493 Step[700/6995], lr: 0.001000, mv_avg_loss: 27.103621, loss: 24.489742
19:37:37.318961 Step[800/6995], lr: 0.001000, mv_avg_loss: 28.633549, loss: 15.817305
19:37:42.114829 Step[900/6995], lr: 0.001000, mv_avg_loss: 31.078861, loss: 31.100904
19:37:46.894311 Step[1000/6995], lr: 0.001000, mv_avg_loss: 29.313103, loss: 17.570076
19:37:51.687702 Step[1100/6995], lr: 0.001000, mv_avg_loss: 26.843002, loss: 43.150108
19:37:56.477422 Step[1200/6995], lr: 0.001000, mv_avg_loss: 31.139641, loss: 26.223104
19:38:01.267657 Step[1300/6995], lr: 0.001000, mv_avg_loss: 29.471987, loss: 25.503185
19:38:06.063476 Step[1400/6995], lr: 0.001000, mv_avg_loss: 31.183603, loss: 21.052282
19:38:10.860914 Step[1500/6995], lr: 0.001000, mv_avg_loss: 31.995073, loss: 23.050034
19:38:15.652105 Step[1600/6995], lr: 0.001000, mv_avg_loss: 30.566904, loss: 29.675529
19:38:20.440467 Step[1700/6995], lr: 0.001000, mv_avg_loss: 32.637474, loss: 47.850273
19:38:25.236471 Step[1800/6995], lr: 0.001000, mv_avg_loss: 31.074280, loss: 45.675762
19:38:30.034133 Step[1900/6995], lr: 0.001000, mv_avg_loss: 30.447590, loss: 28.557621
19:38:34.839825 Step[2000/6995], lr: 0.001000, mv_avg_loss: 29.413141, loss: 28.228539
19:38:39.636108 Step[2100/6995], lr: 0.001000, mv_avg_loss: 25.606310, loss: 18.678661
19:38:44.430953 Step[2200/6995], lr: 0.001000, mv_avg_loss: 30.014654, loss: 22.837301
19:38:49.230000 Step[2300/6995], lr: 0.001000, mv_avg_loss: 31.444145, loss: 37.980362
19:38:54.035222 Step[2400/6995], lr: 0.001000, mv_avg_loss: 28.322065, loss: 26.196262
19:38:58.848854 Step[2500/6995], lr: 0.001000, mv_avg_loss: 27.351738, loss: 27.003601
19:39:03.643800 Step[2600/6995], lr: 0.001000, mv_avg_loss: 27.403555, loss: 19.697952
19:39:08.451401 Step[2700/6995], lr: 0.001000, mv_avg_loss: 29.953680, loss: 35.279335
19:39:13.260907 Step[2800/6995], lr: 0.001000, mv_avg_loss: 31.479033, loss: 33.581306
19:39:18.085769 Step[2900/6995], lr: 0.001000, mv_avg_loss: 33.765652, loss: 36.505730
19:39:22.886574 Step[3000/6995], lr: 0.001000, mv_avg_loss: 26.714243, loss: 18.789154
19:39:27.692749 Step[3100/6995], lr: 0.001000, mv_avg_loss: 33.140057, loss: 21.224857
19:39:32.495996 Step[3200/6995], lr: 0.001000, mv_avg_loss: 32.437740, loss: 27.548010
19:39:37.297811 Step[3300/6995], lr: 0.001000, mv_avg_loss: 36.876942, loss: 52.985527
19:39:42.101065 Step[3400/6995], lr: 0.001000, mv_avg_loss: 30.662893, loss: 25.203419
19:39:46.903768 Step[3500/6995], lr: 0.001000, mv_avg_loss: 31.738937, loss: 22.783764
19:39:51.708878 Step[3600/6995], lr: 0.001000, mv_avg_loss: 31.228762, loss: 34.847424
19:39:56.503469 Step[3700/6995], lr: 0.001000, mv_avg_loss: 32.947117, loss: 44.205208
19:40:01.310828 Step[3800/6995], lr: 0.001000, mv_avg_loss: 31.157822, loss: 25.202698
19:40:06.116349 Step[3900/6995], lr: 0.001000, mv_avg_loss: 29.354410, loss: 33.129906
19:40:10.909100 Step[4000/6995], lr: 0.001000, mv_avg_loss: 32.502544, loss: 25.831671
19:40:15.721513 Step[4100/6995], lr: 0.001000, mv_avg_loss: 28.211967, loss: 23.887539
19:40:20.549976 Step[4200/6995], lr: 0.001000, mv_avg_loss: 30.825500, loss: 29.753162
19:40:25.351892 Step[4300/6995], lr: 0.001000, mv_avg_loss: 30.544212, loss: 23.883209
19:40:30.153747 Step[4400/6995], lr: 0.001000, mv_avg_loss: 29.463940, loss: 28.336075
19:40:34.954169 Step[4500/6995], lr: 0.001000, mv_avg_loss: 31.175171, loss: 31.367367
19:40:39.760518 Step[4600/6995], lr: 0.001000, mv_avg_loss: 35.000294, loss: 29.862141
19:40:44.560892 Step[4700/6995], lr: 0.001000, mv_avg_loss: 31.931345, loss: 31.054703
19:40:49.365015 Step[4800/6995], lr: 0.001000, mv_avg_loss: 30.604387, loss: 24.024490
19:40:54.157084 Step[4900/6995], lr: 0.001000, mv_avg_loss: 30.842583, loss: 35.196869
19:40:58.949005 Step[5000/6995], lr: 0.001000, mv_avg_loss: 29.874308, loss: 24.261555
19:41:03.756286 Step[5100/6995], lr: 0.001000, mv_avg_loss: 30.580280, loss: 30.721313
19:41:08.566821 Step[5200/6995], lr: 0.001000, mv_avg_loss: 29.882057, loss: 31.705389
19:41:13.364198 Step[5300/6995], lr: 0.001000, mv_avg_loss: 29.346613, loss: 23.313427
19:41:18.167949 Step[5400/6995], lr: 0.001000, mv_avg_loss: 29.126032, loss: 17.417074
19:41:22.968868 Step[5500/6995], lr: 0.001000, mv_avg_loss: 26.681126, loss: 37.806850
19:41:27.758995 Step[5600/6995], lr: 0.001000, mv_avg_loss: 30.154203, loss: 30.754248
19:41:32.560897 Step[5700/6995], lr: 0.001000, mv_avg_loss: 32.026756, loss: 25.879364
19:41:37.378711 Step[5800/6995], lr: 0.001000, mv_avg_loss: 31.381699, loss: 69.666718
19:41:42.180136 Step[5900/6995], lr: 0.001000, mv_avg_loss: 30.233343, loss: 28.221655
19:41:46.974067 Step[6000/6995], lr: 0.001000, mv_avg_loss: 28.174189, loss: 18.982758
19:41:51.783390 Step[6100/6995], lr: 0.001000, mv_avg_loss: 34.173962, loss: 26.747932
19:41:56.593078 Step[6200/6995], lr: 0.001000, mv_avg_loss: 31.063951, loss: 24.703283
19:42:01.391561 Step[6300/6995], lr: 0.001000, mv_avg_loss: 30.622633, loss: 28.357746
19:42:06.189992 Step[6400/6995], lr: 0.001000, mv_avg_loss: 27.043266, loss: 27.949875
19:42:10.988788 Step[6500/6995], lr: 0.001000, mv_avg_loss: 32.051708, loss: 41.031616
19:42:15.793091 Step[6600/6995], lr: 0.001000, mv_avg_loss: 31.280163, loss: 35.013870
19:42:20.594412 Step[6700/6995], lr: 0.001000, mv_avg_loss: 28.079489, loss: 23.301174
19:42:25.394505 Step[6800/6995], lr: 0.001000, mv_avg_loss: 25.198616, loss: 52.530754
19:42:30.198276 Step[6900/6995], lr: 0.001000, mv_avg_loss: 27.323982, loss: 27.568899
Testing for epoch: 28
Average test PNSR is 25.351170 for 500 images
Start to train epoch 29
19:42:56.969721 Step[0/6995], lr: 0.001000, mv_avg_loss: 26.547028, loss: 25.963181
19:43:01.762888 Step[100/6995], lr: 0.001000, mv_avg_loss: 26.830278, loss: 22.661831
19:43:06.551567 Step[200/6995], lr: 0.001000, mv_avg_loss: 27.838352, loss: 38.898911
19:43:11.337662 Step[300/6995], lr: 0.001000, mv_avg_loss: 26.111164, loss: 21.968372
19:43:16.122558 Step[400/6995], lr: 0.001000, mv_avg_loss: 30.783833, loss: 33.701031
19:43:20.907372 Step[500/6995], lr: 0.001000, mv_avg_loss: 31.223320, loss: 25.834126
19:43:25.703854 Step[600/6995], lr: 0.001000, mv_avg_loss: 31.221762, loss: 21.347914
19:43:30.497007 Step[700/6995], lr: 0.001000, mv_avg_loss: 29.441236, loss: 22.412285
19:43:35.296301 Step[800/6995], lr: 0.001000, mv_avg_loss: 27.422443, loss: 22.215889
19:43:40.077708 Step[900/6995], lr: 0.001000, mv_avg_loss: 32.021000, loss: 31.682980
19:43:44.869616 Step[1000/6995], lr: 0.001000, mv_avg_loss: 26.778530, loss: 31.117371
19:43:49.669856 Step[1100/6995], lr: 0.001000, mv_avg_loss: 27.714342, loss: 25.651257
19:43:54.459697 Step[1200/6995], lr: 0.001000, mv_avg_loss: 30.795685, loss: 23.744366
19:43:59.255504 Step[1300/6995], lr: 0.001000, mv_avg_loss: 32.745304, loss: 24.776396
19:44:04.035645 Step[1400/6995], lr: 0.001000, mv_avg_loss: 30.340904, loss: 24.248871
19:44:08.832575 Step[1500/6995], lr: 0.001000, mv_avg_loss: 30.078630, loss: 31.782871
19:44:13.629776 Step[1600/6995], lr: 0.001000, mv_avg_loss: 29.334789, loss: 22.866676
19:44:18.431941 Step[1700/6995], lr: 0.001000, mv_avg_loss: 36.576233, loss: 42.060989
19:44:23.232598 Step[1800/6995], lr: 0.001000, mv_avg_loss: 31.573456, loss: 29.966177
19:44:28.028669 Step[1900/6995], lr: 0.001000, mv_avg_loss: 28.502977, loss: 20.468784
19:44:32.810849 Step[2000/6995], lr: 0.001000, mv_avg_loss: 27.845144, loss: 37.360985
19:44:37.606220 Step[2100/6995], lr: 0.001000, mv_avg_loss: 26.196735, loss: 25.527107
19:44:42.428826 Step[2200/6995], lr: 0.001000, mv_avg_loss: 27.976288, loss: 31.843979
19:44:47.208403 Step[2300/6995], lr: 0.001000, mv_avg_loss: 27.976862, loss: 17.772169
19:44:52.005352 Step[2400/6995], lr: 0.001000, mv_avg_loss: 28.506689, loss: 36.895012
19:44:56.785666 Step[2500/6995], lr: 0.001000, mv_avg_loss: 30.217007, loss: 59.669323
19:45:01.578775 Step[2600/6995], lr: 0.001000, mv_avg_loss: 28.365990, loss: 28.084534
19:45:06.377956 Step[2700/6995], lr: 0.001000, mv_avg_loss: 30.713305, loss: 25.035534
19:45:11.164906 Step[2800/6995], lr: 0.001000, mv_avg_loss: 30.783194, loss: 17.966911
19:45:15.969348 Step[2900/6995], lr: 0.001000, mv_avg_loss: 30.720261, loss: 20.785448
19:45:20.747679 Step[3000/6995], lr: 0.001000, mv_avg_loss: 31.119432, loss: 32.959160
19:45:25.528482 Step[3100/6995], lr: 0.001000, mv_avg_loss: 30.433058, loss: 64.921112
19:45:30.310839 Step[3200/6995], lr: 0.001000, mv_avg_loss: 32.713478, loss: 22.894188
19:45:35.117523 Step[3300/6995], lr: 0.001000, mv_avg_loss: 28.506283, loss: 19.428801
19:45:39.896621 Step[3400/6995], lr: 0.001000, mv_avg_loss: 33.540318, loss: 49.957012
19:45:44.682536 Step[3500/6995], lr: 0.001000, mv_avg_loss: 42.767849, loss: 47.372025
19:45:49.476796 Step[3600/6995], lr: 0.001000, mv_avg_loss: 33.067623, loss: 25.288290
19:45:54.264173 Step[3700/6995], lr: 0.001000, mv_avg_loss: 30.737183, loss: 51.838341
19:45:59.039035 Step[3800/6995], lr: 0.001000, mv_avg_loss: 30.844107, loss: 27.771978
19:46:03.828614 Step[3900/6995], lr: 0.001000, mv_avg_loss: 31.296003, loss: 32.953228
19:46:08.610518 Step[4000/6995], lr: 0.001000, mv_avg_loss: 31.365759, loss: 27.391317
19:46:13.408062 Step[4100/6995], lr: 0.001000, mv_avg_loss: 29.806450, loss: 26.014343
19:46:18.206684 Step[4200/6995], lr: 0.001000, mv_avg_loss: 28.934410, loss: 52.918697
19:46:22.987781 Step[4300/6995], lr: 0.001000, mv_avg_loss: 25.177111, loss: 26.070236
19:46:27.776823 Step[4400/6995], lr: 0.001000, mv_avg_loss: 29.888828, loss: 27.647530
19:46:32.568565 Step[4500/6995], lr: 0.001000, mv_avg_loss: 26.980183, loss: 23.402052
19:46:37.355033 Step[4600/6995], lr: 0.001000, mv_avg_loss: 29.394388, loss: 21.460205
19:46:42.144326 Step[4700/6995], lr: 0.001000, mv_avg_loss: 30.658958, loss: 28.769562
19:46:46.931260 Step[4800/6995], lr: 0.001000, mv_avg_loss: 28.196756, loss: 23.606441
19:46:51.712615 Step[4900/6995], lr: 0.001000, mv_avg_loss: 29.211977, loss: 27.152733
19:46:56.492172 Step[5000/6995], lr: 0.001000, mv_avg_loss: 25.532784, loss: 14.882851
19:47:01.271335 Step[5100/6995], lr: 0.001000, mv_avg_loss: 30.600267, loss: 44.329735
19:47:06.060477 Step[5200/6995], lr: 0.001000, mv_avg_loss: 30.009605, loss: 48.848442
19:47:10.850346 Step[5300/6995], lr: 0.001000, mv_avg_loss: 26.211470, loss: 18.065754
19:47:15.642750 Step[5400/6995], lr: 0.001000, mv_avg_loss: 31.342590, loss: 26.315243
19:47:20.426935 Step[5500/6995], lr: 0.001000, mv_avg_loss: 28.097441, loss: 28.708464
19:47:25.208777 Step[5600/6995], lr: 0.001000, mv_avg_loss: 27.209362, loss: 24.617704
19:47:29.997406 Step[5700/6995], lr: 0.001000, mv_avg_loss: 32.066673, loss: 38.568768
19:47:34.787298 Step[5800/6995], lr: 0.001000, mv_avg_loss: 29.006533, loss: 32.936264
19:47:39.573045 Step[5900/6995], lr: 0.001000, mv_avg_loss: 32.360210, loss: 20.911091
19:47:44.360654 Step[6000/6995], lr: 0.001000, mv_avg_loss: 30.990030, loss: 20.874878
19:47:49.143337 Step[6100/6995], lr: 0.001000, mv_avg_loss: 32.445721, loss: 46.949135
19:47:53.939444 Step[6200/6995], lr: 0.001000, mv_avg_loss: 29.749790, loss: 23.176893
19:47:58.727086 Step[6300/6995], lr: 0.001000, mv_avg_loss: 30.071722, loss: 25.680761
19:48:03.512259 Step[6400/6995], lr: 0.001000, mv_avg_loss: 29.887506, loss: 21.923971
19:48:08.296708 Step[6500/6995], lr: 0.001000, mv_avg_loss: 30.086359, loss: 19.126625
19:48:13.076918 Step[6600/6995], lr: 0.001000, mv_avg_loss: 27.875761, loss: 20.412350
19:48:17.870455 Step[6700/6995], lr: 0.001000, mv_avg_loss: 28.581865, loss: 30.484947
19:48:22.672810 Step[6800/6995], lr: 0.001000, mv_avg_loss: 33.463184, loss: 27.792030
19:48:27.446961 Step[6900/6995], lr: 0.001000, mv_avg_loss: 36.505367, loss: 50.335449
saving model for epoch 29
Testing for epoch: 29
Average test PNSR is 25.393241 for 500 images
Start to train epoch 30
19:48:53.978953 Step[0/6995], lr: 0.001000, mv_avg_loss: 29.051592, loss: 24.467957
19:48:58.752992 Step[100/6995], lr: 0.001000, mv_avg_loss: 33.274227, loss: 20.194088
19:49:03.550585 Step[200/6995], lr: 0.001000, mv_avg_loss: 28.007660, loss: 15.836716
19:49:08.319178 Step[300/6995], lr: 0.001000, mv_avg_loss: 28.646889, loss: 31.807379
19:49:13.095674 Step[400/6995], lr: 0.001000, mv_avg_loss: 27.258972, loss: 15.753725
19:49:17.891201 Step[500/6995], lr: 0.001000, mv_avg_loss: 28.643671, loss: 44.439800
19:49:22.670251 Step[600/6995], lr: 0.001000, mv_avg_loss: 28.461191, loss: 32.993725
19:49:27.453893 Step[700/6995], lr: 0.001000, mv_avg_loss: 27.148144, loss: 22.727186
19:49:32.249689 Step[800/6995], lr: 0.001000, mv_avg_loss: 26.537235, loss: 22.222172
19:49:37.018457 Step[900/6995], lr: 0.001000, mv_avg_loss: 27.497362, loss: 35.687370
19:49:41.798128 Step[1000/6995], lr: 0.001000, mv_avg_loss: 24.887781, loss: 22.012230
19:49:46.584673 Step[1100/6995], lr: 0.001000, mv_avg_loss: 29.295614, loss: 37.981941
19:49:51.366611 Step[1200/6995], lr: 0.001000, mv_avg_loss: 29.395771, loss: 22.906040
19:49:56.147042 Step[1300/6995], lr: 0.001000, mv_avg_loss: 32.242184, loss: 23.710989
19:50:00.931399 Step[1400/6995], lr: 0.001000, mv_avg_loss: 27.855940, loss: 52.833584
19:50:05.735934 Step[1500/6995], lr: 0.001000, mv_avg_loss: 25.829811, loss: 23.273884
19:50:10.517426 Step[1600/6995], lr: 0.001000, mv_avg_loss: 30.150414, loss: 34.463287
19:50:15.311710 Step[1700/6995], lr: 0.001000, mv_avg_loss: 25.557331, loss: 46.369957
19:50:20.105698 Step[1800/6995], lr: 0.001000, mv_avg_loss: 27.550531, loss: 26.507076
19:50:24.898524 Step[1900/6995], lr: 0.001000, mv_avg_loss: 25.379494, loss: 24.382580
19:50:29.699624 Step[2000/6995], lr: 0.001000, mv_avg_loss: 26.626661, loss: 35.545521
19:50:34.498412 Step[2100/6995], lr: 0.001000, mv_avg_loss: 27.584171, loss: 31.115910
19:50:39.286962 Step[2200/6995], lr: 0.001000, mv_avg_loss: 29.190685, loss: 27.954563
19:50:44.072491 Step[2300/6995], lr: 0.001000, mv_avg_loss: 26.402994, loss: 34.478481
19:50:48.873495 Step[2400/6995], lr: 0.001000, mv_avg_loss: 27.473860, loss: 22.109856
19:50:53.687548 Step[2500/6995], lr: 0.001000, mv_avg_loss: 28.645836, loss: 23.037186
19:50:58.483938 Step[2600/6995], lr: 0.001000, mv_avg_loss: 29.339891, loss: 22.475357
19:51:03.265849 Step[2700/6995], lr: 0.001000, mv_avg_loss: 30.764318, loss: 26.967178
19:51:08.072115 Step[2800/6995], lr: 0.001000, mv_avg_loss: 31.979353, loss: 22.675985
19:51:12.876448 Step[2900/6995], lr: 0.001000, mv_avg_loss: 25.220385, loss: 24.564466
19:51:17.688651 Step[3000/6995], lr: 0.001000, mv_avg_loss: 26.769173, loss: 29.944138
19:51:22.501644 Step[3100/6995], lr: 0.001000, mv_avg_loss: 29.602732, loss: 33.800739
19:51:27.310953 Step[3200/6995], lr: 0.001000, mv_avg_loss: 31.700888, loss: 28.066589
19:51:32.102570 Step[3300/6995], lr: 0.001000, mv_avg_loss: 28.941978, loss: 16.207630
19:51:36.896581 Step[3400/6995], lr: 0.001000, mv_avg_loss: 31.539097, loss: 50.026817
19:51:41.705564 Step[3500/6995], lr: 0.001000, mv_avg_loss: 30.173038, loss: 23.391119
19:51:46.517294 Step[3600/6995], lr: 0.001000, mv_avg_loss: 27.149624, loss: 27.941090
19:51:51.316903 Step[3700/6995], lr: 0.001000, mv_avg_loss: 25.207714, loss: 32.786140
19:51:56.129513 Step[3800/6995], lr: 0.001000, mv_avg_loss: 36.842976, loss: 56.564507
19:52:00.927376 Step[3900/6995], lr: 0.001000, mv_avg_loss: 30.000986, loss: 43.197968
19:52:05.731741 Step[4000/6995], lr: 0.001000, mv_avg_loss: 27.943806, loss: 28.871534
19:52:10.537895 Step[4100/6995], lr: 0.001000, mv_avg_loss: 26.043896, loss: 34.327873
19:52:15.353237 Step[4200/6995], lr: 0.001000, mv_avg_loss: 30.770483, loss: 23.464542
19:52:20.157312 Step[4300/6995], lr: 0.001000, mv_avg_loss: 28.513004, loss: 28.639050
19:52:24.953345 Step[4400/6995], lr: 0.001000, mv_avg_loss: 28.083813, loss: 55.646217
19:52:29.758826 Step[4500/6995], lr: 0.001000, mv_avg_loss: 25.995565, loss: 30.293692
19:52:34.558922 Step[4600/6995], lr: 0.001000, mv_avg_loss: 28.967566, loss: 28.659018
19:52:39.364822 Step[4700/6995], lr: 0.001000, mv_avg_loss: 32.058968, loss: 26.503098
19:52:44.175308 Step[4800/6995], lr: 0.001000, mv_avg_loss: 32.176056, loss: 30.841904
19:52:48.978775 Step[4900/6995], lr: 0.001000, mv_avg_loss: 28.764353, loss: 24.317717
19:52:53.784789 Step[5000/6995], lr: 0.001000, mv_avg_loss: 26.529005, loss: 58.113152
19:52:58.584767 Step[5100/6995], lr: 0.001000, mv_avg_loss: 24.560381, loss: 34.427696
19:53:03.426742 Step[5200/6995], lr: 0.001000, mv_avg_loss: 29.972937, loss: 41.179558
19:53:08.223127 Step[5300/6995], lr: 0.001000, mv_avg_loss: 29.672993, loss: 22.799393
19:53:13.030994 Step[5400/6995], lr: 0.001000, mv_avg_loss: 31.216467, loss: 40.585457
19:53:17.843247 Step[5500/6995], lr: 0.001000, mv_avg_loss: 29.254206, loss: 22.021276
19:53:22.658769 Step[5600/6995], lr: 0.001000, mv_avg_loss: 29.213205, loss: 22.279327
19:53:27.464486 Step[5700/6995], lr: 0.001000, mv_avg_loss: 29.283537, loss: 36.360435
19:53:32.274157 Step[5800/6995], lr: 0.001000, mv_avg_loss: 27.901674, loss: 46.788994
19:53:37.085415 Step[5900/6995], lr: 0.001000, mv_avg_loss: 29.017330, loss: 14.481129
19:53:41.909268 Step[6000/6995], lr: 0.001000, mv_avg_loss: 31.100094, loss: 23.739235
19:53:46.717760 Step[6100/6995], lr: 0.001000, mv_avg_loss: 29.828636, loss: 42.843765
19:53:51.531561 Step[6200/6995], lr: 0.001000, mv_avg_loss: 26.854876, loss: 18.123295
19:53:56.344961 Step[6300/6995], lr: 0.001000, mv_avg_loss: 27.692341, loss: 26.828217
19:54:01.162529 Step[6400/6995], lr: 0.001000, mv_avg_loss: 27.421223, loss: 30.447201
19:54:05.973501 Step[6500/6995], lr: 0.001000, mv_avg_loss: 27.495419, loss: 53.250729
19:54:10.781571 Step[6600/6995], lr: 0.001000, mv_avg_loss: 31.848879, loss: 33.198212
19:54:15.594509 Step[6700/6995], lr: 0.001000, mv_avg_loss: 31.823542, loss: 32.595230
19:54:20.402003 Step[6800/6995], lr: 0.001000, mv_avg_loss: 34.886036, loss: 34.295151
19:54:25.220845 Step[6900/6995], lr: 0.001000, mv_avg_loss: 27.028879, loss: 31.507092
Testing for epoch: 30
Average test PNSR is 25.314758 for 500 images
Start to train epoch 31
19:54:52.097357 Step[0/6995], lr: 0.001000, mv_avg_loss: 29.921101, loss: 45.209564
19:54:56.894854 Step[100/6995], lr: 0.001000, mv_avg_loss: 25.624872, loss: 24.048536
19:55:01.654574 Step[200/6995], lr: 0.001000, mv_avg_loss: 28.089872, loss: 26.101828
19:55:06.419180 Step[300/6995], lr: 0.001000, mv_avg_loss: 32.056107, loss: 20.975657
19:55:11.200729 Step[400/6995], lr: 0.001000, mv_avg_loss: 27.175068, loss: 19.550003
19:55:15.971846 Step[500/6995], lr: 0.001000, mv_avg_loss: 27.777456, loss: 25.823568
19:55:20.755985 Step[600/6995], lr: 0.001000, mv_avg_loss: 26.447916, loss: 14.087477
19:55:25.532602 Step[700/6995], lr: 0.001000, mv_avg_loss: 27.284794, loss: 18.387953
19:55:30.310488 Step[800/6995], lr: 0.001000, mv_avg_loss: 26.753769, loss: 32.109451
19:55:35.091217 Step[900/6995], lr: 0.001000, mv_avg_loss: 29.118690, loss: 35.765381
19:55:39.865533 Step[1000/6995], lr: 0.001000, mv_avg_loss: 27.494331, loss: 12.791922
19:55:44.642436 Step[1100/6995], lr: 0.001000, mv_avg_loss: 27.528383, loss: 33.417805
19:55:49.417095 Step[1200/6995], lr: 0.001000, mv_avg_loss: 26.566542, loss: 37.080612
19:55:54.193917 Step[1300/6995], lr: 0.001000, mv_avg_loss: 28.366337, loss: 28.617764
19:55:58.977474 Step[1400/6995], lr: 0.001000, mv_avg_loss: 31.278082, loss: 32.292923
19:56:03.752148 Step[1500/6995], lr: 0.001000, mv_avg_loss: 30.143419, loss: 69.094086
19:56:08.527104 Step[1600/6995], lr: 0.001000, mv_avg_loss: 29.056829, loss: 33.291512
19:56:13.318507 Step[1700/6995], lr: 0.001000, mv_avg_loss: 26.163042, loss: 17.145298
19:56:18.097054 Step[1800/6995], lr: 0.001000, mv_avg_loss: 27.477875, loss: 24.347443
19:56:22.871650 Step[1900/6995], lr: 0.001000, mv_avg_loss: 27.377434, loss: 19.413774
19:56:27.640507 Step[2000/6995], lr: 0.001000, mv_avg_loss: 28.958786, loss: 30.131882
19:56:32.417214 Step[2100/6995], lr: 0.001000, mv_avg_loss: 27.984179, loss: 21.939590
19:56:37.197989 Step[2200/6995], lr: 0.001000, mv_avg_loss: 26.227016, loss: 30.226360
19:56:41.970843 Step[2300/6995], lr: 0.001000, mv_avg_loss: 31.143147, loss: 45.813625
19:56:46.742717 Step[2400/6995], lr: 0.001000, mv_avg_loss: 27.245140, loss: 23.462362
19:56:51.519830 Step[2500/6995], lr: 0.001000, mv_avg_loss: 28.484457, loss: 18.509962
19:56:56.289135 Step[2600/6995], lr: 0.001000, mv_avg_loss: 29.059574, loss: 32.905174
19:57:01.057132 Step[2700/6995], lr: 0.001000, mv_avg_loss: 27.776035, loss: 26.198996
19:57:05.833073 Step[2800/6995], lr: 0.001000, mv_avg_loss: 27.443268, loss: 23.976727
19:57:10.610074 Step[2900/6995], lr: 0.001000, mv_avg_loss: 29.725824, loss: 22.309864
19:57:15.378750 Step[3000/6995], lr: 0.001000, mv_avg_loss: 28.072329, loss: 55.639145
19:57:20.146253 Step[3100/6995], lr: 0.001000, mv_avg_loss: 25.203396, loss: 20.437275
19:57:24.938622 Step[3200/6995], lr: 0.001000, mv_avg_loss: 30.031612, loss: 25.555182
19:57:29.702982 Step[3300/6995], lr: 0.001000, mv_avg_loss: 27.947695, loss: 38.740768
19:57:34.476755 Step[3400/6995], lr: 0.001000, mv_avg_loss: 26.620012, loss: 18.499666
19:57:39.249267 Step[3500/6995], lr: 0.001000, mv_avg_loss: 26.568546, loss: 19.965706
19:57:44.009542 Step[3600/6995], lr: 0.001000, mv_avg_loss: 30.132420, loss: 16.040253
19:57:48.762479 Step[3700/6995], lr: 0.001000, mv_avg_loss: 28.341450, loss: 26.085516
19:57:53.516596 Step[3800/6995], lr: 0.001000, mv_avg_loss: 28.648933, loss: 23.463339
19:57:58.264024 Step[3900/6995], lr: 0.001000, mv_avg_loss: 29.769859, loss: 24.259228
19:58:03.015387 Step[4000/6995], lr: 0.001000, mv_avg_loss: 28.984541, loss: 20.131149
19:58:07.780004 Step[4100/6995], lr: 0.001000, mv_avg_loss: 28.605412, loss: 21.527227
19:58:12.550115 Step[4200/6995], lr: 0.001000, mv_avg_loss: 30.273268, loss: 33.767174
19:58:17.311696 Step[4300/6995], lr: 0.001000, mv_avg_loss: 27.706163, loss: 16.700888
19:58:22.074385 Step[4400/6995], lr: 0.001000, mv_avg_loss: 31.627333, loss: 24.095066
19:58:26.828499 Step[4500/6995], lr: 0.001000, mv_avg_loss: 29.548071, loss: 26.966040
19:58:31.600498 Step[4600/6995], lr: 0.001000, mv_avg_loss: 27.698086, loss: 31.752752
19:58:36.374071 Step[4700/6995], lr: 0.001000, mv_avg_loss: 25.037392, loss: 26.126217
19:58:41.134959 Step[4800/6995], lr: 0.001000, mv_avg_loss: 25.581350, loss: 16.319622
19:58:45.898761 Step[4900/6995], lr: 0.001000, mv_avg_loss: 28.627426, loss: 18.457737
19:58:50.656578 Step[5000/6995], lr: 0.001000, mv_avg_loss: 31.084299, loss: 63.998669
19:58:55.422743 Step[5100/6995], lr: 0.001000, mv_avg_loss: 33.999634, loss: 31.930067
19:59:00.189537 Step[5200/6995], lr: 0.001000, mv_avg_loss: 28.883766, loss: 23.799429
19:59:04.957376 Step[5300/6995], lr: 0.001000, mv_avg_loss: 28.275669, loss: 19.218147
19:59:09.727671 Step[5400/6995], lr: 0.001000, mv_avg_loss: 28.654640, loss: 31.259296
19:59:14.519635 Step[5500/6995], lr: 0.001000, mv_avg_loss: 27.361271, loss: 12.988208
19:59:19.290101 Step[5600/6995], lr: 0.001000, mv_avg_loss: 27.024506, loss: 29.923277
19:59:24.050935 Step[5700/6995], lr: 0.001000, mv_avg_loss: 30.356415, loss: 33.042770
19:59:28.819757 Step[5800/6995], lr: 0.001000, mv_avg_loss: 24.832817, loss: 19.273323
19:59:33.587948 Step[5900/6995], lr: 0.001000, mv_avg_loss: 29.541578, loss: 25.359724
19:59:38.340504 Step[6000/6995], lr: 0.001000, mv_avg_loss: 31.787497, loss: 24.876257
19:59:43.091254 Step[6100/6995], lr: 0.001000, mv_avg_loss: 28.241325, loss: 25.068213
19:59:47.851164 Step[6200/6995], lr: 0.001000, mv_avg_loss: 30.667622, loss: 16.445646
19:59:52.603866 Step[6300/6995], lr: 0.001000, mv_avg_loss: 25.426165, loss: 35.978065
19:59:57.369348 Step[6400/6995], lr: 0.001000, mv_avg_loss: 27.328817, loss: 31.679104
20:00:02.125553 Step[6500/6995], lr: 0.001000, mv_avg_loss: 31.017504, loss: 35.131058
20:00:06.887341 Step[6600/6995], lr: 0.001000, mv_avg_loss: 27.664204, loss: 15.517020
20:00:11.657665 Step[6700/6995], lr: 0.001000, mv_avg_loss: 28.548037, loss: 23.926907
20:00:16.434021 Step[6800/6995], lr: 0.001000, mv_avg_loss: 28.261906, loss: 24.527296
20:00:21.196653 Step[6900/6995], lr: 0.001000, mv_avg_loss: 30.389687, loss: 18.826666
Testing for epoch: 31
Average test PNSR is 25.286817 for 500 images
Start to train epoch 32
20:00:47.858592 Step[0/6995], lr: 0.001000, mv_avg_loss: 25.653767, loss: 16.015289
20:00:52.594776 Step[100/6995], lr: 0.001000, mv_avg_loss: 24.467585, loss: 22.588583
20:00:57.337271 Step[200/6995], lr: 0.001000, mv_avg_loss: 25.343906, loss: 25.652121
20:01:02.057787 Step[300/6995], lr: 0.001000, mv_avg_loss: 26.438364, loss: 40.867302
20:01:06.788354 Step[400/6995], lr: 0.001000, mv_avg_loss: 26.612413, loss: 26.434185
20:01:11.523766 Step[500/6995], lr: 0.001000, mv_avg_loss: 27.843624, loss: 20.413975
20:01:16.260577 Step[600/6995], lr: 0.001000, mv_avg_loss: 26.710354, loss: 19.117592
20:01:21.007520 Step[700/6995], lr: 0.001000, mv_avg_loss: 26.985754, loss: 18.182959
20:01:25.733735 Step[800/6995], lr: 0.001000, mv_avg_loss: 33.093872, loss: 20.591427
20:01:30.467278 Step[900/6995], lr: 0.001000, mv_avg_loss: 31.481085, loss: 27.505407
20:01:35.198579 Step[1000/6995], lr: 0.001000, mv_avg_loss: 27.857300, loss: 25.127373
20:01:39.939731 Step[1100/6995], lr: 0.001000, mv_avg_loss: 29.014139, loss: 48.341522
20:01:44.695374 Step[1200/6995], lr: 0.001000, mv_avg_loss: 31.517622, loss: 19.649330
20:01:49.431712 Step[1300/6995], lr: 0.001000, mv_avg_loss: 26.404942, loss: 29.189138
20:01:54.171796 Step[1400/6995], lr: 0.001000, mv_avg_loss: 26.048096, loss: 26.461243
20:01:58.918037 Step[1500/6995], lr: 0.001000, mv_avg_loss: 24.203972, loss: 32.663788
20:02:03.659837 Step[1600/6995], lr: 0.001000, mv_avg_loss: 26.006544, loss: 17.566679
20:02:08.398395 Step[1700/6995], lr: 0.001000, mv_avg_loss: 25.103685, loss: 17.156317
20:02:13.148481 Step[1800/6995], lr: 0.001000, mv_avg_loss: 27.433689, loss: 30.217514
20:02:17.897611 Step[1900/6995], lr: 0.001000, mv_avg_loss: 25.378275, loss: 36.041420
20:02:22.647540 Step[2000/6995], lr: 0.001000, mv_avg_loss: 26.611746, loss: 23.785355
20:02:27.403841 Step[2100/6995], lr: 0.001000, mv_avg_loss: 26.111214, loss: 27.158550
20:02:32.158907 Step[2200/6995], lr: 0.001000, mv_avg_loss: 27.169407, loss: 30.589697
20:02:36.918455 Step[2300/6995], lr: 0.001000, mv_avg_loss: 29.488194, loss: 42.952492
20:02:41.685417 Step[2400/6995], lr: 0.001000, mv_avg_loss: 29.369682, loss: 17.052280
20:02:46.453812 Step[2500/6995], lr: 0.001000, mv_avg_loss: 27.875910, loss: 47.375542
20:02:51.217540 Step[2600/6995], lr: 0.001000, mv_avg_loss: 28.635935, loss: 20.287968
20:02:55.964033 Step[2700/6995], lr: 0.001000, mv_avg_loss: 26.055613, loss: 24.014771
20:03:00.720908 Step[2800/6995], lr: 0.001000, mv_avg_loss: 29.628632, loss: 30.824383
20:03:05.474145 Step[2900/6995], lr: 0.001000, mv_avg_loss: 26.774591, loss: 21.307873
20:03:10.238724 Step[3000/6995], lr: 0.001000, mv_avg_loss: 26.332977, loss: 50.320618
20:03:14.995898 Step[3100/6995], lr: 0.001000, mv_avg_loss: 27.055874, loss: 26.108009
20:03:19.746205 Step[3200/6995], lr: 0.001000, mv_avg_loss: 26.753183, loss: 23.594620
20:03:24.513178 Step[3300/6995], lr: 0.001000, mv_avg_loss: 25.745546, loss: 18.939371
20:03:29.274826 Step[3400/6995], lr: 0.001000, mv_avg_loss: 23.837471, loss: 23.965759
20:03:34.035925 Step[3500/6995], lr: 0.001000, mv_avg_loss: 25.184866, loss: 19.955391
20:03:38.793221 Step[3600/6995], lr: 0.001000, mv_avg_loss: 25.036716, loss: 18.790445
20:03:43.544033 Step[3700/6995], lr: 0.001000, mv_avg_loss: 28.268576, loss: 27.433559
20:03:48.303936 Step[3800/6995], lr: 0.001000, mv_avg_loss: 29.096752, loss: 28.879599
20:03:53.065499 Step[3900/6995], lr: 0.001000, mv_avg_loss: 28.205593, loss: 19.709124
20:03:57.831592 Step[4000/6995], lr: 0.001000, mv_avg_loss: 36.631790, loss: 37.459778
20:04:02.589303 Step[4100/6995], lr: 0.001000, mv_avg_loss: 29.071999, loss: 30.908993
20:04:07.350853 Step[4200/6995], lr: 0.001000, mv_avg_loss: 23.969198, loss: 16.137796
20:04:12.108888 Step[4300/6995], lr: 0.001000, mv_avg_loss: 29.111591, loss: 48.132767
20:04:16.863622 Step[4400/6995], lr: 0.001000, mv_avg_loss: 27.556196, loss: 19.550182
20:04:21.631318 Step[4500/6995], lr: 0.001000, mv_avg_loss: 29.014927, loss: 33.388218
20:04:26.389631 Step[4600/6995], lr: 0.001000, mv_avg_loss: 25.120287, loss: 19.173740
20:04:31.153881 Step[4700/6995], lr: 0.001000, mv_avg_loss: 26.127010, loss: 23.038038
20:04:35.914258 Step[4800/6995], lr: 0.001000, mv_avg_loss: 25.156275, loss: 32.822018
20:04:40.676714 Step[4900/6995], lr: 0.001000, mv_avg_loss: 30.147108, loss: 34.797310
20:04:45.445498 Step[5000/6995], lr: 0.001000, mv_avg_loss: 30.139442, loss: 35.410610
20:04:50.211580 Step[5100/6995], lr: 0.001000, mv_avg_loss: 27.260702, loss: 28.161224
20:04:54.979709 Step[5200/6995], lr: 0.001000, mv_avg_loss: 27.178652, loss: 23.480682
20:04:59.746790 Step[5300/6995], lr: 0.001000, mv_avg_loss: 33.702869, loss: 29.897573
20:05:04.513338 Step[5400/6995], lr: 0.001000, mv_avg_loss: 28.447992, loss: 19.264610
20:05:09.273209 Step[5500/6995], lr: 0.001000, mv_avg_loss: 27.813965, loss: 22.489576
20:05:14.036612 Step[5600/6995], lr: 0.001000, mv_avg_loss: 25.594172, loss: 22.372604
20:05:18.797446 Step[5700/6995], lr: 0.001000, mv_avg_loss: 29.312336, loss: 22.744095
20:05:23.556682 Step[5800/6995], lr: 0.001000, mv_avg_loss: 28.180634, loss: 34.859116
20:05:28.322182 Step[5900/6995], lr: 0.001000, mv_avg_loss: 27.410709, loss: 16.293161
20:05:33.091359 Step[6000/6995], lr: 0.001000, mv_avg_loss: 26.708630, loss: 30.256002
20:05:37.867206 Step[6100/6995], lr: 0.001000, mv_avg_loss: 25.200569, loss: 23.271885
20:05:42.662836 Step[6200/6995], lr: 0.001000, mv_avg_loss: 26.872898, loss: 39.511826
20:05:47.427828 Step[6300/6995], lr: 0.001000, mv_avg_loss: 24.209669, loss: 25.797665
20:05:52.198497 Step[6400/6995], lr: 0.001000, mv_avg_loss: 25.635733, loss: 19.055815
20:05:56.972299 Step[6500/6995], lr: 0.001000, mv_avg_loss: 30.888685, loss: 36.458942
20:06:01.747604 Step[6600/6995], lr: 0.001000, mv_avg_loss: 25.768742, loss: 37.319088
20:06:06.519463 Step[6700/6995], lr: 0.001000, mv_avg_loss: 25.649288, loss: 33.215332
20:06:11.287882 Step[6800/6995], lr: 0.001000, mv_avg_loss: 27.381657, loss: 30.348747
20:06:16.070468 Step[6900/6995], lr: 0.001000, mv_avg_loss: 28.526318, loss: 21.962215
Testing for epoch: 32
Average test PNSR is 25.322693 for 500 images
Start to train epoch 33
20:06:43.138046 Step[0/6995], lr: 0.001000, mv_avg_loss: 27.354692, loss: 20.760365
20:06:47.891977 Step[100/6995], lr: 0.001000, mv_avg_loss: 28.426550, loss: 29.964535
20:06:52.647847 Step[200/6995], lr: 0.001000, mv_avg_loss: 29.281700, loss: 26.818710
20:06:57.404840 Step[300/6995], lr: 0.001000, mv_avg_loss: 24.399055, loss: 34.583820
20:07:02.176139 Step[400/6995], lr: 0.001000, mv_avg_loss: 27.378935, loss: 26.723644
20:07:06.940759 Step[500/6995], lr: 0.001000, mv_avg_loss: 28.930878, loss: 24.510448
20:07:11.714340 Step[600/6995], lr: 0.001000, mv_avg_loss: 23.094482, loss: 34.733059
20:07:16.486571 Step[700/6995], lr: 0.001000, mv_avg_loss: 26.465528, loss: 22.009926
20:07:21.258870 Step[800/6995], lr: 0.001000, mv_avg_loss: 26.086000, loss: 36.082550
20:07:26.028979 Step[900/6995], lr: 0.001000, mv_avg_loss: 28.014326, loss: 28.243561
20:07:30.794952 Step[1000/6995], lr: 0.001000, mv_avg_loss: 26.370165, loss: 29.991482
20:07:35.567680 Step[1100/6995], lr: 0.001000, mv_avg_loss: 27.467855, loss: 25.151571
20:07:40.403735 Step[1200/6995], lr: 0.001000, mv_avg_loss: 25.076086, loss: 19.842939
20:07:45.178449 Step[1300/6995], lr: 0.001000, mv_avg_loss: 25.010620, loss: 16.186872
20:07:49.937229 Step[1400/6995], lr: 0.001000, mv_avg_loss: 27.093832, loss: 57.017021
20:07:54.705751 Step[1500/6995], lr: 0.001000, mv_avg_loss: 32.495716, loss: 28.209595
20:07:59.475158 Step[1600/6995], lr: 0.001000, mv_avg_loss: 30.056635, loss: 22.527639
20:08:04.246527 Step[1700/6995], lr: 0.001000, mv_avg_loss: 29.177937, loss: 35.782379
20:08:09.019932 Step[1800/6995], lr: 0.001000, mv_avg_loss: 28.198954, loss: 15.016284
20:08:13.800681 Step[1900/6995], lr: 0.001000, mv_avg_loss: 27.262541, loss: 29.427158
20:08:18.578575 Step[2000/6995], lr: 0.001000, mv_avg_loss: 25.920475, loss: 13.506319
20:08:23.352792 Step[2100/6995], lr: 0.001000, mv_avg_loss: 25.329889, loss: 15.344511
20:08:28.115671 Step[2200/6995], lr: 0.001000, mv_avg_loss: 26.042175, loss: 21.252960
20:08:32.882382 Step[2300/6995], lr: 0.001000, mv_avg_loss: 23.316874, loss: 26.354574
20:08:37.656501 Step[2400/6995], lr: 0.001000, mv_avg_loss: 27.433628, loss: 13.041946
20:08:42.424282 Step[2500/6995], lr: 0.001000, mv_avg_loss: 25.212309, loss: 36.330948
20:08:47.207527 Step[2600/6995], lr: 0.001000, mv_avg_loss: 24.978556, loss: 23.637383
20:08:51.988856 Step[2700/6995], lr: 0.001000, mv_avg_loss: 26.453705, loss: 25.904806
20:08:56.758076 Step[2800/6995], lr: 0.001000, mv_avg_loss: 24.513044, loss: 21.165924
20:09:01.535464 Step[2900/6995], lr: 0.001000, mv_avg_loss: 26.146641, loss: 40.461227
20:09:06.303813 Step[3000/6995], lr: 0.001000, mv_avg_loss: 25.221647, loss: 24.783539
20:09:11.089415 Step[3100/6995], lr: 0.001000, mv_avg_loss: 27.252737, loss: 24.639175
20:09:15.850636 Step[3200/6995], lr: 0.001000, mv_avg_loss: 30.646362, loss: 33.143272
20:09:20.620065 Step[3300/6995], lr: 0.001000, mv_avg_loss: 25.888029, loss: 27.235334
20:09:25.394707 Step[3400/6995], lr: 0.001000, mv_avg_loss: 24.138683, loss: 41.824593
20:09:30.168077 Step[3500/6995], lr: 0.001000, mv_avg_loss: 26.515770, loss: 27.620916
20:09:34.940733 Step[3600/6995], lr: 0.001000, mv_avg_loss: 25.982763, loss: 14.459487
20:09:39.717713 Step[3700/6995], lr: 0.001000, mv_avg_loss: 25.962851, loss: 19.363815
20:09:44.478703 Step[3800/6995], lr: 0.001000, mv_avg_loss: 26.237600, loss: 20.829556
20:09:49.251675 Step[3900/6995], lr: 0.001000, mv_avg_loss: 27.307428, loss: 14.605396
20:09:54.019573 Step[4000/6995], lr: 0.001000, mv_avg_loss: 30.479963, loss: 19.536266
20:09:58.785325 Step[4100/6995], lr: 0.001000, mv_avg_loss: 25.919800, loss: 21.823181
20:10:03.569149 Step[4200/6995], lr: 0.001000, mv_avg_loss: 26.042183, loss: 15.441870
20:10:08.336922 Step[4300/6995], lr: 0.001000, mv_avg_loss: 28.244436, loss: 40.912140
20:10:13.114016 Step[4400/6995], lr: 0.001000, mv_avg_loss: 28.290417, loss: 19.312050
20:10:17.883186 Step[4500/6995], lr: 0.001000, mv_avg_loss: 24.455587, loss: 22.001694
20:10:22.649896 Step[4600/6995], lr: 0.001000, mv_avg_loss: 27.110703, loss: 22.646688
20:10:27.422911 Step[4700/6995], lr: 0.001000, mv_avg_loss: 28.002201, loss: 42.712219
20:10:32.200816 Step[4800/6995], lr: 0.001000, mv_avg_loss: 23.343092, loss: 19.518627
20:10:36.965140 Step[4900/6995], lr: 0.001000, mv_avg_loss: 25.689777, loss: 38.419487
20:10:41.731810 Step[5000/6995], lr: 0.001000, mv_avg_loss: 27.864006, loss: 25.581062
20:10:46.493604 Step[5100/6995], lr: 0.001000, mv_avg_loss: 27.191965, loss: 23.997412
20:10:51.259842 Step[5200/6995], lr: 0.001000, mv_avg_loss: 25.050095, loss: 31.417091
20:10:56.029537 Step[5300/6995], lr: 0.001000, mv_avg_loss: 25.664267, loss: 58.371574
20:11:00.796934 Step[5400/6995], lr: 0.001000, mv_avg_loss: 29.193762, loss: 23.397551
20:11:05.583346 Step[5500/6995], lr: 0.001000, mv_avg_loss: 26.569004, loss: 28.536558
20:11:10.351449 Step[5600/6995], lr: 0.001000, mv_avg_loss: 35.619762, loss: 43.487526
20:11:15.117387 Step[5700/6995], lr: 0.001000, mv_avg_loss: 27.662600, loss: 22.999630
20:11:19.880588 Step[5800/6995], lr: 0.001000, mv_avg_loss: 25.938732, loss: 19.776150
20:11:24.648213 Step[5900/6995], lr: 0.001000, mv_avg_loss: 26.926655, loss: 13.509265
20:11:29.407035 Step[6000/6995], lr: 0.001000, mv_avg_loss: 25.437592, loss: 41.812809
20:11:34.170935 Step[6100/6995], lr: 0.001000, mv_avg_loss: 28.672525, loss: 25.101070
20:11:38.936809 Step[6200/6995], lr: 0.001000, mv_avg_loss: 32.913795, loss: 31.609100
20:11:43.699627 Step[6300/6995], lr: 0.001000, mv_avg_loss: 26.418798, loss: 23.890385
20:11:48.467791 Step[6400/6995], lr: 0.001000, mv_avg_loss: 29.464334, loss: 40.375473
20:11:53.240879 Step[6500/6995], lr: 0.001000, mv_avg_loss: 25.982615, loss: 36.479954
20:11:57.999152 Step[6600/6995], lr: 0.001000, mv_avg_loss: 26.094345, loss: 28.501417
20:12:02.755508 Step[6700/6995], lr: 0.001000, mv_avg_loss: 25.321663, loss: 26.050421
20:12:07.508287 Step[6800/6995], lr: 0.001000, mv_avg_loss: 26.516586, loss: 23.499914
20:12:12.272116 Step[6900/6995], lr: 0.001000, mv_avg_loss: 26.908859, loss: 30.109739
Testing for epoch: 33
Average test PNSR is 25.336623 for 500 images
Start to train epoch 34
20:12:39.070601 Step[0/6995], lr: 0.001000, mv_avg_loss: 27.281778, loss: 23.013046
20:12:43.791828 Step[100/6995], lr: 0.001000, mv_avg_loss: 26.891392, loss: 32.490078
20:12:48.537109 Step[200/6995], lr: 0.001000, mv_avg_loss: 24.633049, loss: 13.414549
20:12:53.285591 Step[300/6995], lr: 0.001000, mv_avg_loss: 25.398567, loss: 22.647985
20:12:58.030868 Step[400/6995], lr: 0.001000, mv_avg_loss: 26.198250, loss: 27.388876
20:13:02.782549 Step[500/6995], lr: 0.001000, mv_avg_loss: 26.728554, loss: 17.915054
20:13:07.515408 Step[600/6995], lr: 0.001000, mv_avg_loss: 26.778463, loss: 20.884386
20:13:12.263557 Step[700/6995], lr: 0.001000, mv_avg_loss: 29.566940, loss: 33.095104
20:13:16.996899 Step[800/6995], lr: 0.001000, mv_avg_loss: 25.880695, loss: 21.771557
20:13:21.745471 Step[900/6995], lr: 0.001000, mv_avg_loss: 23.569201, loss: 24.102921
20:13:26.485286 Step[1000/6995], lr: 0.001000, mv_avg_loss: 26.500891, loss: 22.783768
20:13:31.231203 Step[1100/6995], lr: 0.001000, mv_avg_loss: 25.951607, loss: 29.257452
20:13:35.977032 Step[1200/6995], lr: 0.001000, mv_avg_loss: 23.861286, loss: 19.274273
20:13:40.725436 Step[1300/6995], lr: 0.001000, mv_avg_loss: 28.723351, loss: 12.569794
20:13:45.468660 Step[1400/6995], lr: 0.001000, mv_avg_loss: 26.441528, loss: 21.333208
20:13:50.222598 Step[1500/6995], lr: 0.001000, mv_avg_loss: 25.119995, loss: 29.860485
20:13:54.964916 Step[1600/6995], lr: 0.001000, mv_avg_loss: 29.411102, loss: 26.928890
20:13:59.718878 Step[1700/6995], lr: 0.001000, mv_avg_loss: 26.757956, loss: 32.185280
20:14:04.481901 Step[1800/6995], lr: 0.001000, mv_avg_loss: 26.834490, loss: 28.492712
20:14:09.233292 Step[1900/6995], lr: 0.001000, mv_avg_loss: 24.091379, loss: 18.307535
20:14:13.987419 Step[2000/6995], lr: 0.001000, mv_avg_loss: 28.535753, loss: 24.980957
20:14:18.737009 Step[2100/6995], lr: 0.001000, mv_avg_loss: 25.197004, loss: 16.849539
20:14:23.500067 Step[2200/6995], lr: 0.001000, mv_avg_loss: 28.400154, loss: 21.054146
20:14:28.248644 Step[2300/6995], lr: 0.001000, mv_avg_loss: 25.917480, loss: 33.592819
20:14:33.000029 Step[2400/6995], lr: 0.001000, mv_avg_loss: 27.698215, loss: 20.618605
20:14:37.746432 Step[2500/6995], lr: 0.001000, mv_avg_loss: 26.288157, loss: 32.149876
20:14:42.490742 Step[2600/6995], lr: 0.001000, mv_avg_loss: 29.684038, loss: 37.020836
20:14:47.243179 Step[2700/6995], lr: 0.001000, mv_avg_loss: 27.804132, loss: 24.289427
20:14:51.986823 Step[2800/6995], lr: 0.001000, mv_avg_loss: 27.550827, loss: 15.128258
20:14:56.729275 Step[2900/6995], lr: 0.001000, mv_avg_loss: 26.556467, loss: 25.970282
20:15:01.492274 Step[3000/6995], lr: 0.001000, mv_avg_loss: 24.251701, loss: 24.650763
20:15:06.249050 Step[3100/6995], lr: 0.001000, mv_avg_loss: 25.242653, loss: 30.012737
20:15:11.001064 Step[3200/6995], lr: 0.001000, mv_avg_loss: 25.908060, loss: 28.222691
20:15:15.758024 Step[3300/6995], lr: 0.001000, mv_avg_loss: 24.221804, loss: 26.421028
20:15:20.505330 Step[3400/6995], lr: 0.001000, mv_avg_loss: 25.850643, loss: 29.617229
20:15:25.259446 Step[3500/6995], lr: 0.001000, mv_avg_loss: 24.326088, loss: 21.872040
20:15:30.011576 Step[3600/6995], lr: 0.001000, mv_avg_loss: 29.088547, loss: 32.327034
20:15:34.754856 Step[3700/6995], lr: 0.001000, mv_avg_loss: 24.511198, loss: 23.880718
20:15:39.500507 Step[3800/6995], lr: 0.001000, mv_avg_loss: 28.905626, loss: 20.975897
20:15:44.264669 Step[3900/6995], lr: 0.001000, mv_avg_loss: 24.981594, loss: 34.522026
20:15:49.016775 Step[4000/6995], lr: 0.001000, mv_avg_loss: 28.533617, loss: 26.339560
20:15:53.765797 Step[4100/6995], lr: 0.001000, mv_avg_loss: 28.822262, loss: 34.223721
20:15:58.513029 Step[4200/6995], lr: 0.001000, mv_avg_loss: 30.007612, loss: 34.993805
20:16:03.262243 Step[4300/6995], lr: 0.001000, mv_avg_loss: 28.081799, loss: 29.264074
20:16:08.007530 Step[4400/6995], lr: 0.001000, mv_avg_loss: 26.286179, loss: 17.646116
20:16:12.761694 Step[4500/6995], lr: 0.001000, mv_avg_loss: 27.326111, loss: 41.382126
20:16:17.511891 Step[4600/6995], lr: 0.001000, mv_avg_loss: 25.530430, loss: 15.534190
20:16:22.268368 Step[4700/6995], lr: 0.001000, mv_avg_loss: 28.347363, loss: 15.148491
20:16:27.024018 Step[4800/6995], lr: 0.001000, mv_avg_loss: 26.005981, loss: 20.699219
20:16:31.776481 Step[4900/6995], lr: 0.001000, mv_avg_loss: 24.282562, loss: 33.056694
20:16:36.532017 Step[5000/6995], lr: 0.001000, mv_avg_loss: 25.056614, loss: 38.535782
20:16:41.290657 Step[5100/6995], lr: 0.001000, mv_avg_loss: 25.754992, loss: 30.436277
20:16:46.050455 Step[5200/6995], lr: 0.001000, mv_avg_loss: 27.649071, loss: 24.103033
20:16:50.793759 Step[5300/6995], lr: 0.001000, mv_avg_loss: 22.489893, loss: 14.151884
20:16:55.539744 Step[5400/6995], lr: 0.001000, mv_avg_loss: 25.053989, loss: 17.774693
20:17:00.291614 Step[5500/6995], lr: 0.001000, mv_avg_loss: 25.359903, loss: 27.371374
20:17:05.035671 Step[5600/6995], lr: 0.001000, mv_avg_loss: 26.350029, loss: 13.516979
20:17:09.793656 Step[5700/6995], lr: 0.001000, mv_avg_loss: 28.129063, loss: 26.229511
20:17:14.538282 Step[5800/6995], lr: 0.001000, mv_avg_loss: 26.163670, loss: 22.414822
20:17:19.278212 Step[5900/6995], lr: 0.001000, mv_avg_loss: 25.609388, loss: 18.038763
20:17:24.030852 Step[6000/6995], lr: 0.001000, mv_avg_loss: 25.354843, loss: 20.716049
20:17:28.775010 Step[6100/6995], lr: 0.001000, mv_avg_loss: 28.070028, loss: 18.904325
20:17:33.530897 Step[6200/6995], lr: 0.001000, mv_avg_loss: 31.604380, loss: 24.402151
20:17:38.284210 Step[6300/6995], lr: 0.001000, mv_avg_loss: 28.668552, loss: 25.134899
20:17:43.042012 Step[6400/6995], lr: 0.001000, mv_avg_loss: 23.052298, loss: 41.636478
20:17:47.796392 Step[6500/6995], lr: 0.001000, mv_avg_loss: 24.998215, loss: 29.888876
20:17:52.553153 Step[6600/6995], lr: 0.001000, mv_avg_loss: 26.710396, loss: 24.828701
20:17:57.314656 Step[6700/6995], lr: 0.001000, mv_avg_loss: 23.735138, loss: 19.108032
20:18:02.066785 Step[6800/6995], lr: 0.001000, mv_avg_loss: 24.156254, loss: 29.313656
20:18:06.830243 Step[6900/6995], lr: 0.001000, mv_avg_loss: 25.280622, loss: 17.851980
saving model for epoch 34
Testing for epoch: 34
Average test PNSR is 25.435217 for 500 images
Start to train epoch 35
20:18:33.740572 Step[0/6995], lr: 0.001000, mv_avg_loss: 24.724325, loss: 34.188992
20:18:38.474107 Step[100/6995], lr: 0.001000, mv_avg_loss: 24.210083, loss: 45.111328
20:18:43.253702 Step[200/6995], lr: 0.001000, mv_avg_loss: 25.191740, loss: 14.935530
20:18:48.007087 Step[300/6995], lr: 0.001000, mv_avg_loss: 25.572807, loss: 28.765533
20:18:52.753677 Step[400/6995], lr: 0.001000, mv_avg_loss: 22.832932, loss: 21.745314
20:18:57.504875 Step[500/6995], lr: 0.001000, mv_avg_loss: 33.658501, loss: 49.428650
20:19:02.268684 Step[600/6995], lr: 0.001000, mv_avg_loss: 28.138723, loss: 24.050831
20:19:07.018447 Step[700/6995], lr: 0.001000, mv_avg_loss: 33.582684, loss: 23.802570
20:19:11.765703 Step[800/6995], lr: 0.001000, mv_avg_loss: 27.632980, loss: 12.171103
20:19:16.527264 Step[900/6995], lr: 0.001000, mv_avg_loss: 25.960646, loss: 25.928913
20:19:21.284879 Step[1000/6995], lr: 0.001000, mv_avg_loss: 26.731773, loss: 25.771797
20:19:26.042050 Step[1100/6995], lr: 0.001000, mv_avg_loss: 24.467442, loss: 19.023582
20:19:30.797982 Step[1200/6995], lr: 0.001000, mv_avg_loss: 26.142208, loss: 32.326187
20:19:35.551187 Step[1300/6995], lr: 0.001000, mv_avg_loss: 27.520782, loss: 18.609455
20:19:40.303833 Step[1400/6995], lr: 0.001000, mv_avg_loss: 25.195732, loss: 23.071934
20:19:45.059479 Step[1500/6995], lr: 0.001000, mv_avg_loss: 26.138906, loss: 15.518397
20:19:49.820192 Step[1600/6995], lr: 0.001000, mv_avg_loss: 25.731354, loss: 32.200768
20:19:54.577857 Step[1700/6995], lr: 0.001000, mv_avg_loss: 25.539930, loss: 31.766088
20:19:59.331789 Step[1800/6995], lr: 0.001000, mv_avg_loss: 28.167336, loss: 26.066481
20:20:04.091588 Step[1900/6995], lr: 0.001000, mv_avg_loss: 25.795151, loss: 48.250786
20:20:08.859981 Step[2000/6995], lr: 0.001000, mv_avg_loss: 25.391830, loss: 30.709690
20:20:13.621423 Step[2100/6995], lr: 0.001000, mv_avg_loss: 24.816036, loss: 27.535622
20:20:18.387353 Step[2200/6995], lr: 0.001000, mv_avg_loss: 26.490454, loss: 44.036163
20:20:23.147923 Step[2300/6995], lr: 0.001000, mv_avg_loss: 24.409765, loss: 27.941280
20:20:27.907952 Step[2400/6995], lr: 0.001000, mv_avg_loss: 25.244221, loss: 21.228916
20:20:32.689725 Step[2500/6995], lr: 0.001000, mv_avg_loss: 25.007219, loss: 18.750874
20:20:37.457626 Step[2600/6995], lr: 0.001000, mv_avg_loss: 25.025539, loss: 20.602119
20:20:42.217758 Step[2700/6995], lr: 0.001000, mv_avg_loss: 25.449860, loss: 19.247135
20:20:46.998580 Step[2800/6995], lr: 0.001000, mv_avg_loss: 27.283833, loss: 34.922585
20:20:51.770023 Step[2900/6995], lr: 0.001000, mv_avg_loss: 26.369530, loss: 17.162201
20:20:56.528112 Step[3000/6995], lr: 0.001000, mv_avg_loss: 24.078020, loss: 52.711159
20:21:01.303157 Step[3100/6995], lr: 0.001000, mv_avg_loss: 25.915785, loss: 24.772812
20:21:06.068706 Step[3200/6995], lr: 0.001000, mv_avg_loss: 25.895247, loss: 20.857861
20:21:10.835856 Step[3300/6995], lr: 0.001000, mv_avg_loss: 30.063097, loss: 66.394745
20:21:15.594638 Step[3400/6995], lr: 0.001000, mv_avg_loss: 25.581451, loss: 27.221970
20:21:20.368742 Step[3500/6995], lr: 0.001000, mv_avg_loss: 25.302948, loss: 24.989559
20:21:25.131716 Step[3600/6995], lr: 0.001000, mv_avg_loss: 24.990099, loss: 31.986038
20:21:29.906321 Step[3700/6995], lr: 0.001000, mv_avg_loss: 25.118805, loss: 23.669144
20:21:34.682765 Step[3800/6995], lr: 0.001000, mv_avg_loss: 28.275015, loss: 25.172859
20:21:39.451632 Step[3900/6995], lr: 0.001000, mv_avg_loss: 31.534733, loss: 20.367971
20:21:44.207591 Step[4000/6995], lr: 0.001000, mv_avg_loss: 30.950417, loss: 29.443302
20:21:48.972540 Step[4100/6995], lr: 0.001000, mv_avg_loss: 27.816135, loss: 21.974560
20:21:53.764715 Step[4200/6995], lr: 0.001000, mv_avg_loss: 27.457811, loss: 18.621626
20:21:58.539093 Step[4300/6995], lr: 0.001000, mv_avg_loss: 28.599020, loss: 21.157169
20:22:03.316990 Step[4400/6995], lr: 0.001000, mv_avg_loss: 30.181303, loss: 54.813278
20:22:08.096818 Step[4500/6995], lr: 0.001000, mv_avg_loss: 25.986811, loss: 15.873398
20:22:12.869399 Step[4600/6995], lr: 0.001000, mv_avg_loss: 26.015951, loss: 16.418522
20:22:17.650304 Step[4700/6995], lr: 0.001000, mv_avg_loss: 26.942602, loss: 27.716124
20:22:22.427271 Step[4800/6995], lr: 0.001000, mv_avg_loss: 26.297810, loss: 25.119923
20:22:27.213115 Step[4900/6995], lr: 0.001000, mv_avg_loss: 24.174301, loss: 27.036718
20:22:31.984672 Step[5000/6995], lr: 0.001000, mv_avg_loss: 24.487116, loss: 19.996304
20:22:36.766363 Step[5100/6995], lr: 0.001000, mv_avg_loss: 24.011204, loss: 18.470827
20:22:41.559547 Step[5200/6995], lr: 0.001000, mv_avg_loss: 28.700867, loss: 19.247566
20:22:46.339289 Step[5300/6995], lr: 0.001000, mv_avg_loss: 25.942421, loss: 34.899345
20:22:51.127956 Step[5400/6995], lr: 0.001000, mv_avg_loss: 25.743290, loss: 16.395348
20:22:55.903339 Step[5500/6995], lr: 0.001000, mv_avg_loss: 24.337875, loss: 14.508627
20:23:00.682084 Step[5600/6995], lr: 0.001000, mv_avg_loss: 26.359535, loss: 38.465996
20:23:05.457970 Step[5700/6995], lr: 0.001000, mv_avg_loss: 25.349419, loss: 15.432865
20:23:10.235676 Step[5800/6995], lr: 0.001000, mv_avg_loss: 28.518061, loss: 26.161932
20:23:15.019508 Step[5900/6995], lr: 0.001000, mv_avg_loss: 24.747656, loss: 16.657228
20:23:19.799869 Step[6000/6995], lr: 0.001000, mv_avg_loss: 23.203182, loss: 21.496002
20:23:24.577971 Step[6100/6995], lr: 0.001000, mv_avg_loss: 26.110281, loss: 33.687157
20:23:29.350113 Step[6200/6995], lr: 0.001000, mv_avg_loss: 25.877146, loss: 32.451870
20:23:34.119960 Step[6300/6995], lr: 0.001000, mv_avg_loss: 23.198942, loss: 14.288582
20:23:38.902031 Step[6400/6995], lr: 0.001000, mv_avg_loss: 25.955256, loss: 16.786383
20:23:43.680298 Step[6500/6995], lr: 0.001000, mv_avg_loss: 25.030441, loss: 25.085766
20:23:48.454201 Step[6600/6995], lr: 0.001000, mv_avg_loss: 25.672506, loss: 15.074720
20:23:53.232097 Step[6700/6995], lr: 0.001000, mv_avg_loss: 29.819675, loss: 55.223495
20:23:58.009949 Step[6800/6995], lr: 0.001000, mv_avg_loss: 26.004219, loss: 29.290281
20:24:02.794396 Step[6900/6995], lr: 0.001000, mv_avg_loss: 26.856707, loss: 23.770826
Testing for epoch: 35
Average test PNSR is 25.037639 for 500 images
Start to train epoch 36
20:24:30.179898 Step[0/6995], lr: 0.001000, mv_avg_loss: 26.398922, loss: 43.345562
20:24:34.946594 Step[100/6995], lr: 0.001000, mv_avg_loss: 26.389608, loss: 20.516567
20:24:39.706042 Step[200/6995], lr: 0.001000, mv_avg_loss: 27.066584, loss: 45.962387
20:24:44.458229 Step[300/6995], lr: 0.001000, mv_avg_loss: 27.649649, loss: 25.817070
20:24:49.211313 Step[400/6995], lr: 0.001000, mv_avg_loss: 26.617184, loss: 27.893295
20:24:53.975756 Step[500/6995], lr: 0.001000, mv_avg_loss: 27.273634, loss: 39.042782
20:24:58.737243 Step[600/6995], lr: 0.001000, mv_avg_loss: 27.924589, loss: 28.427647
20:25:03.503790 Step[700/6995], lr: 0.001000, mv_avg_loss: 31.789204, loss: 17.593767
20:25:08.262251 Step[800/6995], lr: 0.001000, mv_avg_loss: 23.929192, loss: 16.788916
20:25:13.017678 Step[900/6995], lr: 0.001000, mv_avg_loss: 26.315956, loss: 22.234505
20:25:17.776180 Step[1000/6995], lr: 0.001000, mv_avg_loss: 25.681417, loss: 33.540756
20:25:22.547921 Step[1100/6995], lr: 0.001000, mv_avg_loss: 25.403683, loss: 22.937284
20:25:27.320035 Step[1200/6995], lr: 0.001000, mv_avg_loss: 23.421906, loss: 20.761608
20:25:32.087642 Step[1300/6995], lr: 0.001000, mv_avg_loss: 25.709913, loss: 21.762703
20:25:36.842939 Step[1400/6995], lr: 0.001000, mv_avg_loss: 22.165396, loss: 31.080784
20:25:41.603035 Step[1500/6995], lr: 0.001000, mv_avg_loss: 24.688049, loss: 28.803986
20:25:46.373541 Step[1600/6995], lr: 0.001000, mv_avg_loss: 24.190186, loss: 22.177557
20:25:51.167869 Step[1700/6995], lr: 0.001000, mv_avg_loss: 27.045370, loss: 17.038895
20:25:55.944106 Step[1800/6995], lr: 0.001000, mv_avg_loss: 26.069498, loss: 28.538000
20:26:00.747525 Step[1900/6995], lr: 0.001000, mv_avg_loss: 25.917763, loss: 16.573055
20:26:05.535588 Step[2000/6995], lr: 0.001000, mv_avg_loss: 25.479815, loss: 19.162767
20:26:10.316578 Step[2100/6995], lr: 0.001000, mv_avg_loss: 23.366245, loss: 24.377941
20:26:15.102294 Step[2200/6995], lr: 0.001000, mv_avg_loss: 27.200474, loss: 17.679823
20:26:19.890272 Step[2300/6995], lr: 0.001000, mv_avg_loss: 24.811457, loss: 27.183722
20:26:24.680580 Step[2400/6995], lr: 0.001000, mv_avg_loss: 24.891457, loss: 28.347891
20:26:29.459473 Step[2500/6995], lr: 0.001000, mv_avg_loss: 27.626848, loss: 26.472733
20:26:34.236763 Step[2600/6995], lr: 0.001000, mv_avg_loss: 23.784662, loss: 22.150009
20:26:39.017122 Step[2700/6995], lr: 0.001000, mv_avg_loss: 25.172312, loss: 30.303677
20:26:43.806017 Step[2800/6995], lr: 0.001000, mv_avg_loss: 23.004097, loss: 18.696302
20:26:48.595929 Step[2900/6995], lr: 0.001000, mv_avg_loss: 24.328703, loss: 26.561987
20:26:53.383550 Step[3000/6995], lr: 0.001000, mv_avg_loss: 24.802561, loss: 15.871462
20:26:58.163991 Step[3100/6995], lr: 0.001000, mv_avg_loss: 25.857578, loss: 21.312471
20:27:02.967006 Step[3200/6995], lr: 0.001000, mv_avg_loss: 26.264256, loss: 16.865911
20:27:07.756219 Step[3300/6995], lr: 0.001000, mv_avg_loss: 23.879892, loss: 13.087124
20:27:12.529570 Step[3400/6995], lr: 0.001000, mv_avg_loss: 23.351353, loss: 29.457092
20:27:17.321737 Step[3500/6995], lr: 0.001000, mv_avg_loss: 22.932098, loss: 18.474518
20:27:22.104021 Step[3600/6995], lr: 0.001000, mv_avg_loss: 22.962284, loss: 22.929815
20:27:26.891455 Step[3700/6995], lr: 0.001000, mv_avg_loss: 23.495222, loss: 32.194321
20:27:31.667564 Step[3800/6995], lr: 0.001000, mv_avg_loss: 27.692356, loss: 61.814865
20:27:36.452539 Step[3900/6995], lr: 0.001000, mv_avg_loss: 26.662245, loss: 39.975990
20:27:41.227522 Step[4000/6995], lr: 0.001000, mv_avg_loss: 24.091593, loss: 16.955254
20:27:46.016920 Step[4100/6995], lr: 0.001000, mv_avg_loss: 27.536360, loss: 18.798868
20:27:50.797558 Step[4200/6995], lr: 0.001000, mv_avg_loss: 25.855957, loss: 43.315239
20:27:55.568722 Step[4300/6995], lr: 0.001000, mv_avg_loss: 27.783682, loss: 28.675400
20:28:00.356846 Step[4400/6995], lr: 0.001000, mv_avg_loss: 26.660812, loss: 20.926826
20:28:05.155763 Step[4500/6995], lr: 0.001000, mv_avg_loss: 27.480740, loss: 36.609779
20:28:09.934015 Step[4600/6995], lr: 0.001000, mv_avg_loss: 24.638796, loss: 18.057886
20:28:14.703886 Step[4700/6995], lr: 0.001000, mv_avg_loss: 24.478041, loss: 22.532804
20:28:19.485485 Step[4800/6995], lr: 0.001000, mv_avg_loss: 25.969721, loss: 21.044527
20:28:24.269742 Step[4900/6995], lr: 0.001000, mv_avg_loss: 43.000313, loss: 25.256628
20:28:29.057249 Step[5000/6995], lr: 0.001000, mv_avg_loss: 25.471022, loss: 18.963682
20:28:33.851322 Step[5100/6995], lr: 0.001000, mv_avg_loss: 25.445612, loss: 12.999205
20:28:38.632799 Step[5200/6995], lr: 0.001000, mv_avg_loss: 24.427454, loss: 21.175350
20:28:43.404631 Step[5300/6995], lr: 0.001000, mv_avg_loss: 23.262642, loss: 13.258929
20:28:48.181189 Step[5400/6995], lr: 0.001000, mv_avg_loss: 23.392975, loss: 20.568579
20:28:52.958767 Step[5500/6995], lr: 0.001000, mv_avg_loss: 24.895439, loss: 30.024534
20:28:57.740699 Step[5600/6995], lr: 0.001000, mv_avg_loss: 24.953325, loss: 19.176231
20:29:02.516355 Step[5700/6995], lr: 0.001000, mv_avg_loss: 25.994955, loss: 16.540953
20:29:07.293348 Step[5800/6995], lr: 0.001000, mv_avg_loss: 23.822853, loss: 34.067032
20:29:12.065517 Step[5900/6995], lr: 0.001000, mv_avg_loss: 26.171638, loss: 16.506609
20:29:16.845202 Step[6000/6995], lr: 0.001000, mv_avg_loss: 27.049660, loss: 17.237530
20:29:21.620047 Step[6100/6995], lr: 0.001000, mv_avg_loss: 24.390802, loss: 18.654259
20:29:26.395632 Step[6200/6995], lr: 0.001000, mv_avg_loss: 23.122372, loss: 18.991398
20:29:31.176455 Step[6300/6995], lr: 0.001000, mv_avg_loss: 24.466328, loss: 26.495628
20:29:35.961386 Step[6400/6995], lr: 0.001000, mv_avg_loss: 27.013178, loss: 30.024633
20:29:40.738252 Step[6500/6995], lr: 0.001000, mv_avg_loss: 23.830793, loss: 45.938438
20:29:45.513628 Step[6600/6995], lr: 0.001000, mv_avg_loss: 23.416714, loss: 29.832073
20:29:50.289733 Step[6700/6995], lr: 0.001000, mv_avg_loss: 26.562065, loss: 19.627728
20:29:55.086461 Step[6800/6995], lr: 0.001000, mv_avg_loss: 26.882483, loss: 27.431644
20:29:59.865061 Step[6900/6995], lr: 0.001000, mv_avg_loss: 28.450165, loss: 18.995586
Testing for epoch: 36
Average test PNSR is 25.291290 for 500 images
Start to train epoch 37
20:30:26.585803 Step[0/6995], lr: 0.001000, mv_avg_loss: 26.811287, loss: 27.677017
20:30:31.349876 Step[100/6995], lr: 0.001000, mv_avg_loss: 24.346359, loss: 30.148916
20:30:36.103385 Step[200/6995], lr: 0.001000, mv_avg_loss: 30.030357, loss: 34.998615
20:30:40.859828 Step[300/6995], lr: 0.001000, mv_avg_loss: 23.875875, loss: 14.094179
20:30:45.611647 Step[400/6995], lr: 0.001000, mv_avg_loss: 25.510563, loss: 31.118002
20:30:50.389555 Step[500/6995], lr: 0.001000, mv_avg_loss: 22.698593, loss: 17.110840
20:30:55.152886 Step[600/6995], lr: 0.001000, mv_avg_loss: 25.534618, loss: 38.807674
20:30:59.931068 Step[700/6995], lr: 0.001000, mv_avg_loss: 26.401909, loss: 15.674019
20:31:04.686003 Step[800/6995], lr: 0.001000, mv_avg_loss: 23.001047, loss: 19.482330
20:31:09.455937 Step[900/6995], lr: 0.001000, mv_avg_loss: 25.326607, loss: 16.026485
20:31:14.238238 Step[1000/6995], lr: 0.001000, mv_avg_loss: 24.711239, loss: 18.786999
20:31:19.017743 Step[1100/6995], lr: 0.001000, mv_avg_loss: 24.907309, loss: 15.651470
20:31:23.836742 Step[1200/6995], lr: 0.001000, mv_avg_loss: 23.826887, loss: 18.079887
20:31:28.604141 Step[1300/6995], lr: 0.001000, mv_avg_loss: 24.848677, loss: 16.521240
20:31:33.381731 Step[1400/6995], lr: 0.001000, mv_avg_loss: 24.658974, loss: 29.842422
20:31:38.158752 Step[1500/6995], lr: 0.001000, mv_avg_loss: 22.612490, loss: 19.415031
20:31:42.931116 Step[1600/6995], lr: 0.001000, mv_avg_loss: 25.184238, loss: 22.306774
20:31:47.715091 Step[1700/6995], lr: 0.001000, mv_avg_loss: 22.654749, loss: 12.522464
20:31:52.501354 Step[1800/6995], lr: 0.001000, mv_avg_loss: 26.547426, loss: 22.418173
20:31:57.307709 Step[1900/6995], lr: 0.001000, mv_avg_loss: 27.733732, loss: 31.979073
20:32:02.090542 Step[2000/6995], lr: 0.001000, mv_avg_loss: 25.319038, loss: 14.797195
20:32:06.872065 Step[2100/6995], lr: 0.001000, mv_avg_loss: 23.687723, loss: 24.444241
20:32:11.661679 Step[2200/6995], lr: 0.001000, mv_avg_loss: 24.709620, loss: 35.014336
20:32:16.456986 Step[2300/6995], lr: 0.001000, mv_avg_loss: 23.803267, loss: 38.647911
20:32:21.241600 Step[2400/6995], lr: 0.001000, mv_avg_loss: 26.030098, loss: 35.778557
20:32:26.019591 Step[2500/6995], lr: 0.001000, mv_avg_loss: 23.988844, loss: 19.120472
20:32:30.802416 Step[2600/6995], lr: 0.001000, mv_avg_loss: 25.514553, loss: 22.852180
20:32:35.592090 Step[2700/6995], lr: 0.001000, mv_avg_loss: 29.338215, loss: 18.725937
20:32:40.372292 Step[2800/6995], lr: 0.001000, mv_avg_loss: 26.572771, loss: 20.366776
20:32:45.156231 Step[2900/6995], lr: 0.001000, mv_avg_loss: 26.786510, loss: 43.259079
20:32:49.938283 Step[3000/6995], lr: 0.001000, mv_avg_loss: 23.333395, loss: 18.065687
20:32:54.727258 Step[3100/6995], lr: 0.001000, mv_avg_loss: 25.536425, loss: 31.450191
20:32:59.513162 Step[3200/6995], lr: 0.001000, mv_avg_loss: 24.123371, loss: 29.495733
20:33:04.293538 Step[3300/6995], lr: 0.001000, mv_avg_loss: 26.231300, loss: 26.508284
20:33:09.083761 Step[3400/6995], lr: 0.001000, mv_avg_loss: 25.499992, loss: 18.307671
20:33:13.871286 Step[3500/6995], lr: 0.001000, mv_avg_loss: 26.986694, loss: 13.496074
20:33:18.678223 Step[3600/6995], lr: 0.001000, mv_avg_loss: 24.289042, loss: 26.370703
20:33:23.468368 Step[3700/6995], lr: 0.001000, mv_avg_loss: 22.864502, loss: 19.321056
20:33:28.252675 Step[3800/6995], lr: 0.001000, mv_avg_loss: 22.099501, loss: 22.257772
20:33:33.043062 Step[3900/6995], lr: 0.001000, mv_avg_loss: 24.974077, loss: 24.310284
20:33:37.834925 Step[4000/6995], lr: 0.001000, mv_avg_loss: 25.975155, loss: 24.554922
20:33:42.622708 Step[4100/6995], lr: 0.001000, mv_avg_loss: 23.376125, loss: 20.089165
20:33:47.414541 Step[4200/6995], lr: 0.001000, mv_avg_loss: 21.624258, loss: 22.366364
20:33:52.213685 Step[4300/6995], lr: 0.001000, mv_avg_loss: 24.826647, loss: 18.425295
20:33:57.006428 Step[4400/6995], lr: 0.001000, mv_avg_loss: 25.170271, loss: 33.732338
20:34:01.806957 Step[4500/6995], lr: 0.001000, mv_avg_loss: 23.986782, loss: 22.627907
20:34:06.600948 Step[4600/6995], lr: 0.001000, mv_avg_loss: 25.795006, loss: 35.011547
20:34:11.388962 Step[4700/6995], lr: 0.001000, mv_avg_loss: 27.869102, loss: 63.468651
20:34:16.191285 Step[4800/6995], lr: 0.001000, mv_avg_loss: 32.629665, loss: 43.236797
20:34:20.979108 Step[4900/6995], lr: 0.001000, mv_avg_loss: 31.731798, loss: 23.029644
20:34:25.779378 Step[5000/6995], lr: 0.001000, mv_avg_loss: 25.866030, loss: 19.530809
20:34:30.581551 Step[5100/6995], lr: 0.001000, mv_avg_loss: 25.860586, loss: 21.596022
20:34:35.377462 Step[5200/6995], lr: 0.001000, mv_avg_loss: 25.726688, loss: 54.860317
20:34:40.173355 Step[5300/6995], lr: 0.001000, mv_avg_loss: 23.453094, loss: 20.299564
20:34:44.963085 Step[5400/6995], lr: 0.001000, mv_avg_loss: 22.937939, loss: 24.822800
20:34:49.767681 Step[5500/6995], lr: 0.001000, mv_avg_loss: 22.653061, loss: 27.536749
20:34:54.561331 Step[5600/6995], lr: 0.001000, mv_avg_loss: 22.998034, loss: 18.301746
20:34:59.361398 Step[5700/6995], lr: 0.001000, mv_avg_loss: 24.510868, loss: 13.088501
20:35:04.144287 Step[5800/6995], lr: 0.001000, mv_avg_loss: 22.152353, loss: 25.981459
20:35:08.943877 Step[5900/6995], lr: 0.001000, mv_avg_loss: 23.482050, loss: 19.616356
20:35:13.750468 Step[6000/6995], lr: 0.001000, mv_avg_loss: 23.204977, loss: 19.909645
20:35:18.544856 Step[6100/6995], lr: 0.001000, mv_avg_loss: 24.203793, loss: 17.128372
20:35:23.371180 Step[6200/6995], lr: 0.001000, mv_avg_loss: 22.491779, loss: 16.356932
20:35:28.177712 Step[6300/6995], lr: 0.001000, mv_avg_loss: 23.203180, loss: 39.644653
20:35:32.972729 Step[6400/6995], lr: 0.001000, mv_avg_loss: 25.617975, loss: 25.479771
20:35:37.775241 Step[6500/6995], lr: 0.001000, mv_avg_loss: 22.516670, loss: 19.876963
20:35:42.571789 Step[6600/6995], lr: 0.001000, mv_avg_loss: 26.482523, loss: 28.452999
20:35:47.358341 Step[6700/6995], lr: 0.001000, mv_avg_loss: 24.424465, loss: 27.794704
20:35:52.164374 Step[6800/6995], lr: 0.001000, mv_avg_loss: 23.623440, loss: 19.408611
20:35:56.958038 Step[6900/6995], lr: 0.001000, mv_avg_loss: 23.686953, loss: 15.982952
Testing for epoch: 37
Average test PNSR is 25.347487 for 500 images
Start to train epoch 38
20:36:23.940932 Step[0/6995], lr: 0.001000, mv_avg_loss: 24.124722, loss: 30.104185
20:36:28.736412 Step[100/6995], lr: 0.001000, mv_avg_loss: 24.755257, loss: 27.384689
20:36:33.528365 Step[200/6995], lr: 0.001000, mv_avg_loss: 23.619497, loss: 29.841366
20:36:38.317581 Step[300/6995], lr: 0.001000, mv_avg_loss: 27.058596, loss: 17.703529
20:36:43.112803 Step[400/6995], lr: 0.001000, mv_avg_loss: 22.779654, loss: 15.730411
20:36:47.894294 Step[500/6995], lr: 0.001000, mv_avg_loss: 26.321543, loss: 23.222713
20:36:52.689190 Step[600/6995], lr: 0.001000, mv_avg_loss: 27.013357, loss: 20.082020
20:36:57.489210 Step[700/6995], lr: 0.001000, mv_avg_loss: 21.243547, loss: 35.638931
20:37:02.286614 Step[800/6995], lr: 0.001000, mv_avg_loss: 23.496689, loss: 57.488094
20:37:07.084225 Step[900/6995], lr: 0.001000, mv_avg_loss: 27.143162, loss: 27.952972
20:37:11.881655 Step[1000/6995], lr: 0.001000, mv_avg_loss: 23.354303, loss: 17.177351
20:37:16.674481 Step[1100/6995], lr: 0.001000, mv_avg_loss: 23.748072, loss: 17.277594
20:37:21.485267 Step[1200/6995], lr: 0.001000, mv_avg_loss: 25.329227, loss: 26.703293
20:37:26.272683 Step[1300/6995], lr: 0.001000, mv_avg_loss: 26.092121, loss: 29.384451
20:37:31.076665 Step[1400/6995], lr: 0.001000, mv_avg_loss: 23.297234, loss: 13.417019
20:37:35.883414 Step[1500/6995], lr: 0.001000, mv_avg_loss: 23.593306, loss: 55.114944
20:37:40.684206 Step[1600/6995], lr: 0.001000, mv_avg_loss: 22.950144, loss: 28.452930
20:37:45.490070 Step[1700/6995], lr: 0.001000, mv_avg_loss: 25.309357, loss: 27.420218
20:37:50.288748 Step[1800/6995], lr: 0.001000, mv_avg_loss: 27.491022, loss: 18.571180
20:37:55.100233 Step[1900/6995], lr: 0.001000, mv_avg_loss: 26.055162, loss: 16.117718
20:37:59.888508 Step[2000/6995], lr: 0.001000, mv_avg_loss: 26.329617, loss: 17.601952
20:38:04.679344 Step[2100/6995], lr: 0.001000, mv_avg_loss: 22.560795, loss: 36.307549
20:38:09.484036 Step[2200/6995], lr: 0.001000, mv_avg_loss: 24.348869, loss: 47.348579
20:38:14.283967 Step[2300/6995], lr: 0.001000, mv_avg_loss: 24.707836, loss: 37.406479
20:38:19.090042 Step[2400/6995], lr: 0.001000, mv_avg_loss: 24.063007, loss: 28.407318
20:38:23.893787 Step[2500/6995], lr: 0.001000, mv_avg_loss: 26.440437, loss: 46.186523
20:38:28.688739 Step[2600/6995], lr: 0.001000, mv_avg_loss: 28.835249, loss: 36.757534
20:38:33.499616 Step[2700/6995], lr: 0.001000, mv_avg_loss: 23.924328, loss: 22.639145
20:38:38.308454 Step[2800/6995], lr: 0.001000, mv_avg_loss: 21.874846, loss: 27.477329
20:38:43.097632 Step[2900/6995], lr: 0.001000, mv_avg_loss: 27.774763, loss: 26.671234
20:38:47.900047 Step[3000/6995], lr: 0.001000, mv_avg_loss: 33.456886, loss: 38.813438
20:38:52.692822 Step[3100/6995], lr: 0.001000, mv_avg_loss: 24.958374, loss: 39.220028
20:38:57.485621 Step[3200/6995], lr: 0.001000, mv_avg_loss: 24.497768, loss: 24.365231
20:39:02.284360 Step[3300/6995], lr: 0.001000, mv_avg_loss: 22.696955, loss: 12.041142
20:39:07.071923 Step[3400/6995], lr: 0.001000, mv_avg_loss: 24.647820, loss: 18.552094
20:39:11.862688 Step[3500/6995], lr: 0.001000, mv_avg_loss: 23.541491, loss: 18.951622
20:39:16.656890 Step[3600/6995], lr: 0.001000, mv_avg_loss: 23.721029, loss: 44.520916
20:39:21.463799 Step[3700/6995], lr: 0.001000, mv_avg_loss: 23.991323, loss: 47.275177
20:39:26.265034 Step[3800/6995], lr: 0.001000, mv_avg_loss: 23.803047, loss: 22.356131
20:39:31.064200 Step[3900/6995], lr: 0.001000, mv_avg_loss: 24.926188, loss: 30.456907
20:39:35.859466 Step[4000/6995], lr: 0.001000, mv_avg_loss: 26.222580, loss: 15.272532
20:39:40.656756 Step[4100/6995], lr: 0.001000, mv_avg_loss: 23.947733, loss: 24.401327
20:39:45.467736 Step[4200/6995], lr: 0.001000, mv_avg_loss: 25.623388, loss: 32.507294
20:39:50.267508 Step[4300/6995], lr: 0.001000, mv_avg_loss: 25.628351, loss: 19.636623
20:39:55.058184 Step[4400/6995], lr: 0.001000, mv_avg_loss: 23.209185, loss: 21.087059
20:39:59.850940 Step[4500/6995], lr: 0.001000, mv_avg_loss: 23.614809, loss: 22.658901
20:40:04.643141 Step[4600/6995], lr: 0.001000, mv_avg_loss: 24.221621, loss: 20.577478
20:40:09.440252 Step[4700/6995], lr: 0.001000, mv_avg_loss: 22.481571, loss: 19.154449
20:40:14.228220 Step[4800/6995], lr: 0.001000, mv_avg_loss: 23.890930, loss: 15.325081
20:40:19.016299 Step[4900/6995], lr: 0.001000, mv_avg_loss: 25.782175, loss: 27.270779
20:40:23.819102 Step[5000/6995], lr: 0.001000, mv_avg_loss: 26.759356, loss: 21.514572
20:40:28.612673 Step[5100/6995], lr: 0.001000, mv_avg_loss: 22.969639, loss: 21.397324
20:40:33.475253 Step[5200/6995], lr: 0.001000, mv_avg_loss: 23.026375, loss: 34.399822
20:40:38.280703 Step[5300/6995], lr: 0.001000, mv_avg_loss: 24.170876, loss: 27.465704
20:40:43.065280 Step[5400/6995], lr: 0.001000, mv_avg_loss: 24.318607, loss: 16.091969
20:40:47.859238 Step[5500/6995], lr: 0.001000, mv_avg_loss: 22.623478, loss: 18.249474
20:40:52.646570 Step[5600/6995], lr: 0.001000, mv_avg_loss: 22.886213, loss: 15.988485
20:40:57.438376 Step[5700/6995], lr: 0.001000, mv_avg_loss: 26.869198, loss: 22.724731
20:41:02.236873 Step[5800/6995], lr: 0.001000, mv_avg_loss: 25.261984, loss: 23.500486
20:41:07.019684 Step[5900/6995], lr: 0.001000, mv_avg_loss: 23.919786, loss: 18.567562
20:41:11.803756 Step[6000/6995], lr: 0.001000, mv_avg_loss: 21.794407, loss: 18.067799
20:41:16.606250 Step[6100/6995], lr: 0.001000, mv_avg_loss: 23.769577, loss: 28.269857
20:41:21.394442 Step[6200/6995], lr: 0.001000, mv_avg_loss: 24.629110, loss: 14.719255
20:41:26.184423 Step[6300/6995], lr: 0.001000, mv_avg_loss: 24.506498, loss: 25.916119
20:41:30.970248 Step[6400/6995], lr: 0.001000, mv_avg_loss: 23.242146, loss: 18.191496
20:41:35.746919 Step[6500/6995], lr: 0.001000, mv_avg_loss: 25.373941, loss: 29.453716
20:41:40.535655 Step[6600/6995], lr: 0.001000, mv_avg_loss: 24.613493, loss: 23.249107
20:41:45.316121 Step[6700/6995], lr: 0.001000, mv_avg_loss: 24.882771, loss: 14.920628
20:41:50.107728 Step[6800/6995], lr: 0.001000, mv_avg_loss: 26.385323, loss: 28.800854
20:41:54.889847 Step[6900/6995], lr: 0.001000, mv_avg_loss: 28.164633, loss: 23.585876
Testing for epoch: 38
Average test PNSR is 25.360482 for 500 images
Start to train epoch 39
20:42:21.658223 Step[0/6995], lr: 0.001000, mv_avg_loss: 23.617008, loss: 25.712439
20:42:26.415485 Step[100/6995], lr: 0.001000, mv_avg_loss: 24.383274, loss: 14.306522
20:42:31.189830 Step[200/6995], lr: 0.001000, mv_avg_loss: 24.820221, loss: 18.874792
20:42:35.957281 Step[300/6995], lr: 0.001000, mv_avg_loss: 26.315132, loss: 15.630978
20:42:40.739738 Step[400/6995], lr: 0.001000, mv_avg_loss: 26.748678, loss: 38.283646
20:42:45.514563 Step[500/6995], lr: 0.001000, mv_avg_loss: 24.108423, loss: 18.026876
20:42:50.290821 Step[600/6995], lr: 0.001000, mv_avg_loss: 25.201277, loss: 21.843252
20:42:55.069130 Step[700/6995], lr: 0.001000, mv_avg_loss: 25.194426, loss: 23.408773
20:42:59.822825 Step[800/6995], lr: 0.001000, mv_avg_loss: 23.196787, loss: 17.802059
20:43:04.610145 Step[900/6995], lr: 0.001000, mv_avg_loss: 24.968973, loss: 15.853029
20:43:09.378462 Step[1000/6995], lr: 0.001000, mv_avg_loss: 24.908020, loss: 14.900724
20:43:14.167277 Step[1100/6995], lr: 0.001000, mv_avg_loss: 23.310648, loss: 28.121700
20:43:18.948571 Step[1200/6995], lr: 0.001000, mv_avg_loss: 23.085075, loss: 21.949026
20:43:23.721465 Step[1300/6995], lr: 0.001000, mv_avg_loss: 24.265635, loss: 23.134399
20:43:28.500030 Step[1400/6995], lr: 0.001000, mv_avg_loss: 25.850388, loss: 28.242056
20:43:33.262008 Step[1500/6995], lr: 0.001000, mv_avg_loss: 23.421398, loss: 27.366592
20:43:38.036497 Step[1600/6995], lr: 0.001000, mv_avg_loss: 22.082993, loss: 21.889343
20:43:42.816550 Step[1700/6995], lr: 0.001000, mv_avg_loss: 25.724911, loss: 16.507576
20:43:47.602309 Step[1800/6995], lr: 0.001000, mv_avg_loss: 27.418102, loss: 29.669415
20:43:52.387462 Step[1900/6995], lr: 0.001000, mv_avg_loss: 28.282291, loss: 22.164770
20:43:57.172147 Step[2000/6995], lr: 0.001000, mv_avg_loss: 23.483667, loss: 24.637861
20:44:01.957745 Step[2100/6995], lr: 0.001000, mv_avg_loss: 23.522505, loss: 18.254210
20:44:06.776727 Step[2200/6995], lr: 0.001000, mv_avg_loss: 22.970512, loss: 31.622868
20:44:11.541166 Step[2300/6995], lr: 0.001000, mv_avg_loss: 27.944965, loss: 27.026871
20:44:16.327010 Step[2400/6995], lr: 0.001000, mv_avg_loss: 24.958288, loss: 13.370830
20:44:21.108327 Step[2500/6995], lr: 0.001000, mv_avg_loss: 23.381714, loss: 25.245304
20:44:25.883115 Step[2600/6995], lr: 0.001000, mv_avg_loss: 21.464647, loss: 28.261745
20:44:30.664719 Step[2700/6995], lr: 0.001000, mv_avg_loss: 23.504381, loss: 18.424934
20:44:35.456826 Step[2800/6995], lr: 0.001000, mv_avg_loss: 24.071814, loss: 22.933218
20:44:40.235497 Step[2900/6995], lr: 0.001000, mv_avg_loss: 21.680351, loss: 25.647833
20:44:45.010509 Step[3000/6995], lr: 0.001000, mv_avg_loss: 22.513910, loss: 15.621422
20:44:49.790380 Step[3100/6995], lr: 0.001000, mv_avg_loss: 24.675041, loss: 16.661957
20:44:54.580503 Step[3200/6995], lr: 0.001000, mv_avg_loss: 23.623640, loss: 24.471807
20:44:59.361977 Step[3300/6995], lr: 0.001000, mv_avg_loss: 24.650452, loss: 28.455853
20:45:04.152027 Step[3400/6995], lr: 0.001000, mv_avg_loss: 24.630131, loss: 21.085670
20:45:08.928931 Step[3500/6995], lr: 0.001000, mv_avg_loss: 24.902771, loss: 41.115936
20:45:13.722985 Step[3600/6995], lr: 0.001000, mv_avg_loss: 22.763533, loss: 28.867153
20:45:18.508153 Step[3700/6995], lr: 0.001000, mv_avg_loss: 24.467392, loss: 21.935011
20:45:23.282039 Step[3800/6995], lr: 0.001000, mv_avg_loss: 27.485115, loss: 15.420558
20:45:28.064698 Step[3900/6995], lr: 0.001000, mv_avg_loss: 23.719328, loss: 27.075577
20:45:32.845629 Step[4000/6995], lr: 0.001000, mv_avg_loss: 24.604000, loss: 26.637892
20:45:37.633187 Step[4100/6995], lr: 0.001000, mv_avg_loss: 24.797327, loss: 19.293337
20:45:42.409529 Step[4200/6995], lr: 0.001000, mv_avg_loss: 23.446411, loss: 27.712658
20:45:47.189652 Step[4300/6995], lr: 0.001000, mv_avg_loss: 27.095835, loss: 20.970774
20:45:51.972130 Step[4400/6995], lr: 0.001000, mv_avg_loss: 29.487404, loss: 32.368542
20:45:56.767997 Step[4500/6995], lr: 0.001000, mv_avg_loss: 24.829933, loss: 23.181633
20:46:01.553521 Step[4600/6995], lr: 0.001000, mv_avg_loss: 25.112509, loss: 33.899879
20:46:06.324239 Step[4700/6995], lr: 0.001000, mv_avg_loss: 25.643318, loss: 32.973389
20:46:11.115116 Step[4800/6995], lr: 0.001000, mv_avg_loss: 23.939056, loss: 20.290701
20:46:15.899591 Step[4900/6995], lr: 0.001000, mv_avg_loss: 25.663437, loss: 19.682953
20:46:20.680512 Step[5000/6995], lr: 0.001000, mv_avg_loss: 23.646204, loss: 17.259363
20:46:25.451910 Step[5100/6995], lr: 0.001000, mv_avg_loss: 21.976650, loss: 17.723225
20:46:30.231755 Step[5200/6995], lr: 0.001000, mv_avg_loss: 22.799587, loss: 27.386755
20:46:35.017556 Step[5300/6995], lr: 0.001000, mv_avg_loss: 23.115540, loss: 13.420123
20:46:39.800967 Step[5400/6995], lr: 0.001000, mv_avg_loss: 23.700609, loss: 21.294079
20:46:44.573286 Step[5500/6995], lr: 0.001000, mv_avg_loss: 25.101871, loss: 16.313988
20:46:49.366483 Step[5600/6995], lr: 0.001000, mv_avg_loss: 23.865658, loss: 18.675774
20:46:54.156643 Step[5700/6995], lr: 0.001000, mv_avg_loss: 25.536325, loss: 20.811390
20:46:58.934890 Step[5800/6995], lr: 0.001000, mv_avg_loss: 22.470711, loss: 19.115580
20:47:03.720979 Step[5900/6995], lr: 0.001000, mv_avg_loss: 26.498251, loss: 25.024506
20:47:08.522942 Step[6000/6995], lr: 0.001000, mv_avg_loss: 24.873417, loss: 23.317869
20:47:13.310650 Step[6100/6995], lr: 0.001000, mv_avg_loss: 32.330589, loss: 22.204458
20:47:18.092672 Step[6200/6995], lr: 0.001000, mv_avg_loss: 25.322197, loss: 16.702320
20:47:22.876071 Step[6300/6995], lr: 0.001000, mv_avg_loss: 25.178934, loss: 37.570656
20:47:27.661104 Step[6400/6995], lr: 0.001000, mv_avg_loss: 25.048622, loss: 26.664925
20:47:32.455119 Step[6500/6995], lr: 0.001000, mv_avg_loss: 24.076674, loss: 20.070408
20:47:37.250268 Step[6600/6995], lr: 0.001000, mv_avg_loss: 24.691597, loss: 18.177223
20:47:42.036808 Step[6700/6995], lr: 0.001000, mv_avg_loss: 21.862881, loss: 19.271667
20:47:46.841382 Step[6800/6995], lr: 0.001000, mv_avg_loss: 25.981142, loss: 28.916298
20:47:51.623706 Step[6900/6995], lr: 0.001000, mv_avg_loss: 25.487495, loss: 20.210056
saving model for epoch 39
Testing for epoch: 39
Average test PNSR is 25.328154 for 500 images
Start to train epoch 40
20:48:18.578970 Step[0/6995], lr: 0.000010, mv_avg_loss: 25.231592, loss: 18.846842
20:48:23.351873 Step[100/6995], lr: 0.000010, mv_avg_loss: 21.276619, loss: 21.094006
20:48:28.175571 Step[200/6995], lr: 0.000010, mv_avg_loss: 21.139065, loss: 10.169672
20:48:32.951785 Step[300/6995], lr: 0.000010, mv_avg_loss: 19.915480, loss: 34.331970
20:48:37.727000 Step[400/6995], lr: 0.000010, mv_avg_loss: 19.985142, loss: 21.958815
20:48:42.517723 Step[500/6995], lr: 0.000010, mv_avg_loss: 17.731176, loss: 19.975124
20:48:47.298720 Step[600/6995], lr: 0.000010, mv_avg_loss: 19.912203, loss: 23.785084
20:48:52.091475 Step[700/6995], lr: 0.000010, mv_avg_loss: 19.314037, loss: 11.789378
20:48:56.898483 Step[800/6995], lr: 0.000010, mv_avg_loss: 19.314207, loss: 22.001640
20:49:01.681903 Step[900/6995], lr: 0.000010, mv_avg_loss: 19.492590, loss: 22.276310
20:49:06.466525 Step[1000/6995], lr: 0.000010, mv_avg_loss: 18.682726, loss: 12.167999
20:49:11.240500 Step[1100/6995], lr: 0.000010, mv_avg_loss: 17.121193, loss: 12.490635
20:49:16.027553 Step[1200/6995], lr: 0.000010, mv_avg_loss: 17.166271, loss: 15.673144
20:49:20.810394 Step[1300/6995], lr: 0.000010, mv_avg_loss: 19.701611, loss: 26.024517
20:49:25.598543 Step[1400/6995], lr: 0.000010, mv_avg_loss: 17.584833, loss: 25.219143
20:49:30.389792 Step[1500/6995], lr: 0.000010, mv_avg_loss: 18.761856, loss: 17.452366
20:49:35.174269 Step[1600/6995], lr: 0.000010, mv_avg_loss: 17.705212, loss: 12.203353
20:49:39.967589 Step[1700/6995], lr: 0.000010, mv_avg_loss: 19.553642, loss: 26.473400
20:49:44.762580 Step[1800/6995], lr: 0.000010, mv_avg_loss: 17.463461, loss: 14.179553
20:49:49.551987 Step[1900/6995], lr: 0.000010, mv_avg_loss: 16.954750, loss: 17.814774
20:49:54.348539 Step[2000/6995], lr: 0.000010, mv_avg_loss: 17.677912, loss: 15.962966
20:49:59.136399 Step[2100/6995], lr: 0.000010, mv_avg_loss: 17.901445, loss: 10.911815
20:50:03.920272 Step[2200/6995], lr: 0.000010, mv_avg_loss: 16.230892, loss: 10.573565
20:50:08.708129 Step[2300/6995], lr: 0.000010, mv_avg_loss: 18.933954, loss: 25.736935
20:50:13.496828 Step[2400/6995], lr: 0.000010, mv_avg_loss: 18.115208, loss: 22.384735
20:50:18.289385 Step[2500/6995], lr: 0.000010, mv_avg_loss: 17.334234, loss: 22.451895
20:50:23.087735 Step[2600/6995], lr: 0.000010, mv_avg_loss: 17.210526, loss: 15.986576
20:50:27.877966 Step[2700/6995], lr: 0.000010, mv_avg_loss: 17.278381, loss: 17.876461
20:50:32.682530 Step[2800/6995], lr: 0.000010, mv_avg_loss: 17.863419, loss: 11.911123
20:50:37.480106 Step[2900/6995], lr: 0.000010, mv_avg_loss: 18.540138, loss: 25.301598
20:50:42.291591 Step[3000/6995], lr: 0.000010, mv_avg_loss: 17.453915, loss: 22.657211
20:50:47.086789 Step[3100/6995], lr: 0.000010, mv_avg_loss: 18.130955, loss: 17.656601
20:50:51.883515 Step[3200/6995], lr: 0.000010, mv_avg_loss: 19.541531, loss: 19.291889
20:50:56.683450 Step[3300/6995], lr: 0.000010, mv_avg_loss: 17.897499, loss: 19.140076
20:51:01.484191 Step[3400/6995], lr: 0.000010, mv_avg_loss: 16.887550, loss: 12.786723
20:51:06.289633 Step[3500/6995], lr: 0.000010, mv_avg_loss: 18.330324, loss: 27.059452
20:51:11.092855 Step[3600/6995], lr: 0.000010, mv_avg_loss: 16.701185, loss: 13.746576
20:51:15.892635 Step[3700/6995], lr: 0.000010, mv_avg_loss: 18.863794, loss: 14.512754
20:51:20.696744 Step[3800/6995], lr: 0.000010, mv_avg_loss: 16.763117, loss: 14.374507
20:51:25.500240 Step[3900/6995], lr: 0.000010, mv_avg_loss: 17.871710, loss: 31.384525
20:51:30.299144 Step[4000/6995], lr: 0.000010, mv_avg_loss: 17.223597, loss: 14.266800
20:51:35.105341 Step[4100/6995], lr: 0.000010, mv_avg_loss: 18.682817, loss: 14.159495
20:51:39.896130 Step[4200/6995], lr: 0.000010, mv_avg_loss: 18.124180, loss: 9.866333
20:51:44.695047 Step[4300/6995], lr: 0.000010, mv_avg_loss: 17.749962, loss: 26.002441
20:51:49.504791 Step[4400/6995], lr: 0.000010, mv_avg_loss: 15.888330, loss: 15.965892
20:51:54.308773 Step[4500/6995], lr: 0.000010, mv_avg_loss: 16.819536, loss: 21.154312
20:51:59.109867 Step[4600/6995], lr: 0.000010, mv_avg_loss: 18.222446, loss: 19.879307
20:52:03.912225 Step[4700/6995], lr: 0.000010, mv_avg_loss: 17.347532, loss: 15.457705
20:52:08.719623 Step[4800/6995], lr: 0.000010, mv_avg_loss: 18.001465, loss: 27.154961
20:52:13.520381 Step[4900/6995], lr: 0.000010, mv_avg_loss: 16.239283, loss: 19.958609
20:52:18.323925 Step[5000/6995], lr: 0.000010, mv_avg_loss: 17.701704, loss: 13.878879
20:52:23.123796 Step[5100/6995], lr: 0.000010, mv_avg_loss: 16.971195, loss: 10.575315
20:52:27.944981 Step[5200/6995], lr: 0.000010, mv_avg_loss: 17.580969, loss: 13.158743
20:52:32.742097 Step[5300/6995], lr: 0.000010, mv_avg_loss: 16.905458, loss: 14.549063
20:52:37.543900 Step[5400/6995], lr: 0.000010, mv_avg_loss: 17.991207, loss: 17.528801
20:52:42.331998 Step[5500/6995], lr: 0.000010, mv_avg_loss: 17.114384, loss: 13.717220
20:52:47.132215 Step[5600/6995], lr: 0.000010, mv_avg_loss: 16.840279, loss: 15.268509
20:52:51.928087 Step[5700/6995], lr: 0.000010, mv_avg_loss: 16.043423, loss: 11.483685
20:52:56.726572 Step[5800/6995], lr: 0.000010, mv_avg_loss: 17.274765, loss: 12.156005
20:53:01.524696 Step[5900/6995], lr: 0.000010, mv_avg_loss: 16.887484, loss: 21.309929
20:53:06.323913 Step[6000/6995], lr: 0.000010, mv_avg_loss: 16.915558, loss: 16.746262
20:53:11.165082 Step[6100/6995], lr: 0.000010, mv_avg_loss: 17.356413, loss: 15.964679
20:53:15.963744 Step[6200/6995], lr: 0.000010, mv_avg_loss: 16.416847, loss: 16.302309
20:53:20.765660 Step[6300/6995], lr: 0.000010, mv_avg_loss: 17.107714, loss: 13.259940
20:53:25.567206 Step[6400/6995], lr: 0.000010, mv_avg_loss: 16.507725, loss: 11.084295
20:53:30.353993 Step[6500/6995], lr: 0.000010, mv_avg_loss: 18.132355, loss: 11.438331
20:53:35.161456 Step[6600/6995], lr: 0.000010, mv_avg_loss: 16.415508, loss: 13.603214
20:53:39.973415 Step[6700/6995], lr: 0.000010, mv_avg_loss: 16.978048, loss: 21.775158
20:53:44.775539 Step[6800/6995], lr: 0.000010, mv_avg_loss: 16.877735, loss: 13.078329
20:53:49.569355 Step[6900/6995], lr: 0.000010, mv_avg_loss: 17.999458, loss: 16.926725
Testing for epoch: 40
Average test PNSR is 25.426583 for 500 images
Start to train epoch 41
20:54:16.740783 Step[0/6995], lr: 0.000100, mv_avg_loss: 16.244068, loss: 20.069227
20:54:21.531634 Step[100/6995], lr: 0.000100, mv_avg_loss: 16.829784, loss: 16.065912
20:54:26.318828 Step[200/6995], lr: 0.000100, mv_avg_loss: 17.063828, loss: 14.477600
20:54:31.106281 Step[300/6995], lr: 0.000100, mv_avg_loss: 16.096855, loss: 13.192432
20:54:35.895366 Step[400/6995], lr: 0.000100, mv_avg_loss: 17.752258, loss: 17.506727
20:54:40.674969 Step[500/6995], lr: 0.000100, mv_avg_loss: 16.206562, loss: 19.840364
20:54:45.456830 Step[600/6995], lr: 0.000100, mv_avg_loss: 15.975353, loss: 17.816177
20:54:50.249789 Step[700/6995], lr: 0.000100, mv_avg_loss: 15.638901, loss: 15.017154
20:54:55.032461 Step[800/6995], lr: 0.000100, mv_avg_loss: 15.904070, loss: 19.962757
20:54:59.816198 Step[900/6995], lr: 0.000100, mv_avg_loss: 17.189856, loss: 21.734005
20:55:04.604647 Step[1000/6995], lr: 0.000100, mv_avg_loss: 16.353420, loss: 10.998444
20:55:09.391181 Step[1100/6995], lr: 0.000100, mv_avg_loss: 16.414043, loss: 10.838097
20:55:14.173761 Step[1200/6995], lr: 0.000100, mv_avg_loss: 16.127449, loss: 13.411615
20:55:18.959323 Step[1300/6995], lr: 0.000100, mv_avg_loss: 16.460566, loss: 13.841036
20:55:23.748676 Step[1400/6995], lr: 0.000100, mv_avg_loss: 16.450766, loss: 19.960522
20:55:28.533326 Step[1500/6995], lr: 0.000100, mv_avg_loss: 16.565502, loss: 24.771667
20:55:33.313528 Step[1600/6995], lr: 0.000100, mv_avg_loss: 16.156298, loss: 17.680273
20:55:38.101011 Step[1700/6995], lr: 0.000100, mv_avg_loss: 16.961731, loss: 24.744337
20:55:42.898782 Step[1800/6995], lr: 0.000100, mv_avg_loss: 16.588493, loss: 12.536330
20:55:47.684672 Step[1900/6995], lr: 0.000100, mv_avg_loss: 16.849218, loss: 14.930325
20:55:52.478555 Step[2000/6995], lr: 0.000100, mv_avg_loss: 15.796432, loss: 14.164566
20:55:57.260344 Step[2100/6995], lr: 0.000100, mv_avg_loss: 17.124374, loss: 14.197296
20:56:02.043303 Step[2200/6995], lr: 0.000100, mv_avg_loss: 15.991389, loss: 12.424120
20:56:06.833006 Step[2300/6995], lr: 0.000100, mv_avg_loss: 16.944031, loss: 15.873075
20:56:11.621495 Step[2400/6995], lr: 0.000100, mv_avg_loss: 19.186890, loss: 22.555502
20:56:16.406367 Step[2500/6995], lr: 0.000100, mv_avg_loss: 16.291037, loss: 16.856260
20:56:21.211662 Step[2600/6995], lr: 0.000100, mv_avg_loss: 16.183460, loss: 19.320385
20:56:25.998547 Step[2700/6995], lr: 0.000100, mv_avg_loss: 17.224150, loss: 15.382655
20:56:30.800437 Step[2800/6995], lr: 0.000100, mv_avg_loss: 17.117300, loss: 11.382027
20:56:35.600067 Step[2900/6995], lr: 0.000100, mv_avg_loss: 16.919289, loss: 16.743633
20:56:40.387391 Step[3000/6995], lr: 0.000100, mv_avg_loss: 18.392736, loss: 11.654181
20:56:45.176354 Step[3100/6995], lr: 0.000100, mv_avg_loss: 17.616796, loss: 13.752272
20:56:49.980747 Step[3200/6995], lr: 0.000100, mv_avg_loss: 15.543391, loss: 16.708847
20:56:54.794226 Step[3300/6995], lr: 0.000100, mv_avg_loss: 16.660444, loss: 14.677235
20:56:59.581035 Step[3400/6995], lr: 0.000100, mv_avg_loss: 17.192816, loss: 26.197430
20:57:04.367836 Step[3500/6995], lr: 0.000100, mv_avg_loss: 15.932425, loss: 13.473921
20:57:09.173543 Step[3600/6995], lr: 0.000100, mv_avg_loss: 16.836721, loss: 19.149273
20:57:14.028572 Step[3700/6995], lr: 0.000100, mv_avg_loss: 17.933872, loss: 13.749506
20:57:18.882186 Step[3800/6995], lr: 0.000100, mv_avg_loss: 17.168264, loss: 14.606509
20:57:23.715262 Step[3900/6995], lr: 0.000100, mv_avg_loss: 16.317055, loss: 12.830546
20:57:28.496132 Step[4000/6995], lr: 0.000100, mv_avg_loss: 16.051800, loss: 19.303093
20:57:33.270007 Step[4100/6995], lr: 0.000100, mv_avg_loss: 16.580091, loss: 15.501357
20:57:38.050629 Step[4200/6995], lr: 0.000100, mv_avg_loss: 15.282642, loss: 12.998336
20:57:42.820760 Step[4300/6995], lr: 0.000100, mv_avg_loss: 16.682549, loss: 13.798488
20:57:47.599180 Step[4400/6995], lr: 0.000100, mv_avg_loss: 17.544184, loss: 15.880814
20:57:52.388716 Step[4500/6995], lr: 0.000100, mv_avg_loss: 15.841446, loss: 13.881778
20:57:57.176479 Step[4600/6995], lr: 0.000100, mv_avg_loss: 16.921957, loss: 10.617025
20:58:01.953382 Step[4700/6995], lr: 0.000100, mv_avg_loss: 16.718058, loss: 18.492586
20:58:06.732643 Step[4800/6995], lr: 0.000100, mv_avg_loss: 16.358551, loss: 12.492186
20:58:11.518283 Step[4900/6995], lr: 0.000100, mv_avg_loss: 16.223690, loss: 12.680484
20:58:16.300943 Step[5000/6995], lr: 0.000100, mv_avg_loss: 16.682753, loss: 14.642836
20:58:21.082867 Step[5100/6995], lr: 0.000100, mv_avg_loss: 16.443935, loss: 11.029021
20:58:25.868200 Step[5200/6995], lr: 0.000100, mv_avg_loss: 16.944530, loss: 19.337620
20:58:30.643296 Step[5300/6995], lr: 0.000100, mv_avg_loss: 16.560438, loss: 15.995955
20:58:35.421655 Step[5400/6995], lr: 0.000100, mv_avg_loss: 15.478922, loss: 19.696018
20:58:40.196090 Step[5500/6995], lr: 0.000100, mv_avg_loss: 15.747970, loss: 14.015376
20:58:44.971472 Step[5600/6995], lr: 0.000100, mv_avg_loss: 15.798656, loss: 27.954443
20:58:49.744291 Step[5700/6995], lr: 0.000100, mv_avg_loss: 17.283125, loss: 10.639320
20:58:54.519640 Step[5800/6995], lr: 0.000100, mv_avg_loss: 16.423182, loss: 20.389929
20:58:59.300571 Step[5900/6995], lr: 0.000100, mv_avg_loss: 16.825678, loss: 13.421017
20:59:04.091047 Step[6000/6995], lr: 0.000100, mv_avg_loss: 16.819422, loss: 17.113527
20:59:08.864628 Step[6100/6995], lr: 0.000100, mv_avg_loss: 16.798729, loss: 11.541361
20:59:13.646142 Step[6200/6995], lr: 0.000100, mv_avg_loss: 15.961049, loss: 21.140490
20:59:18.434499 Step[6300/6995], lr: 0.000100, mv_avg_loss: 16.786625, loss: 11.907576
20:59:23.214071 Step[6400/6995], lr: 0.000100, mv_avg_loss: 14.823563, loss: 14.120274
20:59:27.984499 Step[6500/6995], lr: 0.000100, mv_avg_loss: 16.597370, loss: 9.897610
20:59:32.770457 Step[6600/6995], lr: 0.000100, mv_avg_loss: 17.709904, loss: 28.344070
20:59:37.561028 Step[6700/6995], lr: 0.000100, mv_avg_loss: 14.294536, loss: 12.909961
20:59:42.349685 Step[6800/6995], lr: 0.000100, mv_avg_loss: 15.380630, loss: 15.744437
20:59:47.140693 Step[6900/6995], lr: 0.000100, mv_avg_loss: 16.179905, loss: 14.772600
Testing for epoch: 41
Average test PNSR is 25.443089 for 500 images
Start to train epoch 42
21:00:14.068832 Step[0/6995], lr: 0.000100, mv_avg_loss: 17.724545, loss: 14.277351
21:00:18.827808 Step[100/6995], lr: 0.000100, mv_avg_loss: 16.082350, loss: 25.786377
21:00:23.609979 Step[200/6995], lr: 0.000100, mv_avg_loss: 16.800785, loss: 17.561100
21:00:28.376594 Step[300/6995], lr: 0.000100, mv_avg_loss: 15.985188, loss: 21.270102
21:00:33.144057 Step[400/6995], lr: 0.000100, mv_avg_loss: 16.548506, loss: 9.763603
21:00:37.927401 Step[500/6995], lr: 0.000100, mv_avg_loss: 15.659909, loss: 9.942959
21:00:42.708161 Step[600/6995], lr: 0.000100, mv_avg_loss: 15.365632, loss: 12.581988
21:00:47.485789 Step[700/6995], lr: 0.000100, mv_avg_loss: 17.331482, loss: 17.157463
21:00:52.297674 Step[800/6995], lr: 0.000100, mv_avg_loss: 16.525480, loss: 15.481960
21:00:57.085562 Step[900/6995], lr: 0.000100, mv_avg_loss: 15.994377, loss: 16.789074
21:01:01.869655 Step[1000/6995], lr: 0.000100, mv_avg_loss: 15.789939, loss: 10.998034
21:01:06.640871 Step[1100/6995], lr: 0.000100, mv_avg_loss: 16.082582, loss: 30.941242
21:01:11.434724 Step[1200/6995], lr: 0.000100, mv_avg_loss: 15.488125, loss: 11.642279
21:01:16.240763 Step[1300/6995], lr: 0.000100, mv_avg_loss: 16.707645, loss: 15.231112
21:01:21.020354 Step[1400/6995], lr: 0.000100, mv_avg_loss: 16.125021, loss: 17.862301
21:01:25.800892 Step[1500/6995], lr: 0.000100, mv_avg_loss: 16.541477, loss: 15.928558
21:01:30.592869 Step[1600/6995], lr: 0.000100, mv_avg_loss: 15.240943, loss: 15.540854
21:01:35.383776 Step[1700/6995], lr: 0.000100, mv_avg_loss: 16.181515, loss: 17.918171
21:01:40.173808 Step[1800/6995], lr: 0.000100, mv_avg_loss: 16.711864, loss: 27.917645
21:01:44.965702 Step[1900/6995], lr: 0.000100, mv_avg_loss: 15.934768, loss: 12.230816
21:01:49.755598 Step[2000/6995], lr: 0.000100, mv_avg_loss: 16.826954, loss: 11.702177
21:01:54.541834 Step[2100/6995], lr: 0.000100, mv_avg_loss: 14.741706, loss: 14.581125
21:01:59.332534 Step[2200/6995], lr: 0.000100, mv_avg_loss: 15.871927, loss: 18.270025
21:02:04.129253 Step[2300/6995], lr: 0.000100, mv_avg_loss: 17.361563, loss: 9.698128
21:02:08.921164 Step[2400/6995], lr: 0.000100, mv_avg_loss: 16.415113, loss: 24.743343
21:02:13.720470 Step[2500/6995], lr: 0.000100, mv_avg_loss: 16.702505, loss: 16.047003
21:02:18.505020 Step[2600/6995], lr: 0.000100, mv_avg_loss: 15.996765, loss: 19.112030
21:02:23.288353 Step[2700/6995], lr: 0.000100, mv_avg_loss: 16.229719, loss: 9.927298
21:02:28.088435 Step[2800/6995], lr: 0.000100, mv_avg_loss: 16.114452, loss: 17.253521
21:02:32.889317 Step[2900/6995], lr: 0.000100, mv_avg_loss: 16.582039, loss: 16.715843
21:02:37.678562 Step[3000/6995], lr: 0.000100, mv_avg_loss: 16.931114, loss: 17.995359
21:02:42.471198 Step[3100/6995], lr: 0.000100, mv_avg_loss: 15.910972, loss: 10.761375
21:02:47.283031 Step[3200/6995], lr: 0.000100, mv_avg_loss: 14.874268, loss: 14.008120
21:02:52.093193 Step[3300/6995], lr: 0.000100, mv_avg_loss: 15.917447, loss: 11.726668
21:02:56.900406 Step[3400/6995], lr: 0.000100, mv_avg_loss: 16.890169, loss: 10.092239
21:03:01.700078 Step[3500/6995], lr: 0.000100, mv_avg_loss: 14.947013, loss: 12.195776
21:03:06.504932 Step[3600/6995], lr: 0.000100, mv_avg_loss: 16.631897, loss: 13.191757
21:03:11.295983 Step[3700/6995], lr: 0.000100, mv_avg_loss: 16.603453, loss: 21.230059
21:03:16.093966 Step[3800/6995], lr: 0.000100, mv_avg_loss: 15.690757, loss: 14.666386
21:03:20.907942 Step[3900/6995], lr: 0.000100, mv_avg_loss: 15.913340, loss: 18.930424
21:03:25.698464 Step[4000/6995], lr: 0.000100, mv_avg_loss: 14.541081, loss: 12.834824
21:03:30.485054 Step[4100/6995], lr: 0.000100, mv_avg_loss: 16.304178, loss: 14.036052
21:03:35.279716 Step[4200/6995], lr: 0.000100, mv_avg_loss: 17.034863, loss: 16.523567
21:03:40.107395 Step[4300/6995], lr: 0.000100, mv_avg_loss: 17.529837, loss: 16.794552
21:03:44.915435 Step[4400/6995], lr: 0.000100, mv_avg_loss: 16.188999, loss: 13.153206
21:03:49.708964 Step[4500/6995], lr: 0.000100, mv_avg_loss: 16.364990, loss: 22.129299
21:03:54.500967 Step[4600/6995], lr: 0.000100, mv_avg_loss: 15.898150, loss: 11.510772
21:03:59.306682 Step[4700/6995], lr: 0.000100, mv_avg_loss: 17.144552, loss: 19.868633
21:04:04.112093 Step[4800/6995], lr: 0.000100, mv_avg_loss: 16.630663, loss: 19.471008
21:04:08.927016 Step[4900/6995], lr: 0.000100, mv_avg_loss: 16.037619, loss: 15.016682
21:04:13.730140 Step[5000/6995], lr: 0.000100, mv_avg_loss: 15.111503, loss: 18.385183
21:04:18.526215 Step[5100/6995], lr: 0.000100, mv_avg_loss: 16.111612, loss: 19.824566
21:04:23.324064 Step[5200/6995], lr: 0.000100, mv_avg_loss: 17.207829, loss: 17.507755
21:04:28.134878 Step[5300/6995], lr: 0.000100, mv_avg_loss: 14.947907, loss: 9.354679
21:04:32.945578 Step[5400/6995], lr: 0.000100, mv_avg_loss: 15.351138, loss: 9.924606
21:04:37.734745 Step[5500/6995], lr: 0.000100, mv_avg_loss: 16.990871, loss: 11.906306
21:04:42.539417 Step[5600/6995], lr: 0.000100, mv_avg_loss: 17.075216, loss: 22.494488
21:04:47.337122 Step[5700/6995], lr: 0.000100, mv_avg_loss: 17.124401, loss: 18.279882
21:04:52.158375 Step[5800/6995], lr: 0.000100, mv_avg_loss: 15.662978, loss: 12.405489
21:04:56.967898 Step[5900/6995], lr: 0.000100, mv_avg_loss: 16.152128, loss: 14.948711
21:05:01.772708 Step[6000/6995], lr: 0.000100, mv_avg_loss: 16.780088, loss: 21.972441
21:05:06.567207 Step[6100/6995], lr: 0.000100, mv_avg_loss: 17.152544, loss: 12.853659
21:05:11.378319 Step[6200/6995], lr: 0.000100, mv_avg_loss: 17.282526, loss: 18.341637
21:05:16.241331 Step[6300/6995], lr: 0.000100, mv_avg_loss: 15.743489, loss: 11.875835
21:05:21.036484 Step[6400/6995], lr: 0.000100, mv_avg_loss: 15.060286, loss: 17.189777
21:05:25.856696 Step[6500/6995], lr: 0.000100, mv_avg_loss: 15.737741, loss: 16.850182
21:05:30.659399 Step[6600/6995], lr: 0.000100, mv_avg_loss: 15.116799, loss: 16.889307
21:05:35.460296 Step[6700/6995], lr: 0.000100, mv_avg_loss: 16.901527, loss: 16.529324
21:05:40.270337 Step[6800/6995], lr: 0.000100, mv_avg_loss: 17.777395, loss: 17.105743
21:05:45.070686 Step[6900/6995], lr: 0.000100, mv_avg_loss: 15.440450, loss: 21.484631
Testing for epoch: 42
Average test PNSR is 25.424495 for 500 images
Start to train epoch 43
21:06:11.847080 Step[0/6995], lr: 0.000100, mv_avg_loss: 14.904050, loss: 23.150198
21:06:16.635790 Step[100/6995], lr: 0.000100, mv_avg_loss: 16.493809, loss: 13.826689
21:06:21.434308 Step[200/6995], lr: 0.000100, mv_avg_loss: 15.495559, loss: 18.238899
21:06:26.220830 Step[300/6995], lr: 0.000100, mv_avg_loss: 15.804971, loss: 15.077049
21:06:31.013395 Step[400/6995], lr: 0.000100, mv_avg_loss: 14.914783, loss: 13.160408
21:06:35.798134 Step[500/6995], lr: 0.000100, mv_avg_loss: 16.117573, loss: 13.637976
21:06:40.585857 Step[600/6995], lr: 0.000100, mv_avg_loss: 15.300418, loss: 14.375072
21:06:45.368674 Step[700/6995], lr: 0.000100, mv_avg_loss: 15.982047, loss: 10.512989
21:06:50.174277 Step[800/6995], lr: 0.000100, mv_avg_loss: 15.787503, loss: 10.654886
21:06:54.968401 Step[900/6995], lr: 0.000100, mv_avg_loss: 17.744246, loss: 16.908852
21:06:59.752738 Step[1000/6995], lr: 0.000100, mv_avg_loss: 14.663688, loss: 15.770784
21:07:04.555029 Step[1100/6995], lr: 0.000100, mv_avg_loss: 16.184866, loss: 12.069044
21:07:09.357902 Step[1200/6995], lr: 0.000100, mv_avg_loss: 16.195494, loss: 13.202644
21:07:14.149502 Step[1300/6995], lr: 0.000100, mv_avg_loss: 16.065353, loss: 30.841833
21:07:18.940613 Step[1400/6995], lr: 0.000100, mv_avg_loss: 14.720856, loss: 11.312641
21:07:23.734651 Step[1500/6995], lr: 0.000100, mv_avg_loss: 16.444281, loss: 21.719481
21:07:28.518635 Step[1600/6995], lr: 0.000100, mv_avg_loss: 17.109230, loss: 21.169235
21:07:33.306590 Step[1700/6995], lr: 0.000100, mv_avg_loss: 14.691909, loss: 9.901085
21:07:38.095217 Step[1800/6995], lr: 0.000100, mv_avg_loss: 16.580839, loss: 15.574274
21:07:42.908964 Step[1900/6995], lr: 0.000100, mv_avg_loss: 15.721639, loss: 15.702570
21:07:47.704159 Step[2000/6995], lr: 0.000100, mv_avg_loss: 16.265257, loss: 9.802516
21:07:52.495991 Step[2100/6995], lr: 0.000100, mv_avg_loss: 16.699705, loss: 18.380398
21:07:57.282707 Step[2200/6995], lr: 0.000100, mv_avg_loss: 15.601918, loss: 15.096010
21:08:02.069368 Step[2300/6995], lr: 0.000100, mv_avg_loss: 15.508174, loss: 26.228895
21:08:06.868372 Step[2400/6995], lr: 0.000100, mv_avg_loss: 16.010103, loss: 17.220457
21:08:11.651852 Step[2500/6995], lr: 0.000100, mv_avg_loss: 16.350136, loss: 13.611822
21:08:16.449900 Step[2600/6995], lr: 0.000100, mv_avg_loss: 15.767858, loss: 12.695789
21:08:21.241710 Step[2700/6995], lr: 0.000100, mv_avg_loss: 15.986648, loss: 11.716087
21:08:26.040570 Step[2800/6995], lr: 0.000100, mv_avg_loss: 16.593208, loss: 13.737613
21:08:30.828797 Step[2900/6995], lr: 0.000100, mv_avg_loss: 15.472766, loss: 22.755749
21:08:35.630378 Step[3000/6995], lr: 0.000100, mv_avg_loss: 16.376812, loss: 19.663193
21:08:40.422193 Step[3100/6995], lr: 0.000100, mv_avg_loss: 16.804152, loss: 13.685162
21:08:45.213928 Step[3200/6995], lr: 0.000100, mv_avg_loss: 16.894367, loss: 17.250885
21:08:50.019434 Step[3300/6995], lr: 0.000100, mv_avg_loss: 16.022394, loss: 13.050685
21:08:54.809612 Step[3400/6995], lr: 0.000100, mv_avg_loss: 14.712298, loss: 18.498463
21:08:59.608996 Step[3500/6995], lr: 0.000100, mv_avg_loss: 16.949146, loss: 21.627396
21:09:04.392722 Step[3600/6995], lr: 0.000100, mv_avg_loss: 15.589491, loss: 16.548180
21:09:09.172728 Step[3700/6995], lr: 0.000100, mv_avg_loss: 15.537469, loss: 13.109779
21:09:13.947033 Step[3800/6995], lr: 0.000100, mv_avg_loss: 15.699710, loss: 20.467400
21:09:18.735458 Step[3900/6995], lr: 0.000100, mv_avg_loss: 17.299669, loss: 8.483493
21:09:23.517291 Step[4000/6995], lr: 0.000100, mv_avg_loss: 15.044381, loss: 18.559315
21:09:28.300096 Step[4100/6995], lr: 0.000100, mv_avg_loss: 16.719568, loss: 21.988022
21:09:33.078659 Step[4200/6995], lr: 0.000100, mv_avg_loss: 15.635434, loss: 12.440001
21:09:37.882228 Step[4300/6995], lr: 0.000100, mv_avg_loss: 16.141146, loss: 13.492510
21:09:42.665458 Step[4400/6995], lr: 0.000100, mv_avg_loss: 16.248842, loss: 9.021894
21:09:47.444903 Step[4500/6995], lr: 0.000100, mv_avg_loss: 16.235250, loss: 12.321129
21:09:52.225453 Step[4600/6995], lr: 0.000100, mv_avg_loss: 16.988615, loss: 18.232382
21:09:57.007885 Step[4700/6995], lr: 0.000100, mv_avg_loss: 15.901543, loss: 13.262754
21:10:01.794251 Step[4800/6995], lr: 0.000100, mv_avg_loss: 15.877655, loss: 12.986450
21:10:06.571477 Step[4900/6995], lr: 0.000100, mv_avg_loss: 15.143855, loss: 14.034439
21:10:11.351163 Step[5000/6995], lr: 0.000100, mv_avg_loss: 14.723018, loss: 20.463442
21:10:16.150670 Step[5100/6995], lr: 0.000100, mv_avg_loss: 16.204733, loss: 17.741009
21:10:20.937768 Step[5200/6995], lr: 0.000100, mv_avg_loss: 14.718796, loss: 14.472765
21:10:25.725928 Step[5300/6995], lr: 0.000100, mv_avg_loss: 15.773858, loss: 20.740040
21:10:30.524270 Step[5400/6995], lr: 0.000100, mv_avg_loss: 17.364676, loss: 14.913275
21:10:35.312489 Step[5500/6995], lr: 0.000100, mv_avg_loss: 15.250963, loss: 15.361761
21:10:40.093891 Step[5600/6995], lr: 0.000100, mv_avg_loss: 14.800744, loss: 22.033813
21:10:44.885350 Step[5700/6995], lr: 0.000100, mv_avg_loss: 15.605278, loss: 11.742984
21:10:49.670756 Step[5800/6995], lr: 0.000100, mv_avg_loss: 16.015911, loss: 15.343257
21:10:54.448299 Step[5900/6995], lr: 0.000100, mv_avg_loss: 15.495904, loss: 15.974495
21:10:59.238284 Step[6000/6995], lr: 0.000100, mv_avg_loss: 16.847757, loss: 23.843491
21:11:04.014792 Step[6100/6995], lr: 0.000100, mv_avg_loss: 17.106564, loss: 13.596057
21:11:08.804598 Step[6200/6995], lr: 0.000100, mv_avg_loss: 16.917356, loss: 14.631396
21:11:13.580126 Step[6300/6995], lr: 0.000100, mv_avg_loss: 16.232466, loss: 9.568618
21:11:18.365355 Step[6400/6995], lr: 0.000100, mv_avg_loss: 16.090601, loss: 17.794977
21:11:23.149841 Step[6500/6995], lr: 0.000100, mv_avg_loss: 15.593356, loss: 11.641485
21:11:27.930757 Step[6600/6995], lr: 0.000100, mv_avg_loss: 15.410170, loss: 14.245483
21:11:32.710767 Step[6700/6995], lr: 0.000100, mv_avg_loss: 17.489700, loss: 17.896713
21:11:37.495776 Step[6800/6995], lr: 0.000100, mv_avg_loss: 15.678441, loss: 13.248222
21:11:42.274589 Step[6900/6995], lr: 0.000100, mv_avg_loss: 16.313734, loss: 10.592516
Testing for epoch: 43
Average test PNSR is 25.371810 for 500 images
Start to train epoch 44
21:12:09.422485 Step[0/6995], lr: 0.000100, mv_avg_loss: 14.139206, loss: 14.549636
21:12:14.187620 Step[100/6995], lr: 0.000100, mv_avg_loss: 15.068095, loss: 14.028639
21:12:18.954526 Step[200/6995], lr: 0.000100, mv_avg_loss: 15.735144, loss: 10.164677
21:12:23.734815 Step[300/6995], lr: 0.000100, mv_avg_loss: 16.187803, loss: 38.506470
21:12:28.506957 Step[400/6995], lr: 0.000100, mv_avg_loss: 17.084923, loss: 11.302302
21:12:33.242664 Step[500/6995], lr: 0.000100, mv_avg_loss: 15.596586, loss: 10.051506
21:12:37.984492 Step[600/6995], lr: 0.000100, mv_avg_loss: 15.678934, loss: 26.730268
21:12:42.727796 Step[700/6995], lr: 0.000100, mv_avg_loss: 15.561465, loss: 10.294364
21:12:47.476545 Step[800/6995], lr: 0.000100, mv_avg_loss: 15.657687, loss: 20.450674
21:12:52.232145 Step[900/6995], lr: 0.000100, mv_avg_loss: 15.409030, loss: 13.572113
21:12:56.982856 Step[1000/6995], lr: 0.000100, mv_avg_loss: 16.625563, loss: 11.130966
21:13:01.728239 Step[1100/6995], lr: 0.000100, mv_avg_loss: 15.350252, loss: 10.944929
21:13:06.470329 Step[1200/6995], lr: 0.000100, mv_avg_loss: 15.936159, loss: 20.786245
21:13:11.298441 Step[1300/6995], lr: 0.000100, mv_avg_loss: 16.785130, loss: 9.398044
21:13:16.056521 Step[1400/6995], lr: 0.000100, mv_avg_loss: 17.508356, loss: 11.286152
21:13:20.807571 Step[1500/6995], lr: 0.000100, mv_avg_loss: 16.497816, loss: 12.040980
21:13:25.560514 Step[1600/6995], lr: 0.000100, mv_avg_loss: 16.578669, loss: 14.846350
21:13:30.302938 Step[1700/6995], lr: 0.000100, mv_avg_loss: 15.788042, loss: 21.016445
21:13:35.053827 Step[1800/6995], lr: 0.000100, mv_avg_loss: 15.782163, loss: 18.522968
21:13:39.821130 Step[1900/6995], lr: 0.000100, mv_avg_loss: 15.447080, loss: 15.772383
21:13:44.576311 Step[2000/6995], lr: 0.000100, mv_avg_loss: 17.696175, loss: 18.217026
21:13:49.338766 Step[2100/6995], lr: 0.000100, mv_avg_loss: 17.105978, loss: 13.012626
21:13:54.095330 Step[2200/6995], lr: 0.000100, mv_avg_loss: 15.577027, loss: 11.476906
21:13:58.884039 Step[2300/6995], lr: 0.000100, mv_avg_loss: 15.192872, loss: 14.680437
21:14:03.639927 Step[2400/6995], lr: 0.000100, mv_avg_loss: 15.230146, loss: 16.088230
21:14:08.392144 Step[2500/6995], lr: 0.000100, mv_avg_loss: 16.611990, loss: 22.929508
21:14:13.148647 Step[2600/6995], lr: 0.000100, mv_avg_loss: 15.951078, loss: 13.056267
21:14:17.902285 Step[2700/6995], lr: 0.000100, mv_avg_loss: 17.073650, loss: 22.924007
21:14:22.661017 Step[2800/6995], lr: 0.000100, mv_avg_loss: 16.248953, loss: 20.245279
21:14:27.436973 Step[2900/6995], lr: 0.000100, mv_avg_loss: 16.025940, loss: 20.584354
21:14:32.196245 Step[3000/6995], lr: 0.000100, mv_avg_loss: 16.873564, loss: 13.533504
21:14:36.955260 Step[3100/6995], lr: 0.000100, mv_avg_loss: 16.561371, loss: 17.560028
21:14:41.724495 Step[3200/6995], lr: 0.000100, mv_avg_loss: 14.426986, loss: 11.220458
21:14:46.490303 Step[3300/6995], lr: 0.000100, mv_avg_loss: 15.659146, loss: 10.199028
21:14:51.263632 Step[3400/6995], lr: 0.000100, mv_avg_loss: 16.117922, loss: 14.450327
21:14:56.023379 Step[3500/6995], lr: 0.000100, mv_avg_loss: 15.890819, loss: 15.100807
21:15:00.777155 Step[3600/6995], lr: 0.000100, mv_avg_loss: 15.734486, loss: 13.164957
21:15:05.548872 Step[3700/6995], lr: 0.000100, mv_avg_loss: 14.411477, loss: 21.142429
21:15:10.312512 Step[3800/6995], lr: 0.000100, mv_avg_loss: 15.540991, loss: 21.891394
21:15:15.082967 Step[3900/6995], lr: 0.000100, mv_avg_loss: 15.610175, loss: 22.619139
21:15:19.847168 Step[4000/6995], lr: 0.000100, mv_avg_loss: 16.106302, loss: 13.440570
21:15:24.620449 Step[4100/6995], lr: 0.000100, mv_avg_loss: 15.483021, loss: 12.372256
21:15:29.389333 Step[4200/6995], lr: 0.000100, mv_avg_loss: 15.110183, loss: 13.610874
21:15:34.146139 Step[4300/6995], lr: 0.000100, mv_avg_loss: 15.660483, loss: 8.987553
21:15:38.904351 Step[4400/6995], lr: 0.000100, mv_avg_loss: 14.400314, loss: 13.966903
21:15:43.657914 Step[4500/6995], lr: 0.000100, mv_avg_loss: 16.075819, loss: 18.722698
21:15:48.434455 Step[4600/6995], lr: 0.000100, mv_avg_loss: 15.968310, loss: 15.645049
21:15:53.194036 Step[4700/6995], lr: 0.000100, mv_avg_loss: 16.172602, loss: 10.299202
21:15:57.959856 Step[4800/6995], lr: 0.000100, mv_avg_loss: 15.502251, loss: 17.802837
21:16:02.729507 Step[4900/6995], lr: 0.000100, mv_avg_loss: 16.000269, loss: 12.292872
21:16:07.496284 Step[5000/6995], lr: 0.000100, mv_avg_loss: 16.244738, loss: 11.387917
21:16:12.267589 Step[5100/6995], lr: 0.000100, mv_avg_loss: 15.468791, loss: 9.119093
21:16:17.046475 Step[5200/6995], lr: 0.000100, mv_avg_loss: 16.148891, loss: 17.134842
21:16:21.815128 Step[5300/6995], lr: 0.000100, mv_avg_loss: 15.482164, loss: 11.574234
21:16:26.587859 Step[5400/6995], lr: 0.000100, mv_avg_loss: 15.299650, loss: 13.700439
21:16:31.349513 Step[5500/6995], lr: 0.000100, mv_avg_loss: 16.984226, loss: 18.661789
21:16:36.124473 Step[5600/6995], lr: 0.000100, mv_avg_loss: 15.680057, loss: 12.300720
21:16:40.895871 Step[5700/6995], lr: 0.000100, mv_avg_loss: 15.234179, loss: 18.536812
21:16:45.668111 Step[5800/6995], lr: 0.000100, mv_avg_loss: 16.059982, loss: 9.167238
21:16:50.437912 Step[5900/6995], lr: 0.000100, mv_avg_loss: 15.129046, loss: 12.585844
21:16:55.211855 Step[6000/6995], lr: 0.000100, mv_avg_loss: 15.554130, loss: 16.063185
21:16:59.978857 Step[6100/6995], lr: 0.000100, mv_avg_loss: 17.221090, loss: 12.664196
21:17:04.752707 Step[6200/6995], lr: 0.000100, mv_avg_loss: 15.791793, loss: 10.121577
21:17:09.527944 Step[6300/6995], lr: 0.000100, mv_avg_loss: 15.879818, loss: 12.785208
21:17:14.308550 Step[6400/6995], lr: 0.000100, mv_avg_loss: 16.870611, loss: 18.227482
21:17:19.080970 Step[6500/6995], lr: 0.000100, mv_avg_loss: 15.667233, loss: 16.984131
21:17:23.860680 Step[6600/6995], lr: 0.000100, mv_avg_loss: 15.350830, loss: 10.862761
21:17:28.639951 Step[6700/6995], lr: 0.000100, mv_avg_loss: 16.150259, loss: 17.430161
21:17:33.419233 Step[6800/6995], lr: 0.000100, mv_avg_loss: 15.145678, loss: 14.617369
21:17:38.208731 Step[6900/6995], lr: 0.000100, mv_avg_loss: 15.728788, loss: 11.516832
saving model for epoch 44
Testing for epoch: 44
Average test PNSR is 25.409695 for 500 images
Start to train epoch 45
21:18:04.858930 Step[0/6995], lr: 0.000100, mv_avg_loss: 14.963171, loss: 14.011625
21:18:09.615457 Step[100/6995], lr: 0.000100, mv_avg_loss: 14.741265, loss: 11.699171
21:18:14.384722 Step[200/6995], lr: 0.000100, mv_avg_loss: 16.153812, loss: 21.086294
21:18:19.164695 Step[300/6995], lr: 0.000100, mv_avg_loss: 15.835726, loss: 13.158351
21:18:23.928305 Step[400/6995], lr: 0.000100, mv_avg_loss: 16.376968, loss: 19.769051
21:18:28.695582 Step[500/6995], lr: 0.000100, mv_avg_loss: 15.111970, loss: 19.113605
21:18:33.459967 Step[600/6995], lr: 0.000100, mv_avg_loss: 14.909189, loss: 17.102167
21:18:38.238777 Step[700/6995], lr: 0.000100, mv_avg_loss: 15.901439, loss: 11.974007
21:18:43.004589 Step[800/6995], lr: 0.000100, mv_avg_loss: 16.259724, loss: 21.331390
21:18:47.777980 Step[900/6995], lr: 0.000100, mv_avg_loss: 15.164537, loss: 12.100004
21:18:52.555164 Step[1000/6995], lr: 0.000100, mv_avg_loss: 16.083712, loss: 26.833843
21:18:57.325688 Step[1100/6995], lr: 0.000100, mv_avg_loss: 14.984290, loss: 13.062294
21:19:02.099847 Step[1200/6995], lr: 0.000100, mv_avg_loss: 16.686136, loss: 10.103262
21:19:06.875834 Step[1300/6995], lr: 0.000100, mv_avg_loss: 15.402123, loss: 16.730278
21:19:11.657524 Step[1400/6995], lr: 0.000100, mv_avg_loss: 16.185795, loss: 14.800110
21:19:16.437101 Step[1500/6995], lr: 0.000100, mv_avg_loss: 15.141069, loss: 8.133035
21:19:21.220864 Step[1600/6995], lr: 0.000100, mv_avg_loss: 13.979284, loss: 13.042888
21:19:25.996706 Step[1700/6995], lr: 0.000100, mv_avg_loss: 17.224384, loss: 17.160954
21:19:30.768785 Step[1800/6995], lr: 0.000100, mv_avg_loss: 14.678420, loss: 13.667360
21:19:35.553914 Step[1900/6995], lr: 0.000100, mv_avg_loss: 15.703536, loss: 18.036339
21:19:40.331424 Step[2000/6995], lr: 0.000100, mv_avg_loss: 15.038427, loss: 24.382061
21:19:45.107566 Step[2100/6995], lr: 0.000100, mv_avg_loss: 16.018808, loss: 22.536911
21:19:49.899027 Step[2200/6995], lr: 0.000100, mv_avg_loss: 16.417791, loss: 21.782913
21:19:54.681055 Step[2300/6995], lr: 0.000100, mv_avg_loss: 15.604527, loss: 11.600760
21:19:59.460742 Step[2400/6995], lr: 0.000100, mv_avg_loss: 14.870771, loss: 9.644658
21:20:04.236903 Step[2500/6995], lr: 0.000100, mv_avg_loss: 15.559097, loss: 13.079668
21:20:09.015614 Step[2600/6995], lr: 0.000100, mv_avg_loss: 15.313766, loss: 11.668919
21:20:13.783094 Step[2700/6995], lr: 0.000100, mv_avg_loss: 16.414274, loss: 11.386633
21:20:18.557830 Step[2800/6995], lr: 0.000100, mv_avg_loss: 16.498928, loss: 23.607346
21:20:23.342348 Step[2900/6995], lr: 0.000100, mv_avg_loss: 14.676035, loss: 20.038864
21:20:28.126991 Step[3000/6995], lr: 0.000100, mv_avg_loss: 14.493065, loss: 16.846258
21:20:32.913233 Step[3100/6995], lr: 0.000100, mv_avg_loss: 16.550360, loss: 17.170383
21:20:37.693660 Step[3200/6995], lr: 0.000100, mv_avg_loss: 14.614393, loss: 11.952358
21:20:42.483167 Step[3300/6995], lr: 0.000100, mv_avg_loss: 17.446470, loss: 24.275755
21:20:47.265147 Step[3400/6995], lr: 0.000100, mv_avg_loss: 16.891933, loss: 15.352211
21:20:52.040360 Step[3500/6995], lr: 0.000100, mv_avg_loss: 15.774226, loss: 27.511572
21:20:56.824136 Step[3600/6995], lr: 0.000100, mv_avg_loss: 15.890666, loss: 11.051556
21:21:01.586048 Step[3700/6995], lr: 0.000100, mv_avg_loss: 14.450831, loss: 15.378256
21:21:06.369151 Step[3800/6995], lr: 0.000100, mv_avg_loss: 16.555593, loss: 12.258278
21:21:11.150574 Step[3900/6995], lr: 0.000100, mv_avg_loss: 16.634583, loss: 11.248073
21:21:15.919684 Step[4000/6995], lr: 0.000100, mv_avg_loss: 14.826948, loss: 16.449602
21:21:20.695481 Step[4100/6995], lr: 0.000100, mv_avg_loss: 15.319968, loss: 13.653949
21:21:25.478027 Step[4200/6995], lr: 0.000100, mv_avg_loss: 16.862015, loss: 17.917168
21:21:30.251954 Step[4300/6995], lr: 0.000100, mv_avg_loss: 15.875864, loss: 16.248978
21:21:35.020153 Step[4400/6995], lr: 0.000100, mv_avg_loss: 15.273853, loss: 12.039782
21:21:39.794233 Step[4500/6995], lr: 0.000100, mv_avg_loss: 17.135082, loss: 30.820736
21:21:44.565783 Step[4600/6995], lr: 0.000100, mv_avg_loss: 14.946552, loss: 22.826630
21:21:49.332953 Step[4700/6995], lr: 0.000100, mv_avg_loss: 14.599195, loss: 13.311575
21:21:54.102606 Step[4800/6995], lr: 0.000100, mv_avg_loss: 15.844266, loss: 15.843815
21:21:58.866758 Step[4900/6995], lr: 0.000100, mv_avg_loss: 15.829137, loss: 31.099010
21:22:03.646518 Step[5000/6995], lr: 0.000100, mv_avg_loss: 14.717824, loss: 16.126076
21:22:08.417639 Step[5100/6995], lr: 0.000100, mv_avg_loss: 15.808147, loss: 13.627131
21:22:13.182761 Step[5200/6995], lr: 0.000100, mv_avg_loss: 15.788026, loss: 8.589500
21:22:17.968836 Step[5300/6995], lr: 0.000100, mv_avg_loss: 15.212408, loss: 15.229609
21:22:22.738412 Step[5400/6995], lr: 0.000100, mv_avg_loss: 17.094080, loss: 17.544605
21:22:27.513591 Step[5500/6995], lr: 0.000100, mv_avg_loss: 16.285112, loss: 10.623199
21:22:32.286504 Step[5600/6995], lr: 0.000100, mv_avg_loss: 16.579954, loss: 12.077276
21:22:37.052422 Step[5700/6995], lr: 0.000100, mv_avg_loss: 16.063883, loss: 14.188927
21:22:41.809050 Step[5800/6995], lr: 0.000100, mv_avg_loss: 14.677013, loss: 22.194372
21:22:46.581722 Step[5900/6995], lr: 0.000100, mv_avg_loss: 15.037984, loss: 17.564260
21:22:51.340059 Step[6000/6995], lr: 0.000100, mv_avg_loss: 15.856615, loss: 26.474630
21:22:56.105785 Step[6100/6995], lr: 0.000100, mv_avg_loss: 15.359861, loss: 25.492559
21:23:00.876701 Step[6200/6995], lr: 0.000100, mv_avg_loss: 15.470808, loss: 20.077011
21:23:05.651448 Step[6300/6995], lr: 0.000100, mv_avg_loss: 16.278311, loss: 13.950216
21:23:10.424162 Step[6400/6995], lr: 0.000100, mv_avg_loss: 15.159839, loss: 10.804853
21:23:15.187049 Step[6500/6995], lr: 0.000100, mv_avg_loss: 16.115887, loss: 10.826567
21:23:19.950310 Step[6600/6995], lr: 0.000100, mv_avg_loss: 14.326775, loss: 14.951426
21:23:24.720693 Step[6700/6995], lr: 0.000100, mv_avg_loss: 14.622621, loss: 12.312411
21:23:29.495679 Step[6800/6995], lr: 0.000100, mv_avg_loss: 15.708529, loss: 18.937458
21:23:34.281754 Step[6900/6995], lr: 0.000100, mv_avg_loss: 14.730333, loss: 14.706164
Testing for epoch: 45
Average test PNSR is 25.413240 for 500 images
Start to train epoch 46
21:24:01.234338 Step[0/6995], lr: 0.000100, mv_avg_loss: 15.007010, loss: 14.828070
21:24:06.000124 Step[100/6995], lr: 0.000100, mv_avg_loss: 14.786730, loss: 17.916611
21:24:10.779525 Step[200/6995], lr: 0.000100, mv_avg_loss: 16.006557, loss: 16.509064
21:24:15.547117 Step[300/6995], lr: 0.000100, mv_avg_loss: 16.125353, loss: 26.979528
21:24:20.331734 Step[400/6995], lr: 0.000100, mv_avg_loss: 15.185781, loss: 16.735723
21:24:25.104161 Step[500/6995], lr: 0.000100, mv_avg_loss: 15.414309, loss: 36.157047
21:24:29.881058 Step[600/6995], lr: 0.000100, mv_avg_loss: 15.556373, loss: 22.817556
21:24:34.660038 Step[700/6995], lr: 0.000100, mv_avg_loss: 15.431264, loss: 17.716093
21:24:39.435288 Step[800/6995], lr: 0.000100, mv_avg_loss: 15.781186, loss: 14.163356
21:24:44.230347 Step[900/6995], lr: 0.000100, mv_avg_loss: 15.422610, loss: 12.976944
21:24:49.012761 Step[1000/6995], lr: 0.000100, mv_avg_loss: 16.888784, loss: 24.642559
21:24:53.792043 Step[1100/6995], lr: 0.000100, mv_avg_loss: 14.360649, loss: 12.470739
21:24:58.577389 Step[1200/6995], lr: 0.000100, mv_avg_loss: 14.749688, loss: 10.401201
21:25:03.354949 Step[1300/6995], lr: 0.000100, mv_avg_loss: 15.490410, loss: 18.191174
21:25:08.130217 Step[1400/6995], lr: 0.000100, mv_avg_loss: 16.012302, loss: 29.691313
21:25:12.900751 Step[1500/6995], lr: 0.000100, mv_avg_loss: 15.314634, loss: 13.493383
21:25:17.673391 Step[1600/6995], lr: 0.000100, mv_avg_loss: 15.005358, loss: 17.590370
21:25:22.455492 Step[1700/6995], lr: 0.000100, mv_avg_loss: 15.316311, loss: 15.836191
21:25:27.233041 Step[1800/6995], lr: 0.000100, mv_avg_loss: 15.572249, loss: 11.264712
21:25:32.005893 Step[1900/6995], lr: 0.000100, mv_avg_loss: 15.641820, loss: 25.685261
21:25:36.779936 Step[2000/6995], lr: 0.000100, mv_avg_loss: 14.598159, loss: 9.815340
21:25:41.553369 Step[2100/6995], lr: 0.000100, mv_avg_loss: 15.633101, loss: 15.476276
21:25:46.319188 Step[2200/6995], lr: 0.000100, mv_avg_loss: 16.032679, loss: 16.541302
21:25:51.093543 Step[2300/6995], lr: 0.000100, mv_avg_loss: 17.329260, loss: 21.406445
21:25:55.868545 Step[2400/6995], lr: 0.000100, mv_avg_loss: 15.184096, loss: 8.756598
21:26:00.656565 Step[2500/6995], lr: 0.000100, mv_avg_loss: 15.271052, loss: 23.593084
21:26:05.436042 Step[2600/6995], lr: 0.000100, mv_avg_loss: 15.440836, loss: 11.800398
21:26:10.202702 Step[2700/6995], lr: 0.000100, mv_avg_loss: 15.580658, loss: 16.172245
21:26:14.975274 Step[2800/6995], lr: 0.000100, mv_avg_loss: 14.341349, loss: 14.718222
21:26:19.760060 Step[2900/6995], lr: 0.000100, mv_avg_loss: 15.453664, loss: 14.187488
21:26:24.536390 Step[3000/6995], lr: 0.000100, mv_avg_loss: 15.040941, loss: 10.799773
21:26:29.293963 Step[3100/6995], lr: 0.000100, mv_avg_loss: 15.497304, loss: 12.264471
21:26:34.077854 Step[3200/6995], lr: 0.000100, mv_avg_loss: 15.046760, loss: 11.781628
21:26:38.875274 Step[3300/6995], lr: 0.000100, mv_avg_loss: 15.268433, loss: 13.686338
21:26:43.657150 Step[3400/6995], lr: 0.000100, mv_avg_loss: 15.404254, loss: 23.062847
21:26:48.434167 Step[3500/6995], lr: 0.000100, mv_avg_loss: 17.004070, loss: 16.842648
21:26:53.211087 Step[3600/6995], lr: 0.000100, mv_avg_loss: 15.636056, loss: 8.402080
21:26:57.994376 Step[3700/6995], lr: 0.000100, mv_avg_loss: 15.174384, loss: 15.941708
21:27:02.772898 Step[3800/6995], lr: 0.000100, mv_avg_loss: 14.960808, loss: 12.157478
21:27:07.547704 Step[3900/6995], lr: 0.000100, mv_avg_loss: 16.409798, loss: 12.133498
21:27:12.323929 Step[4000/6995], lr: 0.000100, mv_avg_loss: 17.869602, loss: 35.853489
21:27:17.106905 Step[4100/6995], lr: 0.000100, mv_avg_loss: 16.458017, loss: 15.740313
21:27:21.886719 Step[4200/6995], lr: 0.000100, mv_avg_loss: 15.294440, loss: 11.905609
21:27:26.668743 Step[4300/6995], lr: 0.000100, mv_avg_loss: 15.599228, loss: 17.249081
21:27:31.446652 Step[4400/6995], lr: 0.000100, mv_avg_loss: 15.921588, loss: 15.997706
21:27:36.237252 Step[4500/6995], lr: 0.000100, mv_avg_loss: 16.333910, loss: 14.570961
21:27:41.021569 Step[4600/6995], lr: 0.000100, mv_avg_loss: 14.879179, loss: 14.287352
21:27:45.807686 Step[4700/6995], lr: 0.000100, mv_avg_loss: 16.417418, loss: 11.511752
21:27:50.589786 Step[4800/6995], lr: 0.000100, mv_avg_loss: 14.613972, loss: 8.502924
21:27:55.369705 Step[4900/6995], lr: 0.000100, mv_avg_loss: 14.165066, loss: 15.055288
21:28:00.156459 Step[5000/6995], lr: 0.000100, mv_avg_loss: 15.541382, loss: 9.847728
21:28:04.935855 Step[5100/6995], lr: 0.000100, mv_avg_loss: 15.933439, loss: 24.079983
21:28:09.718143 Step[5200/6995], lr: 0.000100, mv_avg_loss: 15.431812, loss: 12.953001
21:28:14.515476 Step[5300/6995], lr: 0.000100, mv_avg_loss: 13.886958, loss: 13.257416
21:28:19.299699 Step[5400/6995], lr: 0.000100, mv_avg_loss: 14.835297, loss: 26.033215
21:28:24.090901 Step[5500/6995], lr: 0.000100, mv_avg_loss: 15.160131, loss: 14.343252
21:28:28.874285 Step[5600/6995], lr: 0.000100, mv_avg_loss: 16.889225, loss: 28.576782
21:28:33.663722 Step[5700/6995], lr: 0.000100, mv_avg_loss: 15.833798, loss: 12.052366
21:28:38.441474 Step[5800/6995], lr: 0.000100, mv_avg_loss: 15.186715, loss: 17.641062
21:28:43.226812 Step[5900/6995], lr: 0.000100, mv_avg_loss: 16.512367, loss: 27.778587
21:28:48.012733 Step[6000/6995], lr: 0.000100, mv_avg_loss: 15.204001, loss: 15.031530
21:28:52.795902 Step[6100/6995], lr: 0.000100, mv_avg_loss: 16.259903, loss: 12.145193
21:28:57.571828 Step[6200/6995], lr: 0.000100, mv_avg_loss: 15.146598, loss: 15.925039
21:29:02.347220 Step[6300/6995], lr: 0.000100, mv_avg_loss: 15.138919, loss: 14.945819
21:29:07.131529 Step[6400/6995], lr: 0.000100, mv_avg_loss: 15.857576, loss: 20.701193
21:29:11.898019 Step[6500/6995], lr: 0.000100, mv_avg_loss: 14.689540, loss: 11.506190
21:29:16.673447 Step[6600/6995], lr: 0.000100, mv_avg_loss: 15.166636, loss: 14.315715
21:29:21.459154 Step[6700/6995], lr: 0.000100, mv_avg_loss: 14.760149, loss: 21.780041
21:29:26.240103 Step[6800/6995], lr: 0.000100, mv_avg_loss: 14.817034, loss: 18.668060
21:29:31.042533 Step[6900/6995], lr: 0.000100, mv_avg_loss: 14.451713, loss: 9.065275
Testing for epoch: 46
Average test PNSR is 25.388399 for 500 images
Start to train epoch 47
21:29:58.111507 Step[0/6995], lr: 0.000100, mv_avg_loss: 15.048820, loss: 9.375644
21:30:02.881682 Step[100/6995], lr: 0.000100, mv_avg_loss: 14.541030, loss: 16.583719
21:30:07.664110 Step[200/6995], lr: 0.000100, mv_avg_loss: 16.270233, loss: 48.059795
21:30:12.431879 Step[300/6995], lr: 0.000100, mv_avg_loss: 15.253486, loss: 13.285002
21:30:17.213706 Step[400/6995], lr: 0.000100, mv_avg_loss: 14.968238, loss: 8.889444
21:30:21.994005 Step[500/6995], lr: 0.000100, mv_avg_loss: 14.795338, loss: 11.561130
21:30:26.766527 Step[600/6995], lr: 0.000100, mv_avg_loss: 15.758780, loss: 23.536413
21:30:31.555801 Step[700/6995], lr: 0.000100, mv_avg_loss: 16.209339, loss: 16.555542
21:30:36.335502 Step[800/6995], lr: 0.000100, mv_avg_loss: 15.706614, loss: 14.669971
21:30:41.117901 Step[900/6995], lr: 0.000100, mv_avg_loss: 14.172217, loss: 19.046780
21:30:45.899729 Step[1000/6995], lr: 0.000100, mv_avg_loss: 15.694997, loss: 24.781818
21:30:50.690733 Step[1100/6995], lr: 0.000100, mv_avg_loss: 16.185816, loss: 11.044456
21:30:55.477534 Step[1200/6995], lr: 0.000100, mv_avg_loss: 14.956432, loss: 14.041236
21:31:00.283080 Step[1300/6995], lr: 0.000100, mv_avg_loss: 14.275260, loss: 10.479278
21:31:05.063509 Step[1400/6995], lr: 0.000100, mv_avg_loss: 16.570751, loss: 18.635689
21:31:09.848908 Step[1500/6995], lr: 0.000100, mv_avg_loss: 16.422049, loss: 23.292934
21:31:14.643645 Step[1600/6995], lr: 0.000100, mv_avg_loss: 15.960391, loss: 12.895582
21:31:19.447373 Step[1700/6995], lr: 0.000100, mv_avg_loss: 15.377382, loss: 17.522455
21:31:24.233668 Step[1800/6995], lr: 0.000100, mv_avg_loss: 15.607584, loss: 13.883838
21:31:29.028771 Step[1900/6995], lr: 0.000100, mv_avg_loss: 14.335070, loss: 14.257625
21:31:33.815592 Step[2000/6995], lr: 0.000100, mv_avg_loss: 16.532347, loss: 29.324341
21:31:38.604164 Step[2100/6995], lr: 0.000100, mv_avg_loss: 16.177172, loss: 14.838408
21:31:43.406432 Step[2200/6995], lr: 0.000100, mv_avg_loss: 15.178401, loss: 18.271263
21:31:48.208008 Step[2300/6995], lr: 0.000100, mv_avg_loss: 14.906628, loss: 15.139419
21:31:53.002930 Step[2400/6995], lr: 0.000100, mv_avg_loss: 16.020477, loss: 12.759146
21:31:57.805328 Step[2500/6995], lr: 0.000100, mv_avg_loss: 14.735454, loss: 10.663143
21:32:02.607742 Step[2600/6995], lr: 0.000100, mv_avg_loss: 15.090201, loss: 11.815229
21:32:07.397876 Step[2700/6995], lr: 0.000100, mv_avg_loss: 14.499956, loss: 15.187790
21:32:12.198334 Step[2800/6995], lr: 0.000100, mv_avg_loss: 14.789783, loss: 33.367752
21:32:16.999119 Step[2900/6995], lr: 0.000100, mv_avg_loss: 15.494939, loss: 14.968925
21:32:21.799388 Step[3000/6995], lr: 0.000100, mv_avg_loss: 15.363073, loss: 16.660936
21:32:26.610177 Step[3100/6995], lr: 0.000100, mv_avg_loss: 15.471680, loss: 10.930048
21:32:31.393598 Step[3200/6995], lr: 0.000100, mv_avg_loss: 15.544888, loss: 20.352251
21:32:36.204346 Step[3300/6995], lr: 0.000100, mv_avg_loss: 15.293758, loss: 25.316412
21:32:41.009667 Step[3400/6995], lr: 0.000100, mv_avg_loss: 14.975711, loss: 14.912243
21:32:45.823468 Step[3500/6995], lr: 0.000100, mv_avg_loss: 15.016580, loss: 11.554815
21:32:50.635123 Step[3600/6995], lr: 0.000100, mv_avg_loss: 15.063563, loss: 11.873297
21:32:55.434521 Step[3700/6995], lr: 0.000100, mv_avg_loss: 16.050331, loss: 23.593449
21:33:00.241998 Step[3800/6995], lr: 0.000100, mv_avg_loss: 15.542238, loss: 10.737956
21:33:05.048758 Step[3900/6995], lr: 0.000100, mv_avg_loss: 15.798656, loss: 22.389036
21:33:09.850633 Step[4000/6995], lr: 0.000100, mv_avg_loss: 14.478656, loss: 16.226660
21:33:14.654105 Step[4100/6995], lr: 0.000100, mv_avg_loss: 14.469195, loss: 14.455431
21:33:19.452376 Step[4200/6995], lr: 0.000100, mv_avg_loss: 15.726998, loss: 10.006307
21:33:24.251627 Step[4300/6995], lr: 0.000100, mv_avg_loss: 16.651316, loss: 11.146617
21:33:29.048500 Step[4400/6995], lr: 0.000100, mv_avg_loss: 13.879425, loss: 20.266747
21:33:33.859422 Step[4500/6995], lr: 0.000100, mv_avg_loss: 16.380119, loss: 12.722186
21:33:38.666367 Step[4600/6995], lr: 0.000100, mv_avg_loss: 16.167345, loss: 16.993296
21:33:43.471779 Step[4700/6995], lr: 0.000100, mv_avg_loss: 14.869355, loss: 21.466381
21:33:48.290394 Step[4800/6995], lr: 0.000100, mv_avg_loss: 13.992103, loss: 17.852093
21:33:53.102113 Step[4900/6995], lr: 0.000100, mv_avg_loss: 15.347409, loss: 9.224062
21:33:57.903514 Step[5000/6995], lr: 0.000100, mv_avg_loss: 14.790264, loss: 19.337399
21:34:02.717103 Step[5100/6995], lr: 0.000100, mv_avg_loss: 16.368729, loss: 18.329691
21:34:07.533321 Step[5200/6995], lr: 0.000100, mv_avg_loss: 15.953461, loss: 10.085545
21:34:12.365514 Step[5300/6995], lr: 0.000100, mv_avg_loss: 15.306284, loss: 9.862013
21:34:17.184652 Step[5400/6995], lr: 0.000100, mv_avg_loss: 17.160814, loss: 17.706882
21:34:21.988055 Step[5500/6995], lr: 0.000100, mv_avg_loss: 13.531364, loss: 11.652278
21:34:26.789981 Step[5600/6995], lr: 0.000100, mv_avg_loss: 15.304645, loss: 14.529846
21:34:31.596421 Step[5700/6995], lr: 0.000100, mv_avg_loss: 15.358649, loss: 15.626348
21:34:36.408013 Step[5800/6995], lr: 0.000100, mv_avg_loss: 15.389221, loss: 9.972930
21:34:41.232036 Step[5900/6995], lr: 0.000100, mv_avg_loss: 15.303897, loss: 14.586386
21:34:46.057187 Step[6000/6995], lr: 0.000100, mv_avg_loss: 14.770582, loss: 18.111401
21:34:50.871309 Step[6100/6995], lr: 0.000100, mv_avg_loss: 14.604892, loss: 8.682869
21:34:55.681756 Step[6200/6995], lr: 0.000100, mv_avg_loss: 15.884177, loss: 12.253159
21:35:00.511796 Step[6300/6995], lr: 0.000100, mv_avg_loss: 15.937786, loss: 15.874767
21:35:05.316393 Step[6400/6995], lr: 0.000100, mv_avg_loss: 15.927396, loss: 13.512492
21:35:10.117931 Step[6500/6995], lr: 0.000100, mv_avg_loss: 14.245701, loss: 12.029620
21:35:14.923312 Step[6600/6995], lr: 0.000100, mv_avg_loss: 15.259044, loss: 11.419989
21:35:19.716112 Step[6700/6995], lr: 0.000100, mv_avg_loss: 15.299387, loss: 11.480780
21:35:24.527156 Step[6800/6995], lr: 0.000100, mv_avg_loss: 15.133638, loss: 15.723865
21:35:29.340498 Step[6900/6995], lr: 0.000100, mv_avg_loss: 14.779770, loss: 16.111900
Testing for epoch: 47
Average test PNSR is 25.447340 for 500 images
Start to train epoch 48
21:35:55.952922 Step[0/6995], lr: 0.000100, mv_avg_loss: 14.656367, loss: 11.709961
21:36:00.745157 Step[100/6995], lr: 0.000100, mv_avg_loss: 15.313396, loss: 11.248354
21:36:05.540856 Step[200/6995], lr: 0.000100, mv_avg_loss: 14.314260, loss: 18.677324
21:36:10.328104 Step[300/6995], lr: 0.000100, mv_avg_loss: 15.975856, loss: 13.062737
21:36:15.121052 Step[400/6995], lr: 0.000100, mv_avg_loss: 15.850503, loss: 16.865265
21:36:19.920483 Step[500/6995], lr: 0.000100, mv_avg_loss: 14.170812, loss: 10.959391
21:36:24.719941 Step[600/6995], lr: 0.000100, mv_avg_loss: 14.637733, loss: 13.957630
21:36:29.517938 Step[700/6995], lr: 0.000100, mv_avg_loss: 14.456025, loss: 19.354462
21:36:34.311933 Step[800/6995], lr: 0.000100, mv_avg_loss: 16.372374, loss: 14.373852
21:36:39.117786 Step[900/6995], lr: 0.000100, mv_avg_loss: 15.538049, loss: 12.812233
21:36:43.910644 Step[1000/6995], lr: 0.000100, mv_avg_loss: 15.260361, loss: 9.590159
21:36:48.719336 Step[1100/6995], lr: 0.000100, mv_avg_loss: 14.728356, loss: 13.244661
21:36:53.512714 Step[1200/6995], lr: 0.000100, mv_avg_loss: 15.594974, loss: 10.676687
21:36:58.304250 Step[1300/6995], lr: 0.000100, mv_avg_loss: 15.726459, loss: 13.403071
21:37:03.098536 Step[1400/6995], lr: 0.000100, mv_avg_loss: 14.384434, loss: 24.763912
21:37:07.887305 Step[1500/6995], lr: 0.000100, mv_avg_loss: 14.502025, loss: 10.943736
21:37:12.675044 Step[1600/6995], lr: 0.000100, mv_avg_loss: 14.879334, loss: 15.803305
21:37:17.476111 Step[1700/6995], lr: 0.000100, mv_avg_loss: 14.765657, loss: 15.529858
21:37:22.282922 Step[1800/6995], lr: 0.000100, mv_avg_loss: 16.237787, loss: 11.812582
21:37:27.073827 Step[1900/6995], lr: 0.000100, mv_avg_loss: 14.363558, loss: 12.702276
21:37:31.872837 Step[2000/6995], lr: 0.000100, mv_avg_loss: 16.063995, loss: 10.820132
21:37:36.667882 Step[2100/6995], lr: 0.000100, mv_avg_loss: 15.219630, loss: 18.049637
21:37:41.460567 Step[2200/6995], lr: 0.000100, mv_avg_loss: 15.349114, loss: 33.820210
21:37:46.251491 Step[2300/6995], lr: 0.000100, mv_avg_loss: 14.606151, loss: 18.567701
21:37:51.049754 Step[2400/6995], lr: 0.000100, mv_avg_loss: 15.741700, loss: 9.534810
21:37:55.838555 Step[2500/6995], lr: 0.000100, mv_avg_loss: 14.038445, loss: 14.426356
21:38:00.646959 Step[2600/6995], lr: 0.000100, mv_avg_loss: 14.500655, loss: 13.449150
21:38:05.441024 Step[2700/6995], lr: 0.000100, mv_avg_loss: 15.589510, loss: 12.591995
21:38:10.236223 Step[2800/6995], lr: 0.000100, mv_avg_loss: 16.160954, loss: 13.406002
21:38:15.015844 Step[2900/6995], lr: 0.000100, mv_avg_loss: 16.322844, loss: 15.263737
21:38:19.815351 Step[3000/6995], lr: 0.000100, mv_avg_loss: 15.017998, loss: 17.081533
21:38:24.596439 Step[3100/6995], lr: 0.000100, mv_avg_loss: 14.936800, loss: 11.259705
21:38:29.383914 Step[3200/6995], lr: 0.000100, mv_avg_loss: 15.760202, loss: 14.133900
21:38:34.166149 Step[3300/6995], lr: 0.000100, mv_avg_loss: 14.586757, loss: 21.050255
21:38:38.953763 Step[3400/6995], lr: 0.000100, mv_avg_loss: 15.592412, loss: 13.675591
21:38:43.745508 Step[3500/6995], lr: 0.000100, mv_avg_loss: 15.254347, loss: 12.724928
21:38:48.529560 Step[3600/6995], lr: 0.000100, mv_avg_loss: 15.174846, loss: 10.944900
21:38:53.323533 Step[3700/6995], lr: 0.000100, mv_avg_loss: 15.091435, loss: 13.880111
21:38:58.122376 Step[3800/6995], lr: 0.000100, mv_avg_loss: 15.382516, loss: 20.548523
21:39:02.924195 Step[3900/6995], lr: 0.000100, mv_avg_loss: 14.892480, loss: 26.134214
21:39:07.706734 Step[4000/6995], lr: 0.000100, mv_avg_loss: 15.104977, loss: 10.546792
21:39:12.500716 Step[4100/6995], lr: 0.000100, mv_avg_loss: 14.378324, loss: 9.344049
21:39:17.296078 Step[4200/6995], lr: 0.000100, mv_avg_loss: 14.766338, loss: 21.438803
21:39:22.106938 Step[4300/6995], lr: 0.000100, mv_avg_loss: 14.995807, loss: 30.070822
21:39:26.895970 Step[4400/6995], lr: 0.000100, mv_avg_loss: 14.489080, loss: 13.942581
21:39:31.681483 Step[4500/6995], lr: 0.000100, mv_avg_loss: 14.718184, loss: 11.090719
21:39:36.455752 Step[4600/6995], lr: 0.000100, mv_avg_loss: 14.876491, loss: 22.606886
21:39:41.229793 Step[4700/6995], lr: 0.000100, mv_avg_loss: 16.451452, loss: 12.063398
21:39:46.021212 Step[4800/6995], lr: 0.000100, mv_avg_loss: 15.966459, loss: 24.805695
21:39:50.800327 Step[4900/6995], lr: 0.000100, mv_avg_loss: 14.371243, loss: 17.852615
21:39:55.586044 Step[5000/6995], lr: 0.000100, mv_avg_loss: 14.901773, loss: 16.621773
21:40:00.386730 Step[5100/6995], lr: 0.000100, mv_avg_loss: 15.794499, loss: 14.086726
21:40:05.171380 Step[5200/6995], lr: 0.000100, mv_avg_loss: 15.801035, loss: 28.602688
21:40:09.961167 Step[5300/6995], lr: 0.000100, mv_avg_loss: 15.329543, loss: 22.175014
21:40:14.746147 Step[5400/6995], lr: 0.000100, mv_avg_loss: 13.904975, loss: 17.823128
21:40:19.525510 Step[5500/6995], lr: 0.000100, mv_avg_loss: 15.679801, loss: 11.483601
21:40:24.300580 Step[5600/6995], lr: 0.000100, mv_avg_loss: 14.379018, loss: 11.261505
21:40:29.083893 Step[5700/6995], lr: 0.000100, mv_avg_loss: 15.489766, loss: 14.301513
21:40:33.870122 Step[5800/6995], lr: 0.000100, mv_avg_loss: 14.220360, loss: 11.557930
21:40:38.651613 Step[5900/6995], lr: 0.000100, mv_avg_loss: 16.159296, loss: 10.528522
21:40:43.426221 Step[6000/6995], lr: 0.000100, mv_avg_loss: 14.961104, loss: 24.199951
21:40:48.207462 Step[6100/6995], lr: 0.000100, mv_avg_loss: 15.400938, loss: 27.818380
21:40:53.001949 Step[6200/6995], lr: 0.000100, mv_avg_loss: 15.903702, loss: 29.754797
21:40:57.787501 Step[6300/6995], lr: 0.000100, mv_avg_loss: 15.435946, loss: 10.029411
21:41:02.800014 Step[6400/6995], lr: 0.000100, mv_avg_loss: 15.348153, loss: 13.423866
21:41:07.576186 Step[6500/6995], lr: 0.000100, mv_avg_loss: 14.191024, loss: 15.429538
21:41:12.366156 Step[6600/6995], lr: 0.000100, mv_avg_loss: 13.846881, loss: 15.510317
21:41:17.182178 Step[6700/6995], lr: 0.000100, mv_avg_loss: 14.854795, loss: 10.560359
21:41:21.961926 Step[6800/6995], lr: 0.000100, mv_avg_loss: 15.353387, loss: 17.100203
21:41:26.750291 Step[6900/6995], lr: 0.000100, mv_avg_loss: 16.030718, loss: 16.353783
Testing for epoch: 48
Average test PNSR is 25.374531 for 500 images
Start to train epoch 49
21:41:53.875529 Step[0/6995], lr: 0.000100, mv_avg_loss: 15.613323, loss: 13.518553
21:41:58.627026 Step[100/6995], lr: 0.000100, mv_avg_loss: 13.628636, loss: 12.540503
21:42:03.379835 Step[200/6995], lr: 0.000100, mv_avg_loss: 15.015259, loss: 15.822792
21:42:08.131147 Step[300/6995], lr: 0.000100, mv_avg_loss: 15.988875, loss: 13.172847
21:42:12.895398 Step[400/6995], lr: 0.000100, mv_avg_loss: 14.656037, loss: 15.060783
21:42:17.659558 Step[500/6995], lr: 0.000100, mv_avg_loss: 14.181538, loss: 19.270439
21:42:22.419975 Step[600/6995], lr: 0.000100, mv_avg_loss: 14.245402, loss: 17.810921
21:42:27.184305 Step[700/6995], lr: 0.000100, mv_avg_loss: 15.658443, loss: 15.816730
21:42:31.937065 Step[800/6995], lr: 0.000100, mv_avg_loss: 15.904303, loss: 12.949000
21:42:36.698538 Step[900/6995], lr: 0.000100, mv_avg_loss: 14.988873, loss: 18.420444
21:42:41.466279 Step[1000/6995], lr: 0.000100, mv_avg_loss: 15.316897, loss: 17.454453
21:42:46.231362 Step[1100/6995], lr: 0.000100, mv_avg_loss: 14.763912, loss: 9.693122
21:42:50.996038 Step[1200/6995], lr: 0.000100, mv_avg_loss: 14.794309, loss: 10.918104
21:42:55.758613 Step[1300/6995], lr: 0.000100, mv_avg_loss: 14.593070, loss: 13.941558
21:43:00.539427 Step[1400/6995], lr: 0.000100, mv_avg_loss: 14.977627, loss: 10.227482
21:43:05.327465 Step[1500/6995], lr: 0.000100, mv_avg_loss: 13.699739, loss: 13.463088
21:43:10.111035 Step[1600/6995], lr: 0.000100, mv_avg_loss: 14.893338, loss: 15.383697
21:43:14.896442 Step[1700/6995], lr: 0.000100, mv_avg_loss: 14.331339, loss: 16.161411
21:43:19.681857 Step[1800/6995], lr: 0.000100, mv_avg_loss: 13.922314, loss: 25.430059
21:43:24.467666 Step[1900/6995], lr: 0.000100, mv_avg_loss: 13.890518, loss: 21.363243
21:43:29.251539 Step[2000/6995], lr: 0.000100, mv_avg_loss: 16.006392, loss: 10.261506
21:43:34.024129 Step[2100/6995], lr: 0.000100, mv_avg_loss: 14.536283, loss: 14.924591
21:43:38.798398 Step[2200/6995], lr: 0.000100, mv_avg_loss: 14.841756, loss: 15.883940
21:43:43.604704 Step[2300/6995], lr: 0.000100, mv_avg_loss: 16.688503, loss: 14.016612
21:43:48.398540 Step[2400/6995], lr: 0.000100, mv_avg_loss: 13.479464, loss: 18.497526
21:43:53.175468 Step[2500/6995], lr: 0.000100, mv_avg_loss: 13.537499, loss: 7.861095
21:43:57.957036 Step[2600/6995], lr: 0.000100, mv_avg_loss: 14.675830, loss: 15.697926
21:44:02.738384 Step[2700/6995], lr: 0.000100, mv_avg_loss: 15.041153, loss: 9.777728
21:44:07.528971 Step[2800/6995], lr: 0.000100, mv_avg_loss: 14.856879, loss: 12.843885
21:44:12.325877 Step[2900/6995], lr: 0.000100, mv_avg_loss: 15.012888, loss: 15.891653
21:44:17.108025 Step[3000/6995], lr: 0.000100, mv_avg_loss: 15.705345, loss: 24.018337
21:44:21.902545 Step[3100/6995], lr: 0.000100, mv_avg_loss: 14.112341, loss: 11.302276
21:44:26.696437 Step[3200/6995], lr: 0.000100, mv_avg_loss: 15.280068, loss: 22.430004
21:44:31.483989 Step[3300/6995], lr: 0.000100, mv_avg_loss: 14.910919, loss: 11.621542
21:44:36.271279 Step[3400/6995], lr: 0.000100, mv_avg_loss: 15.788545, loss: 17.612103
21:44:41.052470 Step[3500/6995], lr: 0.000100, mv_avg_loss: 15.192467, loss: 19.636517
21:44:45.840274 Step[3600/6995], lr: 0.000100, mv_avg_loss: 14.930555, loss: 12.914968
21:44:50.633894 Step[3700/6995], lr: 0.000100, mv_avg_loss: 16.063164, loss: 18.296467
21:44:55.432848 Step[3800/6995], lr: 0.000100, mv_avg_loss: 15.077321, loss: 15.022461
21:45:00.227912 Step[3900/6995], lr: 0.000100, mv_avg_loss: 15.041635, loss: 16.279198
21:45:05.017203 Step[4000/6995], lr: 0.000100, mv_avg_loss: 15.034095, loss: 12.048937
21:45:09.948092 Step[4100/6995], lr: 0.000100, mv_avg_loss: 14.282497, loss: 17.084347
21:45:14.722423 Step[4200/6995], lr: 0.000100, mv_avg_loss: 15.257751, loss: 10.324758
21:45:19.506761 Step[4300/6995], lr: 0.000100, mv_avg_loss: 15.340425, loss: 14.662434
21:45:24.291502 Step[4400/6995], lr: 0.000100, mv_avg_loss: 15.054450, loss: 14.359801
21:45:29.080647 Step[4500/6995], lr: 0.000100, mv_avg_loss: 16.004761, loss: 15.202068
21:45:33.880183 Step[4600/6995], lr: 0.000100, mv_avg_loss: 14.678633, loss: 8.882795
21:45:38.669807 Step[4700/6995], lr: 0.000100, mv_avg_loss: 15.279416, loss: 11.382484
21:45:43.468850 Step[4800/6995], lr: 0.000100, mv_avg_loss: 16.182022, loss: 23.401714
21:45:48.272242 Step[4900/6995], lr: 0.000100, mv_avg_loss: 14.928953, loss: 10.242110
21:45:53.065314 Step[5000/6995], lr: 0.000100, mv_avg_loss: 15.732651, loss: 21.221806
21:45:57.863626 Step[5100/6995], lr: 0.000100, mv_avg_loss: 15.120316, loss: 10.052508
21:46:02.664076 Step[5200/6995], lr: 0.000100, mv_avg_loss: 15.438211, loss: 15.076925
21:46:07.456580 Step[5300/6995], lr: 0.000100, mv_avg_loss: 14.534474, loss: 15.630626
21:46:12.253372 Step[5400/6995], lr: 0.000100, mv_avg_loss: 14.443741, loss: 11.544276
21:46:17.042803 Step[5500/6995], lr: 0.000100, mv_avg_loss: 14.166540, loss: 20.380964
21:46:21.841869 Step[5600/6995], lr: 0.000100, mv_avg_loss: 15.170694, loss: 13.582031
21:46:26.638531 Step[5700/6995], lr: 0.000100, mv_avg_loss: 16.433073, loss: 10.865447
21:46:31.437451 Step[5800/6995], lr: 0.000100, mv_avg_loss: 15.142898, loss: 16.453905
21:46:36.233583 Step[5900/6995], lr: 0.000100, mv_avg_loss: 14.776705, loss: 14.551115
21:46:41.044821 Step[6000/6995], lr: 0.000100, mv_avg_loss: 15.643463, loss: 11.149874
21:46:45.850083 Step[6100/6995], lr: 0.000100, mv_avg_loss: 14.417760, loss: 7.974841
21:46:50.653157 Step[6200/6995], lr: 0.000100, mv_avg_loss: 14.336575, loss: 16.858175
21:46:55.470417 Step[6300/6995], lr: 0.000100, mv_avg_loss: 16.404383, loss: 12.631480
21:47:00.289274 Step[6400/6995], lr: 0.000100, mv_avg_loss: 15.307288, loss: 9.414517
21:47:05.091753 Step[6500/6995], lr: 0.000100, mv_avg_loss: 15.346159, loss: 15.457870
21:47:09.903929 Step[6600/6995], lr: 0.000100, mv_avg_loss: 15.419776, loss: 14.506049
21:47:14.708058 Step[6700/6995], lr: 0.000100, mv_avg_loss: 14.117371, loss: 12.881869
21:47:19.499504 Step[6800/6995], lr: 0.000100, mv_avg_loss: 14.602383, loss: 14.845720
21:47:24.300172 Step[6900/6995], lr: 0.000100, mv_avg_loss: 16.035141, loss: 10.789702
saving model for epoch 49
Testing for epoch: 49
Average test PNSR is 25.371375 for 500 images
Start to train epoch 50
21:47:51.379523 Step[0/6995], lr: 0.000100, mv_avg_loss: 14.388891, loss: 15.676764
21:47:56.194509 Step[100/6995], lr: 0.000100, mv_avg_loss: 15.853411, loss: 11.195717
21:48:00.984421 Step[200/6995], lr: 0.000100, mv_avg_loss: 14.482773, loss: 12.564349
21:48:05.807257 Step[300/6995], lr: 0.000100, mv_avg_loss: 14.499019, loss: 17.696918
21:48:10.609405 Step[400/6995], lr: 0.000100, mv_avg_loss: 14.909025, loss: 12.027571
21:48:15.416082 Step[500/6995], lr: 0.000100, mv_avg_loss: 14.394962, loss: 18.945793
21:48:20.194300 Step[600/6995], lr: 0.000100, mv_avg_loss: 15.088201, loss: 11.556870
21:48:24.995855 Step[700/6995], lr: 0.000100, mv_avg_loss: 13.550298, loss: 28.100670
21:48:29.803135 Step[800/6995], lr: 0.000100, mv_avg_loss: 14.764799, loss: 12.982588
21:48:34.589555 Step[900/6995], lr: 0.000100, mv_avg_loss: 14.268031, loss: 8.998974
21:48:39.381937 Step[1000/6995], lr: 0.000100, mv_avg_loss: 15.843655, loss: 21.801517
21:48:44.200614 Step[1100/6995], lr: 0.000100, mv_avg_loss: 15.514292, loss: 26.701317
21:48:49.001405 Step[1200/6995], lr: 0.000100, mv_avg_loss: 15.763010, loss: 9.985145
21:48:53.794838 Step[1300/6995], lr: 0.000100, mv_avg_loss: 15.088063, loss: 16.232174
21:48:58.607941 Step[1400/6995], lr: 0.000100, mv_avg_loss: 14.068638, loss: 30.259100
21:49:03.401918 Step[1500/6995], lr: 0.000100, mv_avg_loss: 14.865541, loss: 11.323215
21:49:08.204524 Step[1600/6995], lr: 0.000100, mv_avg_loss: 14.263556, loss: 13.190653
21:49:13.011395 Step[1700/6995], lr: 0.000100, mv_avg_loss: 14.389622, loss: 14.032583
21:49:17.807126 Step[1800/6995], lr: 0.000100, mv_avg_loss: 14.537574, loss: 14.402808
21:49:22.600552 Step[1900/6995], lr: 0.000100, mv_avg_loss: 15.306542, loss: 11.735595
21:49:27.392998 Step[2000/6995], lr: 0.000100, mv_avg_loss: 14.417086, loss: 10.535683
21:49:32.206821 Step[2100/6995], lr: 0.000100, mv_avg_loss: 15.720754, loss: 15.246509
21:49:36.992892 Step[2200/6995], lr: 0.000100, mv_avg_loss: 15.202757, loss: 23.197460
21:49:41.783976 Step[2300/6995], lr: 0.000100, mv_avg_loss: 15.581988, loss: 21.533949
21:49:46.572128 Step[2400/6995], lr: 0.000100, mv_avg_loss: 14.717583, loss: 12.983986
21:49:51.377613 Step[2500/6995], lr: 0.000100, mv_avg_loss: 14.356046, loss: 14.449966
21:49:56.179056 Step[2600/6995], lr: 0.000100, mv_avg_loss: 16.323181, loss: 28.052343
21:50:00.974721 Step[2700/6995], lr: 0.000100, mv_avg_loss: 14.441576, loss: 28.491528
21:50:05.761012 Step[2800/6995], lr: 0.000100, mv_avg_loss: 15.312997, loss: 28.279181
21:50:10.564074 Step[2900/6995], lr: 0.000100, mv_avg_loss: 14.648475, loss: 10.105497
21:50:15.346097 Step[3000/6995], lr: 0.000100, mv_avg_loss: 14.675117, loss: 17.208073
21:50:20.138925 Step[3100/6995], lr: 0.000100, mv_avg_loss: 15.264140, loss: 13.635824
21:50:24.930815 Step[3200/6995], lr: 0.000100, mv_avg_loss: 14.713974, loss: 11.456079
21:50:29.709275 Step[3300/6995], lr: 0.000100, mv_avg_loss: 15.294381, loss: 28.528563
21:50:34.510846 Step[3400/6995], lr: 0.000100, mv_avg_loss: 13.883160, loss: 12.346802
21:50:39.314048 Step[3500/6995], lr: 0.000100, mv_avg_loss: 14.766847, loss: 18.262907
21:50:44.116486 Step[3600/6995], lr: 0.000100, mv_avg_loss: 13.775608, loss: 13.284950
21:50:48.908767 Step[3700/6995], lr: 0.000100, mv_avg_loss: 15.356454, loss: 20.519789
21:50:53.699600 Step[3800/6995], lr: 0.000100, mv_avg_loss: 15.169072, loss: 15.150920
21:50:58.503071 Step[3900/6995], lr: 0.000100, mv_avg_loss: 14.156756, loss: 9.723406
21:51:03.295266 Step[4000/6995], lr: 0.000100, mv_avg_loss: 15.317867, loss: 12.309447
21:51:08.083685 Step[4100/6995], lr: 0.000100, mv_avg_loss: 14.244761, loss: 12.356062
21:51:12.883445 Step[4200/6995], lr: 0.000100, mv_avg_loss: 14.634500, loss: 12.828264
21:51:17.677038 Step[4300/6995], lr: 0.000100, mv_avg_loss: 15.035933, loss: 10.423212
21:51:22.467190 Step[4400/6995], lr: 0.000100, mv_avg_loss: 15.771211, loss: 8.980312
21:51:27.267977 Step[4500/6995], lr: 0.000100, mv_avg_loss: 16.305490, loss: 8.599951
21:51:32.049784 Step[4600/6995], lr: 0.000100, mv_avg_loss: 15.426733, loss: 16.356308
21:51:36.846203 Step[4700/6995], lr: 0.000100, mv_avg_loss: 14.551398, loss: 11.936890
21:51:41.641911 Step[4800/6995], lr: 0.000100, mv_avg_loss: 14.637403, loss: 9.580755
21:51:46.428815 Step[4900/6995], lr: 0.000100, mv_avg_loss: 14.884148, loss: 11.847509
21:51:51.210374 Step[5000/6995], lr: 0.000100, mv_avg_loss: 13.248513, loss: 20.238346
21:51:55.988964 Step[5100/6995], lr: 0.000100, mv_avg_loss: 14.431591, loss: 14.855160
21:52:00.778593 Step[5200/6995], lr: 0.000100, mv_avg_loss: 15.311864, loss: 23.354370
21:52:05.584726 Step[5300/6995], lr: 0.000100, mv_avg_loss: 14.039656, loss: 15.385283
21:52:10.377345 Step[5400/6995], lr: 0.000100, mv_avg_loss: 14.321486, loss: 13.042229
21:52:15.167824 Step[5500/6995], lr: 0.000100, mv_avg_loss: 14.948042, loss: 28.977882
21:52:20.034296 Step[5600/6995], lr: 0.000100, mv_avg_loss: 14.798989, loss: 14.554741
21:52:24.823681 Step[5700/6995], lr: 0.000100, mv_avg_loss: 14.526538, loss: 20.593163
21:52:29.611998 Step[5800/6995], lr: 0.000100, mv_avg_loss: 15.262663, loss: 13.604767
21:52:34.398103 Step[5900/6995], lr: 0.000100, mv_avg_loss: 15.314398, loss: 18.552752
21:52:39.187104 Step[6000/6995], lr: 0.000100, mv_avg_loss: 15.093193, loss: 12.619078
21:52:43.977714 Step[6100/6995], lr: 0.000100, mv_avg_loss: 15.060021, loss: 11.462700
21:52:48.766066 Step[6200/6995], lr: 0.000100, mv_avg_loss: 14.007802, loss: 20.503885
21:52:53.561541 Step[6300/6995], lr: 0.000100, mv_avg_loss: 14.572425, loss: 15.755897
21:52:58.359080 Step[6400/6995], lr: 0.000100, mv_avg_loss: 14.582851, loss: 11.023771
21:53:03.145021 Step[6500/6995], lr: 0.000100, mv_avg_loss: 16.027575, loss: 11.779806
21:53:07.928388 Step[6600/6995], lr: 0.000100, mv_avg_loss: 14.527963, loss: 10.491763
21:53:12.723703 Step[6700/6995], lr: 0.000100, mv_avg_loss: 14.833978, loss: 14.722526
21:53:17.511992 Step[6800/6995], lr: 0.000100, mv_avg_loss: 15.383153, loss: 9.516212
21:53:22.292522 Step[6900/6995], lr: 0.000100, mv_avg_loss: 14.042328, loss: 15.171560
Testing for epoch: 50
Average test PNSR is 25.419408 for 500 images
Start to train epoch 51
21:53:49.082448 Step[0/6995], lr: 0.000100, mv_avg_loss: 15.595888, loss: 22.583687
21:53:53.861144 Step[100/6995], lr: 0.000100, mv_avg_loss: 16.410835, loss: 12.660777
21:53:58.637858 Step[200/6995], lr: 0.000100, mv_avg_loss: 15.113039, loss: 13.571674
21:54:03.410030 Step[300/6995], lr: 0.000100, mv_avg_loss: 16.023783, loss: 13.108639
21:54:08.176903 Step[400/6995], lr: 0.000100, mv_avg_loss: 15.138236, loss: 10.318184
21:54:12.973222 Step[500/6995], lr: 0.000100, mv_avg_loss: 14.693904, loss: 10.136725
21:54:17.741248 Step[600/6995], lr: 0.000100, mv_avg_loss: 15.007553, loss: 15.907535
21:54:22.525466 Step[700/6995], lr: 0.000100, mv_avg_loss: 13.822647, loss: 16.143034
21:54:27.319626 Step[800/6995], lr: 0.000100, mv_avg_loss: 15.166722, loss: 12.457804
21:54:32.106029 Step[900/6995], lr: 0.000100, mv_avg_loss: 14.566027, loss: 11.118538
21:54:36.878875 Step[1000/6995], lr: 0.000100, mv_avg_loss: 14.001261, loss: 8.760170
21:54:41.680344 Step[1100/6995], lr: 0.000100, mv_avg_loss: 15.131964, loss: 12.234790
21:54:46.475902 Step[1200/6995], lr: 0.000100, mv_avg_loss: 15.392431, loss: 10.203438
21:54:51.264290 Step[1300/6995], lr: 0.000100, mv_avg_loss: 13.840252, loss: 17.777966
21:54:56.049342 Step[1400/6995], lr: 0.000100, mv_avg_loss: 15.470198, loss: 11.381042
21:55:00.831866 Step[1500/6995], lr: 0.000100, mv_avg_loss: 14.712130, loss: 12.760889
21:55:05.619046 Step[1600/6995], lr: 0.000100, mv_avg_loss: 13.844081, loss: 19.276247
21:55:10.413569 Step[1700/6995], lr: 0.000100, mv_avg_loss: 15.010756, loss: 25.570404
21:55:15.193534 Step[1800/6995], lr: 0.000100, mv_avg_loss: 14.279922, loss: 14.245359
21:55:19.972759 Step[1900/6995], lr: 0.000100, mv_avg_loss: 15.203114, loss: 10.511776
21:55:24.751513 Step[2000/6995], lr: 0.000100, mv_avg_loss: 14.493384, loss: 17.782137
21:55:29.537984 Step[2100/6995], lr: 0.000100, mv_avg_loss: 14.130569, loss: 9.644312
21:55:34.316800 Step[2200/6995], lr: 0.000100, mv_avg_loss: 16.765726, loss: 19.124083
21:55:39.104278 Step[2300/6995], lr: 0.000100, mv_avg_loss: 14.394669, loss: 14.069537
21:55:43.896532 Step[2400/6995], lr: 0.000100, mv_avg_loss: 14.117249, loss: 15.014421
21:55:48.684328 Step[2500/6995], lr: 0.000100, mv_avg_loss: 15.026091, loss: 12.462309
21:55:53.469493 Step[2600/6995], lr: 0.000100, mv_avg_loss: 14.700917, loss: 14.781596
21:55:58.244485 Step[2700/6995], lr: 0.000100, mv_avg_loss: 13.457517, loss: 14.621151
21:56:03.028310 Step[2800/6995], lr: 0.000100, mv_avg_loss: 14.619692, loss: 17.573761
21:56:07.816359 Step[2900/6995], lr: 0.000100, mv_avg_loss: 15.051132, loss: 9.564076
21:56:12.598859 Step[3000/6995], lr: 0.000100, mv_avg_loss: 14.570092, loss: 19.569904
21:56:17.467489 Step[3100/6995], lr: 0.000100, mv_avg_loss: 15.586692, loss: 13.280489
21:56:22.313798 Step[3200/6995], lr: 0.000100, mv_avg_loss: 15.541512, loss: 20.393719
21:56:27.183566 Step[3300/6995], lr: 0.000100, mv_avg_loss: 14.419313, loss: 12.332530
21:56:31.966915 Step[3400/6995], lr: 0.000100, mv_avg_loss: 14.691357, loss: 23.088583
21:56:36.756487 Step[3500/6995], lr: 0.000100, mv_avg_loss: 13.353436, loss: 12.725265
21:56:41.542064 Step[3600/6995], lr: 0.000100, mv_avg_loss: 15.432258, loss: 11.278868
21:56:46.328048 Step[3700/6995], lr: 0.000100, mv_avg_loss: 14.193178, loss: 12.690590
21:56:51.119330 Step[3800/6995], lr: 0.000100, mv_avg_loss: 14.844930, loss: 13.954790
21:56:55.905793 Step[3900/6995], lr: 0.000100, mv_avg_loss: 14.557906, loss: 8.469303
21:57:00.681815 Step[4000/6995], lr: 0.000100, mv_avg_loss: 15.739415, loss: 14.008606
21:57:05.467687 Step[4100/6995], lr: 0.000100, mv_avg_loss: 15.339622, loss: 10.488207
21:57:10.247967 Step[4200/6995], lr: 0.000100, mv_avg_loss: 15.119019, loss: 9.635820
21:57:15.028834 Step[4300/6995], lr: 0.000100, mv_avg_loss: 14.731652, loss: 10.439000
21:57:19.802172 Step[4400/6995], lr: 0.000100, mv_avg_loss: 15.442863, loss: 16.614708
21:57:24.571744 Step[4500/6995], lr: 0.000100, mv_avg_loss: 15.617354, loss: 19.553932
21:57:29.345110 Step[4600/6995], lr: 0.000100, mv_avg_loss: 14.254676, loss: 14.528110
21:57:34.122332 Step[4700/6995], lr: 0.000100, mv_avg_loss: 15.031487, loss: 12.249443
21:57:38.902950 Step[4800/6995], lr: 0.000100, mv_avg_loss: 15.537301, loss: 16.363377
21:57:43.686061 Step[4900/6995], lr: 0.000100, mv_avg_loss: 15.365318, loss: 9.799650
21:57:48.465908 Step[5000/6995], lr: 0.000100, mv_avg_loss: 15.238116, loss: 25.969275
21:57:53.249590 Step[5100/6995], lr: 0.000100, mv_avg_loss: 15.479943, loss: 10.672499
21:57:58.045414 Step[5200/6995], lr: 0.000100, mv_avg_loss: 14.191887, loss: 9.369882
21:58:02.820022 Step[5300/6995], lr: 0.000100, mv_avg_loss: 15.486026, loss: 15.190486
21:58:07.595701 Step[5400/6995], lr: 0.000100, mv_avg_loss: 14.766912, loss: 10.557053
21:58:12.381336 Step[5500/6995], lr: 0.000100, mv_avg_loss: 14.913685, loss: 18.478603
21:58:17.170622 Step[5600/6995], lr: 0.000100, mv_avg_loss: 14.323370, loss: 10.715374
21:58:21.949595 Step[5700/6995], lr: 0.000100, mv_avg_loss: 14.299711, loss: 10.997894
21:58:26.734208 Step[5800/6995], lr: 0.000100, mv_avg_loss: 14.618563, loss: 14.121412
21:58:31.522112 Step[5900/6995], lr: 0.000100, mv_avg_loss: 14.714178, loss: 10.470508
21:58:36.309901 Step[6000/6995], lr: 0.000100, mv_avg_loss: 14.640285, loss: 15.981144
21:58:41.096975 Step[6100/6995], lr: 0.000100, mv_avg_loss: 16.214010, loss: 10.324706
21:58:45.882719 Step[6200/6995], lr: 0.000100, mv_avg_loss: 14.157510, loss: 16.640018
21:58:50.667598 Step[6300/6995], lr: 0.000100, mv_avg_loss: 15.582225, loss: 10.134590
21:58:55.460091 Step[6400/6995], lr: 0.000100, mv_avg_loss: 15.298831, loss: 15.020003
21:59:00.253065 Step[6500/6995], lr: 0.000100, mv_avg_loss: 13.870965, loss: 12.421111
21:59:05.043025 Step[6600/6995], lr: 0.000100, mv_avg_loss: 15.514301, loss: 16.595083
21:59:09.827677 Step[6700/6995], lr: 0.000100, mv_avg_loss: 15.365742, loss: 14.094241
21:59:14.625850 Step[6800/6995], lr: 0.000100, mv_avg_loss: 15.163674, loss: 16.044867
21:59:19.428645 Step[6900/6995], lr: 0.000100, mv_avg_loss: 13.786179, loss: 13.481048
Testing for epoch: 51
Average test PNSR is 25.405603 for 500 images
Start to train epoch 52
21:59:46.100289 Step[0/6995], lr: 0.000100, mv_avg_loss: 14.029103, loss: 14.750408
21:59:50.875409 Step[100/6995], lr: 0.000100, mv_avg_loss: 16.480448, loss: 11.391731
21:59:55.655525 Step[200/6995], lr: 0.000100, mv_avg_loss: 14.256777, loss: 17.141214
22:00:00.447609 Step[300/6995], lr: 0.000100, mv_avg_loss: 14.700814, loss: 30.317984
22:00:05.228619 Step[400/6995], lr: 0.000100, mv_avg_loss: 14.062689, loss: 16.375933
22:00:10.023598 Step[500/6995], lr: 0.000100, mv_avg_loss: 14.738537, loss: 16.052755
22:00:14.816957 Step[600/6995], lr: 0.000100, mv_avg_loss: 15.065936, loss: 14.445373
22:00:19.599717 Step[700/6995], lr: 0.000100, mv_avg_loss: 14.606485, loss: 29.754951
22:00:24.392287 Step[800/6995], lr: 0.000100, mv_avg_loss: 14.856074, loss: 16.329227
22:00:29.172881 Step[900/6995], lr: 0.000100, mv_avg_loss: 13.685743, loss: 11.655966
22:00:33.969197 Step[1000/6995], lr: 0.000100, mv_avg_loss: 15.049382, loss: 20.079145
22:00:38.752219 Step[1100/6995], lr: 0.000100, mv_avg_loss: 14.918932, loss: 15.301376
22:00:43.538236 Step[1200/6995], lr: 0.000100, mv_avg_loss: 14.047337, loss: 12.009288
22:00:48.365888 Step[1300/6995], lr: 0.000100, mv_avg_loss: 14.371491, loss: 15.097826
22:00:53.161556 Step[1400/6995], lr: 0.000100, mv_avg_loss: 15.867093, loss: 17.494864
22:00:57.952349 Step[1500/6995], lr: 0.000100, mv_avg_loss: 15.221567, loss: 9.445402
22:01:02.760748 Step[1600/6995], lr: 0.000100, mv_avg_loss: 14.976772, loss: 15.114143
22:01:07.562864 Step[1700/6995], lr: 0.000100, mv_avg_loss: 15.999708, loss: 12.858517
22:01:12.358699 Step[1800/6995], lr: 0.000100, mv_avg_loss: 14.905217, loss: 8.679439
22:01:17.156044 Step[1900/6995], lr: 0.000100, mv_avg_loss: 13.944709, loss: 18.136147
22:01:21.953270 Step[2000/6995], lr: 0.000100, mv_avg_loss: 14.485752, loss: 19.149227
22:01:26.775970 Step[2100/6995], lr: 0.000100, mv_avg_loss: 13.954056, loss: 23.042496
22:01:31.573678 Step[2200/6995], lr: 0.000100, mv_avg_loss: 13.975237, loss: 8.061793
22:01:36.359430 Step[2300/6995], lr: 0.000100, mv_avg_loss: 14.080602, loss: 11.363256
22:01:41.162853 Step[2400/6995], lr: 0.000100, mv_avg_loss: 14.180922, loss: 10.219608
22:01:45.965010 Step[2500/6995], lr: 0.000100, mv_avg_loss: 14.968678, loss: 12.488051
22:01:50.763785 Step[2600/6995], lr: 0.000100, mv_avg_loss: 14.894109, loss: 13.998713
22:01:55.564972 Step[2700/6995], lr: 0.000100, mv_avg_loss: 14.109597, loss: 11.723083
22:02:00.354887 Step[2800/6995], lr: 0.000100, mv_avg_loss: 14.535803, loss: 22.824451
22:02:05.170126 Step[2900/6995], lr: 0.000100, mv_avg_loss: 13.984428, loss: 11.766399
22:02:09.975607 Step[3000/6995], lr: 0.000100, mv_avg_loss: 14.821753, loss: 11.531271
22:02:14.784929 Step[3100/6995], lr: 0.000100, mv_avg_loss: 15.292543, loss: 22.290335
22:02:19.577584 Step[3200/6995], lr: 0.000100, mv_avg_loss: 14.086757, loss: 20.288725
22:02:24.373608 Step[3300/6995], lr: 0.000100, mv_avg_loss: 14.170726, loss: 8.949397
22:02:29.175234 Step[3400/6995], lr: 0.000100, mv_avg_loss: 14.480580, loss: 15.748046
22:02:33.988547 Step[3500/6995], lr: 0.000100, mv_avg_loss: 14.767856, loss: 8.861301
22:02:38.783677 Step[3600/6995], lr: 0.000100, mv_avg_loss: 13.751102, loss: 7.387321
22:02:43.574724 Step[3700/6995], lr: 0.000100, mv_avg_loss: 15.032018, loss: 12.537508
22:02:48.376757 Step[3800/6995], lr: 0.000100, mv_avg_loss: 15.795874, loss: 10.861620
22:02:53.175973 Step[3900/6995], lr: 0.000100, mv_avg_loss: 14.337185, loss: 30.367392
22:02:57.985600 Step[4000/6995], lr: 0.000100, mv_avg_loss: 14.985533, loss: 18.752024
22:03:02.783948 Step[4100/6995], lr: 0.000100, mv_avg_loss: 14.678301, loss: 16.572805
22:03:07.589121 Step[4200/6995], lr: 0.000100, mv_avg_loss: 14.624901, loss: 17.049511
22:03:12.391269 Step[4300/6995], lr: 0.000100, mv_avg_loss: 15.096180, loss: 15.531489
22:03:17.182861 Step[4400/6995], lr: 0.000100, mv_avg_loss: 14.200770, loss: 16.170227
22:03:21.973820 Step[4500/6995], lr: 0.000100, mv_avg_loss: 15.320863, loss: 9.628090
22:03:26.784246 Step[4600/6995], lr: 0.000100, mv_avg_loss: 14.795164, loss: 10.450006
22:03:31.580701 Step[4700/6995], lr: 0.000100, mv_avg_loss: 14.418821, loss: 21.732510
22:03:36.373027 Step[4800/6995], lr: 0.000100, mv_avg_loss: 14.834461, loss: 17.276600
22:03:41.176315 Step[4900/6995], lr: 0.000100, mv_avg_loss: 14.310737, loss: 8.841563
22:03:45.970157 Step[5000/6995], lr: 0.000100, mv_avg_loss: 14.313765, loss: 15.041369
22:03:50.764383 Step[5100/6995], lr: 0.000100, mv_avg_loss: 14.305970, loss: 19.017342
22:03:55.562910 Step[5200/6995], lr: 0.000100, mv_avg_loss: 14.386358, loss: 10.656430
22:04:00.358844 Step[5300/6995], lr: 0.000100, mv_avg_loss: 16.071924, loss: 7.341022
22:04:05.152427 Step[5400/6995], lr: 0.000100, mv_avg_loss: 15.767034, loss: 16.804989
22:04:09.946043 Step[5500/6995], lr: 0.000100, mv_avg_loss: 15.302919, loss: 11.259738
22:04:14.741313 Step[5600/6995], lr: 0.000100, mv_avg_loss: 13.871770, loss: 13.770057
22:04:19.532126 Step[5700/6995], lr: 0.000100, mv_avg_loss: 14.082016, loss: 12.803451
22:04:24.321999 Step[5800/6995], lr: 0.000100, mv_avg_loss: 14.211277, loss: 17.867506
22:04:29.112287 Step[5900/6995], lr: 0.000100, mv_avg_loss: 15.748846, loss: 18.692276
22:04:33.904446 Step[6000/6995], lr: 0.000100, mv_avg_loss: 14.306045, loss: 14.368744
22:04:38.690124 Step[6100/6995], lr: 0.000100, mv_avg_loss: 15.497767, loss: 17.132164
22:04:43.463127 Step[6200/6995], lr: 0.000100, mv_avg_loss: 15.414043, loss: 6.644537
22:04:48.284033 Step[6300/6995], lr: 0.000100, mv_avg_loss: 14.615004, loss: 14.936186
22:04:53.072477 Step[6400/6995], lr: 0.000100, mv_avg_loss: 14.963698, loss: 13.264984
22:04:57.856013 Step[6500/6995], lr: 0.000100, mv_avg_loss: 14.022803, loss: 9.931363
22:05:02.642888 Step[6600/6995], lr: 0.000100, mv_avg_loss: 13.574962, loss: 16.191135
22:05:07.433812 Step[6700/6995], lr: 0.000100, mv_avg_loss: 14.582816, loss: 16.736900
22:05:12.226025 Step[6800/6995], lr: 0.000100, mv_avg_loss: 14.341411, loss: 19.487772
22:05:17.021272 Step[6900/6995], lr: 0.000100, mv_avg_loss: 14.476841, loss: 15.815720
Testing for epoch: 52
Average test PNSR is 25.361899 for 500 images
Start to train epoch 53
22:05:43.876683 Step[0/6995], lr: 0.000100, mv_avg_loss: 14.548199, loss: 11.045887
22:05:48.647289 Step[100/6995], lr: 0.000100, mv_avg_loss: 14.545128, loss: 17.937502
22:05:53.428827 Step[200/6995], lr: 0.000100, mv_avg_loss: 15.471423, loss: 14.300642
22:05:58.204383 Step[300/6995], lr: 0.000100, mv_avg_loss: 14.507213, loss: 15.380651
22:06:02.982372 Step[400/6995], lr: 0.000100, mv_avg_loss: 15.485506, loss: 8.610639
22:06:07.766721 Step[500/6995], lr: 0.000100, mv_avg_loss: 15.377919, loss: 10.229259
22:06:12.552217 Step[600/6995], lr: 0.000100, mv_avg_loss: 15.216686, loss: 7.932709
22:06:17.323940 Step[700/6995], lr: 0.000100, mv_avg_loss: 15.776011, loss: 13.819803
22:06:22.087450 Step[800/6995], lr: 0.000100, mv_avg_loss: 14.243195, loss: 12.176281
22:06:26.855286 Step[900/6995], lr: 0.000100, mv_avg_loss: 14.491233, loss: 16.352467
22:06:31.647659 Step[1000/6995], lr: 0.000100, mv_avg_loss: 13.971802, loss: 14.076884
22:06:36.423262 Step[1100/6995], lr: 0.000100, mv_avg_loss: 14.264896, loss: 24.526730
22:06:41.195992 Step[1200/6995], lr: 0.000100, mv_avg_loss: 14.280933, loss: 11.505965
22:06:45.980246 Step[1300/6995], lr: 0.000100, mv_avg_loss: 15.616672, loss: 20.813156
22:06:50.768262 Step[1400/6995], lr: 0.000100, mv_avg_loss: 15.018158, loss: 17.066063
22:06:55.539683 Step[1500/6995], lr: 0.000100, mv_avg_loss: 14.745490, loss: 18.844044
22:07:00.309884 Step[1600/6995], lr: 0.000100, mv_avg_loss: 14.818501, loss: 10.421503
22:07:05.085483 Step[1700/6995], lr: 0.000100, mv_avg_loss: 15.301511, loss: 10.780449
22:07:09.872911 Step[1800/6995], lr: 0.000100, mv_avg_loss: 14.675915, loss: 20.227541
22:07:14.653183 Step[1900/6995], lr: 0.000100, mv_avg_loss: 13.823971, loss: 19.365013
22:07:19.444979 Step[2000/6995], lr: 0.000100, mv_avg_loss: 14.068595, loss: 11.360880
22:07:24.230132 Step[2100/6995], lr: 0.000100, mv_avg_loss: 15.233802, loss: 14.828772
22:07:29.038331 Step[2200/6995], lr: 0.000100, mv_avg_loss: 13.631487, loss: 14.974012
22:07:33.822358 Step[2300/6995], lr: 0.000100, mv_avg_loss: 14.818536, loss: 9.014046
22:07:38.612245 Step[2400/6995], lr: 0.000100, mv_avg_loss: 15.211005, loss: 19.482834
22:07:43.385139 Step[2500/6995], lr: 0.000100, mv_avg_loss: 13.705174, loss: 16.911085
22:07:48.162408 Step[2600/6995], lr: 0.000100, mv_avg_loss: 13.875585, loss: 13.966651
22:07:52.957252 Step[2700/6995], lr: 0.000100, mv_avg_loss: 15.003436, loss: 12.732616
22:07:57.736520 Step[2800/6995], lr: 0.000100, mv_avg_loss: 13.316545, loss: 14.929239
22:08:02.508959 Step[2900/6995], lr: 0.000100, mv_avg_loss: 15.713851, loss: 20.083319
22:08:07.283160 Step[3000/6995], lr: 0.000100, mv_avg_loss: 14.383824, loss: 9.092768
22:08:12.047782 Step[3100/6995], lr: 0.000100, mv_avg_loss: 14.492405, loss: 14.357330
22:08:16.835989 Step[3200/6995], lr: 0.000100, mv_avg_loss: 15.176772, loss: 20.399139
22:08:21.621084 Step[3300/6995], lr: 0.000100, mv_avg_loss: 15.223630, loss: 16.996002
22:08:26.397504 Step[3400/6995], lr: 0.000100, mv_avg_loss: 13.591171, loss: 13.227015
22:08:31.170802 Step[3500/6995], lr: 0.000100, mv_avg_loss: 14.397898, loss: 16.919548
22:08:35.946634 Step[3600/6995], lr: 0.000100, mv_avg_loss: 14.891568, loss: 14.641130
22:08:40.730707 Step[3700/6995], lr: 0.000100, mv_avg_loss: 15.105505, loss: 10.218483
22:08:45.508682 Step[3800/6995], lr: 0.000100, mv_avg_loss: 14.296154, loss: 15.680857
22:08:50.275306 Step[3900/6995], lr: 0.000100, mv_avg_loss: 14.257952, loss: 16.136322
22:08:55.064528 Step[4000/6995], lr: 0.000100, mv_avg_loss: 13.959696, loss: 10.682597
22:08:59.855135 Step[4100/6995], lr: 0.000100, mv_avg_loss: 15.462865, loss: 18.249714
22:09:04.624931 Step[4200/6995], lr: 0.000100, mv_avg_loss: 14.090596, loss: 9.478266
22:09:09.446542 Step[4300/6995], lr: 0.000100, mv_avg_loss: 15.009871, loss: 17.430904
22:09:14.222306 Step[4400/6995], lr: 0.000100, mv_avg_loss: 15.272875, loss: 17.246094
22:09:19.002898 Step[4500/6995], lr: 0.000100, mv_avg_loss: 14.175957, loss: 14.602565
22:09:23.783172 Step[4600/6995], lr: 0.000100, mv_avg_loss: 14.672726, loss: 10.477312
22:09:28.555128 Step[4700/6995], lr: 0.000100, mv_avg_loss: 15.321154, loss: 25.032871
22:09:33.328841 Step[4800/6995], lr: 0.000100, mv_avg_loss: 14.397448, loss: 12.542969
22:09:38.106645 Step[4900/6995], lr: 0.000100, mv_avg_loss: 14.326962, loss: 10.075888
22:09:42.893965 Step[5000/6995], lr: 0.000100, mv_avg_loss: 14.577988, loss: 8.295189
22:09:47.674899 Step[5100/6995], lr: 0.000100, mv_avg_loss: 13.855963, loss: 12.456275
22:09:52.448913 Step[5200/6995], lr: 0.000100, mv_avg_loss: 14.032558, loss: 18.192781
22:09:57.216892 Step[5300/6995], lr: 0.000100, mv_avg_loss: 14.203999, loss: 10.521990
22:10:01.996796 Step[5400/6995], lr: 0.000100, mv_avg_loss: 13.796987, loss: 8.071349
22:10:06.770649 Step[5500/6995], lr: 0.000100, mv_avg_loss: 14.797806, loss: 18.493322
22:10:11.565075 Step[5600/6995], lr: 0.000100, mv_avg_loss: 14.995409, loss: 12.666922
22:10:16.341481 Step[5700/6995], lr: 0.000100, mv_avg_loss: 13.207706, loss: 12.318305
22:10:21.107115 Step[5800/6995], lr: 0.000100, mv_avg_loss: 15.209464, loss: 16.502855
22:10:25.869078 Step[5900/6995], lr: 0.000100, mv_avg_loss: 13.746272, loss: 12.082645
22:10:30.618321 Step[6000/6995], lr: 0.000100, mv_avg_loss: 14.079910, loss: 8.800584
22:10:35.359739 Step[6100/6995], lr: 0.000100, mv_avg_loss: 15.396364, loss: 13.722499
22:10:40.120809 Step[6200/6995], lr: 0.000100, mv_avg_loss: 13.768442, loss: 11.676508
22:10:44.874047 Step[6300/6995], lr: 0.000100, mv_avg_loss: 15.731357, loss: 38.071621
22:10:49.632153 Step[6400/6995], lr: 0.000100, mv_avg_loss: 14.265512, loss: 16.501625
22:10:54.389381 Step[6500/6995], lr: 0.000100, mv_avg_loss: 15.097297, loss: 17.731993
22:10:59.146448 Step[6600/6995], lr: 0.000100, mv_avg_loss: 15.397398, loss: 18.669439
22:11:03.903663 Step[6700/6995], lr: 0.000100, mv_avg_loss: 14.292295, loss: 17.643799
22:11:08.660288 Step[6800/6995], lr: 0.000100, mv_avg_loss: 15.361138, loss: 16.510242
22:11:13.417268 Step[6900/6995], lr: 0.000100, mv_avg_loss: 14.774353, loss: 11.190410
Testing for epoch: 53
Average test PNSR is 25.324653 for 500 images
Start to train epoch 54
22:11:40.434007 Step[0/6995], lr: 0.000100, mv_avg_loss: 15.763561, loss: 12.927608
22:11:45.183166 Step[100/6995], lr: 0.000100, mv_avg_loss: 14.678722, loss: 9.323690
22:11:49.928041 Step[200/6995], lr: 0.000100, mv_avg_loss: 14.645679, loss: 10.110571
22:11:54.662561 Step[300/6995], lr: 0.000100, mv_avg_loss: 14.113605, loss: 12.110535
22:11:59.406273 Step[400/6995], lr: 0.000100, mv_avg_loss: 14.782159, loss: 15.532763
22:12:04.130218 Step[500/6995], lr: 0.000100, mv_avg_loss: 14.251257, loss: 24.824997
22:12:08.876015 Step[600/6995], lr: 0.000100, mv_avg_loss: 13.269568, loss: 24.235348
22:12:13.626300 Step[700/6995], lr: 0.000100, mv_avg_loss: 14.362847, loss: 19.942455
22:12:18.375310 Step[800/6995], lr: 0.000100, mv_avg_loss: 13.977999, loss: 20.591133
22:12:23.134569 Step[900/6995], lr: 0.000100, mv_avg_loss: 15.103848, loss: 12.595449
22:12:27.889660 Step[1000/6995], lr: 0.000100, mv_avg_loss: 14.551390, loss: 11.205755
22:12:32.648160 Step[1100/6995], lr: 0.000100, mv_avg_loss: 14.058375, loss: 13.854368
22:12:37.403098 Step[1200/6995], lr: 0.000100, mv_avg_loss: 15.925312, loss: 14.021239
22:12:42.155971 Step[1300/6995], lr: 0.000100, mv_avg_loss: 13.652537, loss: 10.999900
22:12:46.913722 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.853160, loss: 12.861679
22:12:51.679319 Step[1500/6995], lr: 0.000100, mv_avg_loss: 14.701721, loss: 12.831366
22:12:56.436823 Step[1600/6995], lr: 0.000100, mv_avg_loss: 14.451652, loss: 13.004268
22:13:01.192325 Step[1700/6995], lr: 0.000100, mv_avg_loss: 13.682220, loss: 12.220953
22:13:05.945976 Step[1800/6995], lr: 0.000100, mv_avg_loss: 14.251186, loss: 13.436981
22:13:10.712552 Step[1900/6995], lr: 0.000100, mv_avg_loss: 14.178800, loss: 17.868742
22:13:15.473319 Step[2000/6995], lr: 0.000100, mv_avg_loss: 13.714478, loss: 14.902934
22:13:20.236187 Step[2100/6995], lr: 0.000100, mv_avg_loss: 14.738461, loss: 16.812948
22:13:24.995804 Step[2200/6995], lr: 0.000100, mv_avg_loss: 15.181625, loss: 10.107058
22:13:29.793711 Step[2300/6995], lr: 0.000100, mv_avg_loss: 14.258307, loss: 9.145298
22:13:34.558640 Step[2400/6995], lr: 0.000100, mv_avg_loss: 14.576464, loss: 16.031902
22:13:39.325926 Step[2500/6995], lr: 0.000100, mv_avg_loss: 14.331388, loss: 12.664158
22:13:44.099498 Step[2600/6995], lr: 0.000100, mv_avg_loss: 15.489441, loss: 10.907309
22:13:48.868920 Step[2700/6995], lr: 0.000100, mv_avg_loss: 14.556763, loss: 15.606153
22:13:53.641220 Step[2800/6995], lr: 0.000100, mv_avg_loss: 15.009328, loss: 13.923511
22:13:58.402585 Step[2900/6995], lr: 0.000100, mv_avg_loss: 13.786430, loss: 9.949612
22:14:03.167871 Step[3000/6995], lr: 0.000100, mv_avg_loss: 14.529061, loss: 22.083548
22:14:07.938583 Step[3100/6995], lr: 0.000100, mv_avg_loss: 13.533867, loss: 12.044889
22:14:12.699475 Step[3200/6995], lr: 0.000100, mv_avg_loss: 14.945990, loss: 12.082832
22:14:17.464766 Step[3300/6995], lr: 0.000100, mv_avg_loss: 13.813725, loss: 13.115181
22:14:22.239717 Step[3400/6995], lr: 0.000100, mv_avg_loss: 13.277179, loss: 15.102283
22:14:27.021442 Step[3500/6995], lr: 0.000100, mv_avg_loss: 14.721639, loss: 11.029737
22:14:31.792768 Step[3600/6995], lr: 0.000100, mv_avg_loss: 14.305792, loss: 18.047924
22:14:36.558778 Step[3700/6995], lr: 0.000100, mv_avg_loss: 12.831741, loss: 12.824150
22:14:41.322003 Step[3800/6995], lr: 0.000100, mv_avg_loss: 14.416370, loss: 15.874708
22:14:46.080725 Step[3900/6995], lr: 0.000100, mv_avg_loss: 15.047988, loss: 14.753578
22:14:50.842405 Step[4000/6995], lr: 0.000100, mv_avg_loss: 14.777244, loss: 10.939240
22:14:55.606062 Step[4100/6995], lr: 0.000100, mv_avg_loss: 15.710639, loss: 9.820938
22:15:00.367511 Step[4200/6995], lr: 0.000100, mv_avg_loss: 15.222186, loss: 10.140668
22:15:05.135816 Step[4300/6995], lr: 0.000100, mv_avg_loss: 14.681175, loss: 14.280214
22:15:09.907808 Step[4400/6995], lr: 0.000100, mv_avg_loss: 14.508137, loss: 13.532108
22:15:14.688624 Step[4500/6995], lr: 0.000100, mv_avg_loss: 14.252917, loss: 17.428295
22:15:19.474131 Step[4600/6995], lr: 0.000100, mv_avg_loss: 14.946952, loss: 15.734256
22:15:24.252973 Step[4700/6995], lr: 0.000100, mv_avg_loss: 14.640506, loss: 13.315529
22:15:29.026647 Step[4800/6995], lr: 0.000100, mv_avg_loss: 15.375943, loss: 23.975384
22:15:33.795724 Step[4900/6995], lr: 0.000100, mv_avg_loss: 15.720717, loss: 10.575104
22:15:38.550319 Step[5000/6995], lr: 0.000100, mv_avg_loss: 13.592329, loss: 12.062831
22:15:43.314832 Step[5100/6995], lr: 0.000100, mv_avg_loss: 16.887442, loss: 9.816371
22:15:48.083735 Step[5200/6995], lr: 0.000100, mv_avg_loss: 13.477627, loss: 12.304770
22:15:52.847022 Step[5300/6995], lr: 0.000100, mv_avg_loss: 14.653435, loss: 12.269001
22:15:57.605376 Step[5400/6995], lr: 0.000100, mv_avg_loss: 15.416144, loss: 17.567295
22:16:02.362672 Step[5500/6995], lr: 0.000100, mv_avg_loss: 14.359437, loss: 15.464546
22:16:07.119595 Step[5600/6995], lr: 0.000100, mv_avg_loss: 14.898027, loss: 16.287720
22:16:11.888287 Step[5700/6995], lr: 0.000100, mv_avg_loss: 15.438904, loss: 14.474590
22:16:16.666200 Step[5800/6995], lr: 0.000100, mv_avg_loss: 15.008105, loss: 13.647063
22:16:21.442004 Step[5900/6995], lr: 0.000100, mv_avg_loss: 15.035300, loss: 16.439445
22:16:26.213569 Step[6000/6995], lr: 0.000100, mv_avg_loss: 14.581429, loss: 10.670370
22:16:30.987358 Step[6100/6995], lr: 0.000100, mv_avg_loss: 14.412857, loss: 11.625378
22:16:35.756717 Step[6200/6995], lr: 0.000100, mv_avg_loss: 14.403802, loss: 16.188450
22:16:40.531804 Step[6300/6995], lr: 0.000100, mv_avg_loss: 15.022074, loss: 8.703836
22:16:45.301641 Step[6400/6995], lr: 0.000100, mv_avg_loss: 14.857232, loss: 8.986273
22:16:50.068039 Step[6500/6995], lr: 0.000100, mv_avg_loss: 13.788901, loss: 17.095856
22:16:54.849236 Step[6600/6995], lr: 0.000100, mv_avg_loss: 14.754439, loss: 14.050329
22:16:59.619397 Step[6700/6995], lr: 0.000100, mv_avg_loss: 15.099603, loss: 13.626532
22:17:04.383586 Step[6800/6995], lr: 0.000100, mv_avg_loss: 13.391551, loss: 19.113640
22:17:09.148284 Step[6900/6995], lr: 0.000100, mv_avg_loss: 14.163520, loss: 15.227152
saving model for epoch 54
Testing for epoch: 54
Average test PNSR is 25.406225 for 500 images
Start to train epoch 55
22:17:35.868591 Step[0/6995], lr: 0.000100, mv_avg_loss: 14.345533, loss: 11.627570
22:17:40.620734 Step[100/6995], lr: 0.000100, mv_avg_loss: 14.816025, loss: 14.387238
22:17:45.374853 Step[200/6995], lr: 0.000100, mv_avg_loss: 13.452366, loss: 11.593314
22:17:50.148500 Step[300/6995], lr: 0.000100, mv_avg_loss: 13.732230, loss: 11.905851
22:17:54.906788 Step[400/6995], lr: 0.000100, mv_avg_loss: 14.354540, loss: 10.209693
22:17:59.670630 Step[500/6995], lr: 0.000100, mv_avg_loss: 14.510029, loss: 11.468036
22:18:04.434920 Step[600/6995], lr: 0.000100, mv_avg_loss: 15.060290, loss: 11.928100
22:18:09.183866 Step[700/6995], lr: 0.000100, mv_avg_loss: 12.951878, loss: 11.126419
22:18:13.947518 Step[800/6995], lr: 0.000100, mv_avg_loss: 13.867537, loss: 16.412594
22:18:18.707554 Step[900/6995], lr: 0.000100, mv_avg_loss: 16.560787, loss: 11.171438
22:18:23.475668 Step[1000/6995], lr: 0.000100, mv_avg_loss: 14.460224, loss: 15.794037
22:18:28.329316 Step[1100/6995], lr: 0.000100, mv_avg_loss: 14.583193, loss: 13.307902
22:18:33.081884 Step[1200/6995], lr: 0.000100, mv_avg_loss: 14.906433, loss: 17.100580
22:18:37.843518 Step[1300/6995], lr: 0.000100, mv_avg_loss: 13.722125, loss: 11.798747
22:18:42.596809 Step[1400/6995], lr: 0.000100, mv_avg_loss: 15.769462, loss: 13.627910
22:18:47.350548 Step[1500/6995], lr: 0.000100, mv_avg_loss: 12.758965, loss: 12.175426
22:18:52.123080 Step[1600/6995], lr: 0.000100, mv_avg_loss: 15.486561, loss: 12.401965
22:18:56.885375 Step[1700/6995], lr: 0.000100, mv_avg_loss: 14.601991, loss: 17.503935
22:19:01.651500 Step[1800/6995], lr: 0.000100, mv_avg_loss: 14.515671, loss: 10.734119
22:19:06.407888 Step[1900/6995], lr: 0.000100, mv_avg_loss: 14.186595, loss: 11.763191
22:19:11.172708 Step[2000/6995], lr: 0.000100, mv_avg_loss: 14.294430, loss: 10.559565
22:19:15.947542 Step[2100/6995], lr: 0.000100, mv_avg_loss: 13.871832, loss: 13.076050
22:19:20.708636 Step[2200/6995], lr: 0.000100, mv_avg_loss: 14.870310, loss: 13.860657
22:19:25.476211 Step[2300/6995], lr: 0.000100, mv_avg_loss: 14.935483, loss: 21.835562
22:19:30.230778 Step[2400/6995], lr: 0.000100, mv_avg_loss: 13.993641, loss: 11.642653
22:19:34.991107 Step[2500/6995], lr: 0.000100, mv_avg_loss: 14.393792, loss: 13.456802
22:19:39.754733 Step[2600/6995], lr: 0.000100, mv_avg_loss: 14.541058, loss: 12.777194
22:19:44.519078 Step[2700/6995], lr: 0.000100, mv_avg_loss: 15.512762, loss: 8.998923
22:19:49.279288 Step[2800/6995], lr: 0.000100, mv_avg_loss: 14.279514, loss: 13.434916
22:19:54.040286 Step[2900/6995], lr: 0.000100, mv_avg_loss: 14.123528, loss: 13.138965
22:19:58.792222 Step[3000/6995], lr: 0.000100, mv_avg_loss: 15.825534, loss: 12.274958
22:20:03.562822 Step[3100/6995], lr: 0.000100, mv_avg_loss: 14.071180, loss: 18.386633
22:20:08.317125 Step[3200/6995], lr: 0.000100, mv_avg_loss: 13.492246, loss: 20.580044
22:20:13.070420 Step[3300/6995], lr: 0.000100, mv_avg_loss: 14.049048, loss: 12.053947
22:20:17.838250 Step[3400/6995], lr: 0.000100, mv_avg_loss: 14.513076, loss: 15.904221
22:20:22.588667 Step[3500/6995], lr: 0.000100, mv_avg_loss: 14.236354, loss: 13.555426
22:20:27.351586 Step[3600/6995], lr: 0.000100, mv_avg_loss: 13.277447, loss: 11.686809
22:20:32.105696 Step[3700/6995], lr: 0.000100, mv_avg_loss: 14.077646, loss: 14.543361
22:20:36.871198 Step[3800/6995], lr: 0.000100, mv_avg_loss: 14.670220, loss: 16.761414
22:20:41.630148 Step[3900/6995], lr: 0.000100, mv_avg_loss: 14.228252, loss: 16.448406
22:20:46.383005 Step[4000/6995], lr: 0.000100, mv_avg_loss: 12.999078, loss: 13.258895
22:20:51.133422 Step[4100/6995], lr: 0.000100, mv_avg_loss: 14.505140, loss: 9.010637
22:20:55.900090 Step[4200/6995], lr: 0.000100, mv_avg_loss: 14.933747, loss: 15.154408
22:21:00.657185 Step[4300/6995], lr: 0.000100, mv_avg_loss: 14.445004, loss: 26.494526
22:21:05.403327 Step[4400/6995], lr: 0.000100, mv_avg_loss: 14.659841, loss: 10.523934
22:21:10.168883 Step[4500/6995], lr: 0.000100, mv_avg_loss: 15.138850, loss: 20.994415
22:21:14.928790 Step[4600/6995], lr: 0.000100, mv_avg_loss: 14.123659, loss: 14.869080
22:21:19.687847 Step[4700/6995], lr: 0.000100, mv_avg_loss: 15.209538, loss: 17.814878
22:21:24.431579 Step[4800/6995], lr: 0.000100, mv_avg_loss: 14.724637, loss: 18.930996
22:21:29.195690 Step[4900/6995], lr: 0.000100, mv_avg_loss: 14.595193, loss: 11.901454
22:21:33.952462 Step[5000/6995], lr: 0.000100, mv_avg_loss: 13.476271, loss: 13.567762
22:21:38.706515 Step[5100/6995], lr: 0.000100, mv_avg_loss: 14.481999, loss: 11.144699
22:21:43.451107 Step[5200/6995], lr: 0.000100, mv_avg_loss: 15.372474, loss: 12.299988
22:21:48.235517 Step[5300/6995], lr: 0.000100, mv_avg_loss: 14.022853, loss: 15.434482
22:21:52.984147 Step[5400/6995], lr: 0.000100, mv_avg_loss: 13.879791, loss: 9.564501
22:21:57.727510 Step[5500/6995], lr: 0.000100, mv_avg_loss: 13.904346, loss: 16.575169
22:22:02.478737 Step[5600/6995], lr: 0.000100, mv_avg_loss: 13.880468, loss: 9.172175
22:22:07.236044 Step[5700/6995], lr: 0.000100, mv_avg_loss: 15.095858, loss: 30.496513
22:22:11.993075 Step[5800/6995], lr: 0.000100, mv_avg_loss: 14.688901, loss: 15.733099
22:22:16.748653 Step[5900/6995], lr: 0.000100, mv_avg_loss: 14.204961, loss: 11.780979
22:22:21.501373 Step[6000/6995], lr: 0.000100, mv_avg_loss: 14.957809, loss: 17.586903
22:22:26.243621 Step[6100/6995], lr: 0.000100, mv_avg_loss: 14.715691, loss: 12.732819
22:22:30.983000 Step[6200/6995], lr: 0.000100, mv_avg_loss: 16.031065, loss: 13.194054
22:22:35.728293 Step[6300/6995], lr: 0.000100, mv_avg_loss: 14.359821, loss: 11.183264
22:22:40.476909 Step[6400/6995], lr: 0.000100, mv_avg_loss: 14.600928, loss: 11.027832
22:22:45.224025 Step[6500/6995], lr: 0.000100, mv_avg_loss: 13.870809, loss: 9.213913
22:22:49.967114 Step[6600/6995], lr: 0.000100, mv_avg_loss: 14.673030, loss: 10.722156
22:22:54.721216 Step[6700/6995], lr: 0.000100, mv_avg_loss: 13.768769, loss: 13.650146
22:22:59.475919 Step[6800/6995], lr: 0.000100, mv_avg_loss: 14.184487, loss: 14.183319
22:23:04.224790 Step[6900/6995], lr: 0.000100, mv_avg_loss: 14.491776, loss: 14.998217
Testing for epoch: 55
Average test PNSR is 25.366041 for 500 images
Start to train epoch 56
22:23:31.097071 Step[0/6995], lr: 0.000100, mv_avg_loss: 13.533079, loss: 13.657915
22:23:35.819865 Step[100/6995], lr: 0.000100, mv_avg_loss: 14.536494, loss: 20.840893
22:23:40.550042 Step[200/6995], lr: 0.000100, mv_avg_loss: 14.503431, loss: 15.179298
22:23:45.279860 Step[300/6995], lr: 0.000100, mv_avg_loss: 13.467756, loss: 10.164600
22:23:50.001324 Step[400/6995], lr: 0.000100, mv_avg_loss: 14.023590, loss: 11.171371
22:23:54.729778 Step[500/6995], lr: 0.000100, mv_avg_loss: 13.330626, loss: 11.320704
22:23:59.481509 Step[600/6995], lr: 0.000100, mv_avg_loss: 13.768865, loss: 11.068748
22:24:04.223821 Step[700/6995], lr: 0.000100, mv_avg_loss: 14.704444, loss: 13.898428
22:24:08.956677 Step[800/6995], lr: 0.000100, mv_avg_loss: 14.777411, loss: 15.200560
22:24:13.698727 Step[900/6995], lr: 0.000100, mv_avg_loss: 15.528081, loss: 9.312194
22:24:18.445864 Step[1000/6995], lr: 0.000100, mv_avg_loss: 14.894938, loss: 13.033084
22:24:23.200689 Step[1100/6995], lr: 0.000100, mv_avg_loss: 13.947816, loss: 13.440928
22:24:27.948311 Step[1200/6995], lr: 0.000100, mv_avg_loss: 15.022580, loss: 12.557776
22:24:32.703652 Step[1300/6995], lr: 0.000100, mv_avg_loss: 13.658137, loss: 12.567294
22:24:37.454052 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.188386, loss: 16.155212
22:24:42.195893 Step[1500/6995], lr: 0.000100, mv_avg_loss: 13.289948, loss: 9.681917
22:24:46.938535 Step[1600/6995], lr: 0.000100, mv_avg_loss: 13.958955, loss: 9.951565
22:24:51.683711 Step[1700/6995], lr: 0.000100, mv_avg_loss: 14.923168, loss: 9.844962
22:24:56.435678 Step[1800/6995], lr: 0.000100, mv_avg_loss: 13.914477, loss: 19.521820
22:25:01.182861 Step[1900/6995], lr: 0.000100, mv_avg_loss: 14.470299, loss: 13.219244
22:25:05.937132 Step[2000/6995], lr: 0.000100, mv_avg_loss: 14.296535, loss: 11.263016
22:25:10.697670 Step[2100/6995], lr: 0.000100, mv_avg_loss: 14.178023, loss: 8.460407
22:25:15.447646 Step[2200/6995], lr: 0.000100, mv_avg_loss: 13.618557, loss: 10.501304
22:25:20.207754 Step[2300/6995], lr: 0.000100, mv_avg_loss: 13.110208, loss: 8.878421
22:25:24.965291 Step[2400/6995], lr: 0.000100, mv_avg_loss: 14.283855, loss: 19.377975
22:25:29.726473 Step[2500/6995], lr: 0.000100, mv_avg_loss: 15.084468, loss: 9.241655
22:25:34.498965 Step[2600/6995], lr: 0.000100, mv_avg_loss: 14.309258, loss: 15.047640
22:25:39.262530 Step[2700/6995], lr: 0.000100, mv_avg_loss: 15.227614, loss: 20.373550
22:25:44.029434 Step[2800/6995], lr: 0.000100, mv_avg_loss: 13.917159, loss: 16.194714
22:25:48.783102 Step[2900/6995], lr: 0.000100, mv_avg_loss: 13.426509, loss: 17.851082
22:25:53.549185 Step[3000/6995], lr: 0.000100, mv_avg_loss: 13.871789, loss: 16.077600
22:25:58.304477 Step[3100/6995], lr: 0.000100, mv_avg_loss: 15.822119, loss: 15.843735
22:26:03.062177 Step[3200/6995], lr: 0.000100, mv_avg_loss: 13.968652, loss: 12.174731
22:26:07.841263 Step[3300/6995], lr: 0.000100, mv_avg_loss: 14.627644, loss: 21.195877
22:26:12.597159 Step[3400/6995], lr: 0.000100, mv_avg_loss: 14.673920, loss: 14.881344
22:26:17.346003 Step[3500/6995], lr: 0.000100, mv_avg_loss: 13.058969, loss: 8.298221
22:26:22.111562 Step[3600/6995], lr: 0.000100, mv_avg_loss: 13.366757, loss: 33.303055
22:26:26.855625 Step[3700/6995], lr: 0.000100, mv_avg_loss: 14.517200, loss: 17.792259
22:26:31.619970 Step[3800/6995], lr: 0.000100, mv_avg_loss: 13.734576, loss: 9.814307
22:26:36.378178 Step[3900/6995], lr: 0.000100, mv_avg_loss: 14.362791, loss: 12.696810
22:26:41.130089 Step[4000/6995], lr: 0.000100, mv_avg_loss: 13.726872, loss: 9.682646
22:26:45.896631 Step[4100/6995], lr: 0.000100, mv_avg_loss: 14.509776, loss: 23.786551
22:26:50.659927 Step[4200/6995], lr: 0.000100, mv_avg_loss: 12.836173, loss: 14.120926
22:26:55.409036 Step[4300/6995], lr: 0.000100, mv_avg_loss: 14.486296, loss: 11.768486
22:27:00.154521 Step[4400/6995], lr: 0.000100, mv_avg_loss: 16.068981, loss: 10.308521
22:27:04.929956 Step[4500/6995], lr: 0.000100, mv_avg_loss: 14.505800, loss: 10.912116
22:27:09.693235 Step[4600/6995], lr: 0.000100, mv_avg_loss: 13.899287, loss: 26.537796
22:27:14.459768 Step[4700/6995], lr: 0.000100, mv_avg_loss: 15.544381, loss: 8.366295
22:27:19.222648 Step[4800/6995], lr: 0.000100, mv_avg_loss: 14.414225, loss: 23.962597
22:27:23.993655 Step[4900/6995], lr: 0.000100, mv_avg_loss: 14.427334, loss: 28.986004
22:27:28.768955 Step[5000/6995], lr: 0.000100, mv_avg_loss: 14.943373, loss: 11.478010
22:27:33.544759 Step[5100/6995], lr: 0.000100, mv_avg_loss: 14.951763, loss: 16.093969
22:27:38.324452 Step[5200/6995], lr: 0.000100, mv_avg_loss: 14.809728, loss: 10.854031
22:27:43.092759 Step[5300/6995], lr: 0.000100, mv_avg_loss: 15.132751, loss: 9.240834
22:27:47.850025 Step[5400/6995], lr: 0.000100, mv_avg_loss: 13.424764, loss: 13.732986
22:27:52.614157 Step[5500/6995], lr: 0.000100, mv_avg_loss: 14.221972, loss: 14.146383
22:27:57.381234 Step[5600/6995], lr: 0.000100, mv_avg_loss: 14.115189, loss: 13.710200
22:28:02.145095 Step[5700/6995], lr: 0.000100, mv_avg_loss: 13.692278, loss: 19.992050
22:28:06.908067 Step[5800/6995], lr: 0.000100, mv_avg_loss: 14.959726, loss: 16.256718
22:28:11.669733 Step[5900/6995], lr: 0.000100, mv_avg_loss: 14.391273, loss: 13.608862
22:28:16.431411 Step[6000/6995], lr: 0.000100, mv_avg_loss: 15.411691, loss: 18.925091
22:28:21.218090 Step[6100/6995], lr: 0.000100, mv_avg_loss: 13.641085, loss: 17.646172
22:28:25.984348 Step[6200/6995], lr: 0.000100, mv_avg_loss: 14.736861, loss: 11.114963
22:28:30.747878 Step[6300/6995], lr: 0.000100, mv_avg_loss: 13.032912, loss: 10.596030
22:28:35.517702 Step[6400/6995], lr: 0.000100, mv_avg_loss: 14.824154, loss: 10.137513
22:28:40.292225 Step[6500/6995], lr: 0.000100, mv_avg_loss: 13.629108, loss: 15.690036
22:28:45.067245 Step[6600/6995], lr: 0.000100, mv_avg_loss: 14.647071, loss: 14.396075
22:28:49.836468 Step[6700/6995], lr: 0.000100, mv_avg_loss: 14.160054, loss: 12.451789
22:28:54.610308 Step[6800/6995], lr: 0.000100, mv_avg_loss: 14.911165, loss: 16.736580
22:28:59.382487 Step[6900/6995], lr: 0.000100, mv_avg_loss: 14.032118, loss: 13.639624
Testing for epoch: 56
Average test PNSR is 25.371163 for 500 images
Start to train epoch 57
22:29:26.248527 Step[0/6995], lr: 0.000100, mv_avg_loss: 13.566280, loss: 13.824745
22:29:31.016567 Step[100/6995], lr: 0.000100, mv_avg_loss: 16.329306, loss: 19.584423
22:29:35.788174 Step[200/6995], lr: 0.000100, mv_avg_loss: 13.334599, loss: 19.586697
22:29:40.550041 Step[300/6995], lr: 0.000100, mv_avg_loss: 13.871883, loss: 9.638192
22:29:45.317828 Step[400/6995], lr: 0.000100, mv_avg_loss: 14.157131, loss: 18.678545
22:29:50.082244 Step[500/6995], lr: 0.000100, mv_avg_loss: 14.174003, loss: 13.724521
22:29:54.845961 Step[600/6995], lr: 0.000100, mv_avg_loss: 13.739558, loss: 9.616674
22:29:59.619690 Step[700/6995], lr: 0.000100, mv_avg_loss: 13.833113, loss: 17.459393
22:30:04.400855 Step[800/6995], lr: 0.000100, mv_avg_loss: 15.437985, loss: 12.861130
22:30:09.170076 Step[900/6995], lr: 0.000100, mv_avg_loss: 14.362176, loss: 9.941820
22:30:13.935541 Step[1000/6995], lr: 0.000100, mv_avg_loss: 14.482169, loss: 13.066733
22:30:18.706872 Step[1100/6995], lr: 0.000100, mv_avg_loss: 13.743763, loss: 17.675856
22:30:23.476987 Step[1200/6995], lr: 0.000100, mv_avg_loss: 13.067306, loss: 9.966841
22:30:28.264462 Step[1300/6995], lr: 0.000100, mv_avg_loss: 14.111734, loss: 18.073620
22:30:33.042076 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.845237, loss: 8.337558
22:30:37.811290 Step[1500/6995], lr: 0.000100, mv_avg_loss: 13.980648, loss: 10.461242
22:30:42.592580 Step[1600/6995], lr: 0.000100, mv_avg_loss: 14.333924, loss: 17.720100
22:30:47.366109 Step[1700/6995], lr: 0.000100, mv_avg_loss: 14.566672, loss: 19.641365
22:30:52.152811 Step[1800/6995], lr: 0.000100, mv_avg_loss: 15.637048, loss: 10.640001
22:30:56.924335 Step[1900/6995], lr: 0.000100, mv_avg_loss: 14.233926, loss: 26.552549
22:31:01.706300 Step[2000/6995], lr: 0.000100, mv_avg_loss: 14.391358, loss: 10.009401
22:31:06.492834 Step[2100/6995], lr: 0.000100, mv_avg_loss: 14.395519, loss: 12.839149
22:31:11.275292 Step[2200/6995], lr: 0.000100, mv_avg_loss: 15.186203, loss: 18.429251
22:31:16.045023 Step[2300/6995], lr: 0.000100, mv_avg_loss: 12.461209, loss: 14.362440
22:31:20.827874 Step[2400/6995], lr: 0.000100, mv_avg_loss: 13.552347, loss: 16.885719
22:31:25.591445 Step[2500/6995], lr: 0.000100, mv_avg_loss: 13.456562, loss: 12.719054
22:31:30.365115 Step[2600/6995], lr: 0.000100, mv_avg_loss: 14.662031, loss: 18.009432
22:31:35.135283 Step[2700/6995], lr: 0.000100, mv_avg_loss: 13.420668, loss: 13.636406
22:31:39.904667 Step[2800/6995], lr: 0.000100, mv_avg_loss: 13.598500, loss: 11.122478
22:31:44.676769 Step[2900/6995], lr: 0.000100, mv_avg_loss: 14.944003, loss: 16.316282
22:31:49.447829 Step[3000/6995], lr: 0.000100, mv_avg_loss: 13.503096, loss: 15.340450
22:31:54.214725 Step[3100/6995], lr: 0.000100, mv_avg_loss: 13.685300, loss: 10.149473
22:31:58.974798 Step[3200/6995], lr: 0.000100, mv_avg_loss: 14.586435, loss: 13.090090
22:32:03.741347 Step[3300/6995], lr: 0.000100, mv_avg_loss: 14.301384, loss: 7.564241
22:32:08.514377 Step[3400/6995], lr: 0.000100, mv_avg_loss: 14.536103, loss: 7.787048
22:32:13.280650 Step[3500/6995], lr: 0.000100, mv_avg_loss: 13.247900, loss: 11.705791
22:32:18.063935 Step[3600/6995], lr: 0.000100, mv_avg_loss: 14.314240, loss: 10.793613
22:32:22.823194 Step[3700/6995], lr: 0.000100, mv_avg_loss: 14.612813, loss: 20.511938
22:32:27.596843 Step[3800/6995], lr: 0.000100, mv_avg_loss: 13.166749, loss: 14.339262
22:32:32.367890 Step[3900/6995], lr: 0.000100, mv_avg_loss: 13.866404, loss: 16.961082
22:32:37.145502 Step[4000/6995], lr: 0.000100, mv_avg_loss: 14.211423, loss: 19.155941
22:32:41.907537 Step[4100/6995], lr: 0.000100, mv_avg_loss: 14.284907, loss: 11.488117
22:32:46.663241 Step[4200/6995], lr: 0.000100, mv_avg_loss: 14.009149, loss: 18.912292
22:32:51.429603 Step[4300/6995], lr: 0.000100, mv_avg_loss: 14.967272, loss: 10.351499
22:32:56.197026 Step[4400/6995], lr: 0.000100, mv_avg_loss: 14.539277, loss: 21.083635
22:33:00.964707 Step[4500/6995], lr: 0.000100, mv_avg_loss: 15.332367, loss: 14.496654
22:33:05.729253 Step[4600/6995], lr: 0.000100, mv_avg_loss: 14.665472, loss: 8.449322
22:33:10.489455 Step[4700/6995], lr: 0.000100, mv_avg_loss: 15.631273, loss: 9.296109
22:33:15.257003 Step[4800/6995], lr: 0.000100, mv_avg_loss: 13.796148, loss: 11.654590
22:33:20.023515 Step[4900/6995], lr: 0.000100, mv_avg_loss: 14.148495, loss: 18.016554
22:33:24.811758 Step[5000/6995], lr: 0.000100, mv_avg_loss: 13.850529, loss: 8.693306
22:33:29.569587 Step[5100/6995], lr: 0.000100, mv_avg_loss: 13.972330, loss: 11.633446
22:33:34.325268 Step[5200/6995], lr: 0.000100, mv_avg_loss: 13.878357, loss: 13.182639
22:33:39.091395 Step[5300/6995], lr: 0.000100, mv_avg_loss: 14.686612, loss: 20.332668
22:33:43.863605 Step[5400/6995], lr: 0.000100, mv_avg_loss: 12.958878, loss: 13.317411
22:33:48.619557 Step[5500/6995], lr: 0.000100, mv_avg_loss: 14.976398, loss: 12.273330
22:33:53.382511 Step[5600/6995], lr: 0.000100, mv_avg_loss: 14.135405, loss: 10.599871
22:33:58.133767 Step[5700/6995], lr: 0.000100, mv_avg_loss: 14.299924, loss: 16.501568
22:34:02.888839 Step[5800/6995], lr: 0.000100, mv_avg_loss: 13.045860, loss: 21.369289
22:34:07.651047 Step[5900/6995], lr: 0.000100, mv_avg_loss: 13.677229, loss: 19.907730
22:34:12.410605 Step[6000/6995], lr: 0.000100, mv_avg_loss: 14.384604, loss: 11.037628
22:34:17.171967 Step[6100/6995], lr: 0.000100, mv_avg_loss: 15.049472, loss: 13.252094
22:34:21.925864 Step[6200/6995], lr: 0.000100, mv_avg_loss: 13.911055, loss: 9.772408
22:34:26.720472 Step[6300/6995], lr: 0.000100, mv_avg_loss: 12.447483, loss: 8.918407
22:34:31.480458 Step[6400/6995], lr: 0.000100, mv_avg_loss: 15.589256, loss: 15.135406
22:34:36.238994 Step[6500/6995], lr: 0.000100, mv_avg_loss: 13.437226, loss: 17.903311
22:34:40.988330 Step[6600/6995], lr: 0.000100, mv_avg_loss: 14.297753, loss: 10.169285
22:34:45.745027 Step[6700/6995], lr: 0.000100, mv_avg_loss: 14.904887, loss: 21.711376
22:34:50.492454 Step[6800/6995], lr: 0.000100, mv_avg_loss: 13.270017, loss: 12.974365
22:34:55.248547 Step[6900/6995], lr: 0.000100, mv_avg_loss: 13.412972, loss: 13.754517
Testing for epoch: 57
Average test PNSR is 25.359901 for 500 images
Start to train epoch 58
22:35:22.172466 Step[0/6995], lr: 0.000100, mv_avg_loss: 12.530308, loss: 13.366087
22:35:26.896558 Step[100/6995], lr: 0.000100, mv_avg_loss: 14.046719, loss: 22.438084
22:35:31.631728 Step[200/6995], lr: 0.000100, mv_avg_loss: 13.542835, loss: 14.897734
22:35:36.343545 Step[300/6995], lr: 0.000100, mv_avg_loss: 14.035192, loss: 11.253668
22:35:41.075585 Step[400/6995], lr: 0.000100, mv_avg_loss: 12.989150, loss: 12.183107
22:35:45.791616 Step[500/6995], lr: 0.000100, mv_avg_loss: 14.029746, loss: 19.677752
22:35:50.524159 Step[600/6995], lr: 0.000100, mv_avg_loss: 14.156739, loss: 8.512911
22:35:55.264196 Step[700/6995], lr: 0.000100, mv_avg_loss: 14.076080, loss: 18.128513
22:35:59.995892 Step[800/6995], lr: 0.000100, mv_avg_loss: 13.653273, loss: 10.020587
22:36:04.722130 Step[900/6995], lr: 0.000100, mv_avg_loss: 15.303402, loss: 19.400101
22:36:09.454324 Step[1000/6995], lr: 0.000100, mv_avg_loss: 14.207171, loss: 13.923625
22:36:14.181088 Step[1100/6995], lr: 0.000100, mv_avg_loss: 14.042669, loss: 12.116164
22:36:18.917472 Step[1200/6995], lr: 0.000100, mv_avg_loss: 14.537765, loss: 16.449516
22:36:23.644366 Step[1300/6995], lr: 0.000100, mv_avg_loss: 14.828705, loss: 17.225256
22:36:28.374313 Step[1400/6995], lr: 0.000100, mv_avg_loss: 15.608259, loss: 16.076241
22:36:33.108107 Step[1500/6995], lr: 0.000100, mv_avg_loss: 14.209282, loss: 10.773245
22:36:37.848281 Step[1600/6995], lr: 0.000100, mv_avg_loss: 14.513507, loss: 17.529577
22:36:42.578429 Step[1700/6995], lr: 0.000100, mv_avg_loss: 14.014030, loss: 9.483373
22:36:47.310441 Step[1800/6995], lr: 0.000100, mv_avg_loss: 14.044012, loss: 10.910254
22:36:52.049810 Step[1900/6995], lr: 0.000100, mv_avg_loss: 14.445131, loss: 12.156479
22:36:56.780235 Step[2000/6995], lr: 0.000100, mv_avg_loss: 13.826643, loss: 10.250229
22:37:01.510930 Step[2100/6995], lr: 0.000100, mv_avg_loss: 14.979377, loss: 9.050332
22:37:06.240556 Step[2200/6995], lr: 0.000100, mv_avg_loss: 13.589090, loss: 10.889820
22:37:10.994979 Step[2300/6995], lr: 0.000100, mv_avg_loss: 14.244481, loss: 16.691166
22:37:15.744705 Step[2400/6995], lr: 0.000100, mv_avg_loss: 13.743736, loss: 20.912666
22:37:20.485205 Step[2500/6995], lr: 0.000100, mv_avg_loss: 14.644641, loss: 13.580856
22:37:25.229956 Step[2600/6995], lr: 0.000100, mv_avg_loss: 15.464696, loss: 14.036512
22:37:29.983178 Step[2700/6995], lr: 0.000100, mv_avg_loss: 13.435813, loss: 10.200100
22:37:34.741383 Step[2800/6995], lr: 0.000100, mv_avg_loss: 14.086483, loss: 9.502687
22:37:39.488625 Step[2900/6995], lr: 0.000100, mv_avg_loss: 14.174659, loss: 12.038958
22:37:44.244081 Step[3000/6995], lr: 0.000100, mv_avg_loss: 15.013206, loss: 18.227007
22:37:48.988011 Step[3100/6995], lr: 0.000100, mv_avg_loss: 13.298882, loss: 11.564204
22:37:53.739303 Step[3200/6995], lr: 0.000100, mv_avg_loss: 15.182515, loss: 14.189835
22:37:58.494494 Step[3300/6995], lr: 0.000100, mv_avg_loss: 14.663889, loss: 16.341164
22:38:03.246606 Step[3400/6995], lr: 0.000100, mv_avg_loss: 13.921921, loss: 17.770836
22:38:07.982371 Step[3500/6995], lr: 0.000100, mv_avg_loss: 14.145798, loss: 13.746048
22:38:12.724984 Step[3600/6995], lr: 0.000100, mv_avg_loss: 13.668252, loss: 9.405856
22:38:17.479783 Step[3700/6995], lr: 0.000100, mv_avg_loss: 13.975958, loss: 17.484932
22:38:22.217815 Step[3800/6995], lr: 0.000100, mv_avg_loss: 13.690414, loss: 20.608078
22:38:26.968504 Step[3900/6995], lr: 0.000100, mv_avg_loss: 14.751312, loss: 16.825687
22:38:31.731537 Step[4000/6995], lr: 0.000100, mv_avg_loss: 14.016486, loss: 10.393680
22:38:36.470754 Step[4100/6995], lr: 0.000100, mv_avg_loss: 13.990751, loss: 9.964725
22:38:41.228161 Step[4200/6995], lr: 0.000100, mv_avg_loss: 13.989769, loss: 9.404673
22:38:46.007800 Step[4300/6995], lr: 0.000100, mv_avg_loss: 14.070389, loss: 12.056735
22:38:50.761557 Step[4400/6995], lr: 0.000100, mv_avg_loss: 14.031363, loss: 17.496782
22:38:55.521333 Step[4500/6995], lr: 0.000100, mv_avg_loss: 13.238910, loss: 13.341231
22:39:00.276625 Step[4600/6995], lr: 0.000100, mv_avg_loss: 14.398793, loss: 9.615504
22:39:05.033278 Step[4700/6995], lr: 0.000100, mv_avg_loss: 14.449402, loss: 17.656919
22:39:09.788549 Step[4800/6995], lr: 0.000100, mv_avg_loss: 15.532384, loss: 11.058349
22:39:14.543919 Step[4900/6995], lr: 0.000100, mv_avg_loss: 14.657752, loss: 9.338497
22:39:19.304298 Step[5000/6995], lr: 0.000100, mv_avg_loss: 14.274659, loss: 16.023306
22:39:24.056884 Step[5100/6995], lr: 0.000100, mv_avg_loss: 14.086258, loss: 16.674959
22:39:28.809677 Step[5200/6995], lr: 0.000100, mv_avg_loss: 14.224784, loss: 13.452059
22:39:33.560201 Step[5300/6995], lr: 0.000100, mv_avg_loss: 14.548895, loss: 12.113096
22:39:38.309593 Step[5400/6995], lr: 0.000100, mv_avg_loss: 13.987651, loss: 9.415168
22:39:43.063193 Step[5500/6995], lr: 0.000100, mv_avg_loss: 13.318535, loss: 13.744608
22:39:47.821884 Step[5600/6995], lr: 0.000100, mv_avg_loss: 14.131068, loss: 11.384809
22:39:52.571107 Step[5700/6995], lr: 0.000100, mv_avg_loss: 13.580166, loss: 9.296715
22:39:57.330296 Step[5800/6995], lr: 0.000100, mv_avg_loss: 14.411708, loss: 17.327337
22:40:02.098754 Step[5900/6995], lr: 0.000100, mv_avg_loss: 14.551531, loss: 11.754195
22:40:06.853473 Step[6000/6995], lr: 0.000100, mv_avg_loss: 14.943687, loss: 16.715015
22:40:11.619613 Step[6100/6995], lr: 0.000100, mv_avg_loss: 14.573802, loss: 12.081457
22:40:16.365628 Step[6200/6995], lr: 0.000100, mv_avg_loss: 15.074679, loss: 20.796873
22:40:21.119709 Step[6300/6995], lr: 0.000100, mv_avg_loss: 14.843822, loss: 11.092001
22:40:25.890960 Step[6400/6995], lr: 0.000100, mv_avg_loss: 14.328895, loss: 14.495109
22:40:30.648088 Step[6500/6995], lr: 0.000100, mv_avg_loss: 14.419870, loss: 13.087721
22:40:35.406291 Step[6600/6995], lr: 0.000100, mv_avg_loss: 13.123053, loss: 11.530135
22:40:40.168600 Step[6700/6995], lr: 0.000100, mv_avg_loss: 13.626317, loss: 10.697783
22:40:44.919828 Step[6800/6995], lr: 0.000100, mv_avg_loss: 14.283880, loss: 9.279621
22:40:49.674436 Step[6900/6995], lr: 0.000100, mv_avg_loss: 13.899695, loss: 12.160282
Testing for epoch: 58
Average test PNSR is 25.380637 for 500 images
Start to train epoch 59
22:41:16.589837 Step[0/6995], lr: 0.000100, mv_avg_loss: 14.629789, loss: 12.517714
22:41:21.348276 Step[100/6995], lr: 0.000100, mv_avg_loss: 13.205721, loss: 9.965518
22:41:26.100109 Step[200/6995], lr: 0.000100, mv_avg_loss: 13.612732, loss: 11.479218
22:41:30.857928 Step[300/6995], lr: 0.000100, mv_avg_loss: 14.277117, loss: 12.799065
22:41:35.625420 Step[400/6995], lr: 0.000100, mv_avg_loss: 13.854609, loss: 20.040686
22:41:40.397186 Step[500/6995], lr: 0.000100, mv_avg_loss: 13.501095, loss: 7.533771
22:41:45.160913 Step[600/6995], lr: 0.000100, mv_avg_loss: 13.985247, loss: 21.791250
22:41:49.935037 Step[700/6995], lr: 0.000100, mv_avg_loss: 13.488187, loss: 17.863903
22:41:54.700411 Step[800/6995], lr: 0.000100, mv_avg_loss: 13.664670, loss: 10.027390
22:41:59.467767 Step[900/6995], lr: 0.000100, mv_avg_loss: 13.873353, loss: 12.633304
22:42:04.231553 Step[1000/6995], lr: 0.000100, mv_avg_loss: 13.352465, loss: 9.444141
22:42:08.992700 Step[1100/6995], lr: 0.000100, mv_avg_loss: 14.532145, loss: 24.183760
22:42:13.760080 Step[1200/6995], lr: 0.000100, mv_avg_loss: 13.464210, loss: 7.961379
22:42:18.524092 Step[1300/6995], lr: 0.000100, mv_avg_loss: 12.986587, loss: 8.172503
22:42:23.280844 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.599746, loss: 18.380451
22:42:28.042485 Step[1500/6995], lr: 0.000100, mv_avg_loss: 13.114869, loss: 20.359884
22:42:32.808779 Step[1600/6995], lr: 0.000100, mv_avg_loss: 13.649637, loss: 12.967715
22:42:37.569764 Step[1700/6995], lr: 0.000100, mv_avg_loss: 13.839876, loss: 13.960658
22:42:42.333554 Step[1800/6995], lr: 0.000100, mv_avg_loss: 13.318540, loss: 11.005221
22:42:47.095531 Step[1900/6995], lr: 0.000100, mv_avg_loss: 14.032169, loss: 11.911400
22:42:51.870709 Step[2000/6995], lr: 0.000100, mv_avg_loss: 14.420081, loss: 9.719509
22:42:56.639526 Step[2100/6995], lr: 0.000100, mv_avg_loss: 13.450692, loss: 14.826052
22:43:01.401728 Step[2200/6995], lr: 0.000100, mv_avg_loss: 13.194564, loss: 9.584881
22:43:06.204875 Step[2300/6995], lr: 0.000100, mv_avg_loss: 13.689390, loss: 18.001804
22:43:10.979686 Step[2400/6995], lr: 0.000100, mv_avg_loss: 14.198547, loss: 20.558340
22:43:15.753636 Step[2500/6995], lr: 0.000100, mv_avg_loss: 14.449829, loss: 13.154615
22:43:20.536750 Step[2600/6995], lr: 0.000100, mv_avg_loss: 13.714089, loss: 10.902184
22:43:25.306711 Step[2700/6995], lr: 0.000100, mv_avg_loss: 12.977673, loss: 9.357126
22:43:30.066100 Step[2800/6995], lr: 0.000100, mv_avg_loss: 13.889915, loss: 17.262161
22:43:34.846824 Step[2900/6995], lr: 0.000100, mv_avg_loss: 14.329100, loss: 9.529125
22:43:39.617916 Step[3000/6995], lr: 0.000100, mv_avg_loss: 15.214351, loss: 6.985155
22:43:44.399735 Step[3100/6995], lr: 0.000100, mv_avg_loss: 14.399004, loss: 12.335114
22:43:49.169220 Step[3200/6995], lr: 0.000100, mv_avg_loss: 14.761846, loss: 14.880157
22:43:53.934191 Step[3300/6995], lr: 0.000100, mv_avg_loss: 15.343136, loss: 15.012138
22:43:58.710416 Step[3400/6995], lr: 0.000100, mv_avg_loss: 14.932156, loss: 8.263774
22:44:03.483456 Step[3500/6995], lr: 0.000100, mv_avg_loss: 14.402864, loss: 10.716194
22:44:08.248364 Step[3600/6995], lr: 0.000100, mv_avg_loss: 13.569327, loss: 14.449362
22:44:13.027523 Step[3700/6995], lr: 0.000100, mv_avg_loss: 13.361997, loss: 9.498908
22:44:17.803909 Step[3800/6995], lr: 0.000100, mv_avg_loss: 14.402458, loss: 10.657464
22:44:22.580771 Step[3900/6995], lr: 0.000100, mv_avg_loss: 13.466002, loss: 12.243426
22:44:27.343748 Step[4000/6995], lr: 0.000100, mv_avg_loss: 13.654842, loss: 19.242081
22:44:32.122297 Step[4100/6995], lr: 0.000100, mv_avg_loss: 14.259339, loss: 11.600079
22:44:36.905888 Step[4200/6995], lr: 0.000100, mv_avg_loss: 13.132904, loss: 8.753533
22:44:41.673165 Step[4300/6995], lr: 0.000100, mv_avg_loss: 13.799316, loss: 10.149952
22:44:46.452732 Step[4400/6995], lr: 0.000100, mv_avg_loss: 13.553747, loss: 15.504644
22:44:51.224869 Step[4500/6995], lr: 0.000100, mv_avg_loss: 13.371256, loss: 15.154613
22:44:55.999534 Step[4600/6995], lr: 0.000100, mv_avg_loss: 13.629784, loss: 13.734965
22:45:00.755699 Step[4700/6995], lr: 0.000100, mv_avg_loss: 15.366967, loss: 9.896929
22:45:05.538546 Step[4800/6995], lr: 0.000100, mv_avg_loss: 13.695338, loss: 14.691439
22:45:10.312030 Step[4900/6995], lr: 0.000100, mv_avg_loss: 14.073156, loss: 12.085624
22:45:15.092186 Step[5000/6995], lr: 0.000100, mv_avg_loss: 13.835026, loss: 11.973933
22:45:19.872767 Step[5100/6995], lr: 0.000100, mv_avg_loss: 14.253850, loss: 14.211485
22:45:24.643049 Step[5200/6995], lr: 0.000100, mv_avg_loss: 13.931787, loss: 9.239340
22:45:29.420239 Step[5300/6995], lr: 0.000100, mv_avg_loss: 13.054674, loss: 19.938232
22:45:34.190016 Step[5400/6995], lr: 0.000100, mv_avg_loss: 12.947989, loss: 8.148717
22:45:38.956637 Step[5500/6995], lr: 0.000100, mv_avg_loss: 14.142646, loss: 16.827648
22:45:43.737374 Step[5600/6995], lr: 0.000100, mv_avg_loss: 15.128033, loss: 6.748703
22:45:48.512205 Step[5700/6995], lr: 0.000100, mv_avg_loss: 13.860087, loss: 10.602176
22:45:53.275486 Step[5800/6995], lr: 0.000100, mv_avg_loss: 14.453503, loss: 12.073351
22:45:58.033234 Step[5900/6995], lr: 0.000100, mv_avg_loss: 13.754783, loss: 10.208094
22:46:02.806882 Step[6000/6995], lr: 0.000100, mv_avg_loss: 13.673279, loss: 11.927925
22:46:07.583667 Step[6100/6995], lr: 0.000100, mv_avg_loss: 12.562618, loss: 10.394574
22:46:12.348121 Step[6200/6995], lr: 0.000100, mv_avg_loss: 14.311877, loss: 17.881256
22:46:17.118004 Step[6300/6995], lr: 0.000100, mv_avg_loss: 12.990884, loss: 12.109289
22:46:21.884154 Step[6400/6995], lr: 0.000100, mv_avg_loss: 14.483815, loss: 19.488028
22:46:26.656349 Step[6500/6995], lr: 0.000100, mv_avg_loss: 14.425076, loss: 9.688311
22:46:31.416524 Step[6600/6995], lr: 0.000100, mv_avg_loss: 13.369671, loss: 8.921721
22:46:36.170341 Step[6700/6995], lr: 0.000100, mv_avg_loss: 13.401346, loss: 14.512815
22:46:40.943967 Step[6800/6995], lr: 0.000100, mv_avg_loss: 15.040506, loss: 11.015006
22:46:45.713727 Step[6900/6995], lr: 0.000100, mv_avg_loss: 15.081775, loss: 8.785120
saving model for epoch 59
Testing for epoch: 59
Average test PNSR is 25.382232 for 500 images
Start to train epoch 60
22:47:12.308609 Step[0/6995], lr: 0.000100, mv_avg_loss: 14.679942, loss: 19.708897
22:47:17.073350 Step[100/6995], lr: 0.000100, mv_avg_loss: 13.259603, loss: 12.331245
22:47:21.847089 Step[200/6995], lr: 0.000100, mv_avg_loss: 12.827951, loss: 18.324383
22:47:26.629402 Step[300/6995], lr: 0.000100, mv_avg_loss: 13.576800, loss: 10.476217
22:47:31.387709 Step[400/6995], lr: 0.000100, mv_avg_loss: 13.109473, loss: 12.599516
22:47:36.146747 Step[500/6995], lr: 0.000100, mv_avg_loss: 14.605510, loss: 18.180698
22:47:40.900846 Step[600/6995], lr: 0.000100, mv_avg_loss: 13.032563, loss: 13.859744
22:47:45.652845 Step[700/6995], lr: 0.000100, mv_avg_loss: 13.953975, loss: 11.169979
22:47:50.410523 Step[800/6995], lr: 0.000100, mv_avg_loss: 14.307636, loss: 22.004711
22:47:55.171742 Step[900/6995], lr: 0.000100, mv_avg_loss: 13.146859, loss: 12.055685
22:47:59.941788 Step[1000/6995], lr: 0.000100, mv_avg_loss: 12.938533, loss: 12.198706
22:48:04.693867 Step[1100/6995], lr: 0.000100, mv_avg_loss: 14.243495, loss: 14.904896
22:48:09.449145 Step[1200/6995], lr: 0.000100, mv_avg_loss: 14.358339, loss: 19.888664
22:48:14.209348 Step[1300/6995], lr: 0.000100, mv_avg_loss: 13.663693, loss: 13.296478
22:48:18.975578 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.835212, loss: 15.806122
22:48:23.733892 Step[1500/6995], lr: 0.000100, mv_avg_loss: 13.380838, loss: 23.135063
22:48:28.492734 Step[1600/6995], lr: 0.000100, mv_avg_loss: 14.698613, loss: 16.974556
22:48:33.252555 Step[1700/6995], lr: 0.000100, mv_avg_loss: 14.536979, loss: 24.745611
22:48:38.014251 Step[1800/6995], lr: 0.000100, mv_avg_loss: 14.226808, loss: 12.042786
22:48:42.764076 Step[1900/6995], lr: 0.000100, mv_avg_loss: 14.584095, loss: 10.742722
22:48:47.511391 Step[2000/6995], lr: 0.000100, mv_avg_loss: 13.758861, loss: 10.871578
22:48:52.282924 Step[2100/6995], lr: 0.000100, mv_avg_loss: 14.219402, loss: 19.197384
22:48:57.053678 Step[2200/6995], lr: 0.000100, mv_avg_loss: 14.545208, loss: 10.751850
22:49:01.808373 Step[2300/6995], lr: 0.000100, mv_avg_loss: 14.018988, loss: 10.940952
22:49:06.564025 Step[2400/6995], lr: 0.000100, mv_avg_loss: 14.340829, loss: 17.310261
22:49:11.324354 Step[2500/6995], lr: 0.000100, mv_avg_loss: 14.349854, loss: 10.623758
22:49:16.076495 Step[2600/6995], lr: 0.000100, mv_avg_loss: 14.189956, loss: 13.561869
22:49:20.820410 Step[2700/6995], lr: 0.000100, mv_avg_loss: 14.316451, loss: 12.030697
22:49:25.585349 Step[2800/6995], lr: 0.000100, mv_avg_loss: 14.304313, loss: 12.077102
22:49:30.335457 Step[2900/6995], lr: 0.000100, mv_avg_loss: 13.720601, loss: 21.557829
22:49:35.083519 Step[3000/6995], lr: 0.000100, mv_avg_loss: 13.959522, loss: 20.785786
22:49:39.829507 Step[3100/6995], lr: 0.000100, mv_avg_loss: 12.951129, loss: 13.914177
22:49:44.586083 Step[3200/6995], lr: 0.000100, mv_avg_loss: 13.347335, loss: 11.221731
22:49:49.348849 Step[3300/6995], lr: 0.000100, mv_avg_loss: 14.345130, loss: 20.357599
22:49:54.109127 Step[3400/6995], lr: 0.000100, mv_avg_loss: 13.993279, loss: 16.952078
22:49:58.871675 Step[3500/6995], lr: 0.000100, mv_avg_loss: 12.575771, loss: 13.877300
22:50:03.628152 Step[3600/6995], lr: 0.000100, mv_avg_loss: 13.618383, loss: 15.653999
22:50:08.382569 Step[3700/6995], lr: 0.000100, mv_avg_loss: 14.135797, loss: 22.119949
22:50:13.135810 Step[3800/6995], lr: 0.000100, mv_avg_loss: 13.530610, loss: 22.032440
22:50:17.891821 Step[3900/6995], lr: 0.000100, mv_avg_loss: 14.458022, loss: 7.849358
22:50:22.637670 Step[4000/6995], lr: 0.000100, mv_avg_loss: 14.455331, loss: 10.015363
22:50:27.395954 Step[4100/6995], lr: 0.000100, mv_avg_loss: 13.676256, loss: 13.289714
22:50:32.149022 Step[4200/6995], lr: 0.000100, mv_avg_loss: 13.039455, loss: 9.793516
22:50:36.897955 Step[4300/6995], lr: 0.000100, mv_avg_loss: 13.847732, loss: 22.532431
22:50:41.651630 Step[4400/6995], lr: 0.000100, mv_avg_loss: 14.454029, loss: 20.805592
22:50:46.407624 Step[4500/6995], lr: 0.000100, mv_avg_loss: 13.221584, loss: 13.982527
22:50:51.166778 Step[4600/6995], lr: 0.000100, mv_avg_loss: 13.744200, loss: 16.693583
22:50:55.946614 Step[4700/6995], lr: 0.000100, mv_avg_loss: 14.200999, loss: 13.986532
22:51:00.697796 Step[4800/6995], lr: 0.000100, mv_avg_loss: 12.711505, loss: 15.418624
22:51:05.446978 Step[4900/6995], lr: 0.000100, mv_avg_loss: 14.055923, loss: 17.876413
22:51:10.187044 Step[5000/6995], lr: 0.000100, mv_avg_loss: 13.259725, loss: 12.458400
22:51:14.942989 Step[5100/6995], lr: 0.000100, mv_avg_loss: 13.300918, loss: 13.242575
22:51:19.693834 Step[5200/6995], lr: 0.000100, mv_avg_loss: 14.383939, loss: 10.477357
22:51:24.576386 Step[5300/6995], lr: 0.000100, mv_avg_loss: 14.064174, loss: 9.476782
22:51:29.316541 Step[5400/6995], lr: 0.000100, mv_avg_loss: 14.588019, loss: 20.555401
22:51:34.061210 Step[5500/6995], lr: 0.000100, mv_avg_loss: 13.817475, loss: 13.760685
22:51:38.804249 Step[5600/6995], lr: 0.000100, mv_avg_loss: 13.667565, loss: 17.459995
22:51:43.561189 Step[5700/6995], lr: 0.000100, mv_avg_loss: 14.193335, loss: 9.291368
22:51:48.311791 Step[5800/6995], lr: 0.000100, mv_avg_loss: 13.380215, loss: 12.543304
22:51:53.066162 Step[5900/6995], lr: 0.000100, mv_avg_loss: 14.210133, loss: 13.948878
22:51:57.818446 Step[6000/6995], lr: 0.000100, mv_avg_loss: 14.070696, loss: 12.403613
22:52:02.562636 Step[6100/6995], lr: 0.000100, mv_avg_loss: 14.639245, loss: 9.444462
22:52:07.319913 Step[6200/6995], lr: 0.000100, mv_avg_loss: 13.577003, loss: 11.225103
22:52:12.070193 Step[6300/6995], lr: 0.000100, mv_avg_loss: 14.445446, loss: 19.178871
22:52:16.828557 Step[6400/6995], lr: 0.000100, mv_avg_loss: 15.748552, loss: 21.124271
22:52:21.579090 Step[6500/6995], lr: 0.000100, mv_avg_loss: 13.764106, loss: 15.744894
22:52:26.329148 Step[6600/6995], lr: 0.000100, mv_avg_loss: 14.455120, loss: 14.617423
22:52:31.088212 Step[6700/6995], lr: 0.000100, mv_avg_loss: 15.807341, loss: 15.330917
22:52:35.839489 Step[6800/6995], lr: 0.000100, mv_avg_loss: 13.774652, loss: 13.955132
22:52:40.584391 Step[6900/6995], lr: 0.000100, mv_avg_loss: 13.826836, loss: 13.375869
Testing for epoch: 60
Average test PNSR is 25.356012 for 500 images
Start to train epoch 61
22:53:07.722419 Step[0/6995], lr: 0.000100, mv_avg_loss: 14.394292, loss: 13.709151
22:53:12.446780 Step[100/6995], lr: 0.000100, mv_avg_loss: 14.449578, loss: 16.740711
22:53:17.181417 Step[200/6995], lr: 0.000100, mv_avg_loss: 13.909748, loss: 14.925068
22:53:21.902863 Step[300/6995], lr: 0.000100, mv_avg_loss: 14.522344, loss: 11.947145
22:53:26.641117 Step[400/6995], lr: 0.000100, mv_avg_loss: 14.119127, loss: 12.079833
22:53:31.369448 Step[500/6995], lr: 0.000100, mv_avg_loss: 13.896507, loss: 14.092010
22:53:36.104073 Step[600/6995], lr: 0.000100, mv_avg_loss: 13.767052, loss: 11.799168
22:53:40.841452 Step[700/6995], lr: 0.000100, mv_avg_loss: 14.061846, loss: 13.355716
22:53:45.571417 Step[800/6995], lr: 0.000100, mv_avg_loss: 15.321523, loss: 22.928219
22:53:50.302086 Step[900/6995], lr: 0.000100, mv_avg_loss: 12.776116, loss: 14.154701
22:53:55.037426 Step[1000/6995], lr: 0.000100, mv_avg_loss: 13.849359, loss: 8.422452
22:53:59.768976 Step[1100/6995], lr: 0.000100, mv_avg_loss: 13.352548, loss: 14.081369
22:54:04.507225 Step[1200/6995], lr: 0.000100, mv_avg_loss: 14.807600, loss: 17.606884
22:54:09.255426 Step[1300/6995], lr: 0.000100, mv_avg_loss: 13.740626, loss: 14.259486
22:54:14.001415 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.980826, loss: 14.103582
22:54:18.747943 Step[1500/6995], lr: 0.000100, mv_avg_loss: 13.579930, loss: 8.899169
22:54:23.500707 Step[1600/6995], lr: 0.000100, mv_avg_loss: 13.417150, loss: 11.345847
22:54:28.252925 Step[1700/6995], lr: 0.000100, mv_avg_loss: 13.842907, loss: 17.150604
22:54:33.014716 Step[1800/6995], lr: 0.000100, mv_avg_loss: 14.084183, loss: 17.076723
22:54:37.768613 Step[1900/6995], lr: 0.000100, mv_avg_loss: 12.898130, loss: 11.339945
22:54:42.523306 Step[2000/6995], lr: 0.000100, mv_avg_loss: 13.825740, loss: 9.613889
22:54:47.281866 Step[2100/6995], lr: 0.000100, mv_avg_loss: 13.242337, loss: 13.303638
22:54:52.068307 Step[2200/6995], lr: 0.000100, mv_avg_loss: 13.756479, loss: 9.247665
22:54:56.839281 Step[2300/6995], lr: 0.000100, mv_avg_loss: 13.064542, loss: 14.830650
22:55:01.607193 Step[2400/6995], lr: 0.000100, mv_avg_loss: 14.973025, loss: 16.097589
22:55:06.356762 Step[2500/6995], lr: 0.000100, mv_avg_loss: 14.124155, loss: 18.062637
22:55:11.112999 Step[2600/6995], lr: 0.000100, mv_avg_loss: 13.468090, loss: 8.791948
22:55:15.861189 Step[2700/6995], lr: 0.000100, mv_avg_loss: 13.715525, loss: 7.702133
22:55:20.614849 Step[2800/6995], lr: 0.000100, mv_avg_loss: 13.563908, loss: 12.605967
22:55:25.369756 Step[2900/6995], lr: 0.000100, mv_avg_loss: 14.397151, loss: 16.566614
22:55:30.130642 Step[3000/6995], lr: 0.000100, mv_avg_loss: 13.640865, loss: 20.340015
22:55:34.891616 Step[3100/6995], lr: 0.000100, mv_avg_loss: 13.150366, loss: 16.925043
22:55:39.651376 Step[3200/6995], lr: 0.000100, mv_avg_loss: 13.669452, loss: 9.398508
22:55:44.407047 Step[3300/6995], lr: 0.000100, mv_avg_loss: 12.969399, loss: 11.661980
22:55:49.188885 Step[3400/6995], lr: 0.000100, mv_avg_loss: 14.749529, loss: 12.977322
22:55:53.952354 Step[3500/6995], lr: 0.000100, mv_avg_loss: 13.457015, loss: 12.226786
22:55:58.708647 Step[3600/6995], lr: 0.000100, mv_avg_loss: 12.800774, loss: 10.074954
22:56:03.473854 Step[3700/6995], lr: 0.000100, mv_avg_loss: 14.032918, loss: 12.385043
22:56:08.237045 Step[3800/6995], lr: 0.000100, mv_avg_loss: 14.414061, loss: 13.200621
22:56:12.990079 Step[3900/6995], lr: 0.000100, mv_avg_loss: 13.348308, loss: 13.386569
22:56:17.743253 Step[4000/6995], lr: 0.000100, mv_avg_loss: 13.958980, loss: 16.231441
22:56:22.503947 Step[4100/6995], lr: 0.000100, mv_avg_loss: 14.646776, loss: 19.148838
22:56:27.266107 Step[4200/6995], lr: 0.000100, mv_avg_loss: 14.284984, loss: 13.499403
22:56:32.032063 Step[4300/6995], lr: 0.000100, mv_avg_loss: 14.392672, loss: 12.314074
22:56:36.803686 Step[4400/6995], lr: 0.000100, mv_avg_loss: 14.060578, loss: 12.764503
22:56:41.581213 Step[4500/6995], lr: 0.000100, mv_avg_loss: 13.523587, loss: 11.701331
22:56:46.342675 Step[4600/6995], lr: 0.000100, mv_avg_loss: 14.166783, loss: 9.142092
22:56:51.108608 Step[4700/6995], lr: 0.000100, mv_avg_loss: 13.283775, loss: 10.196822
22:56:55.857549 Step[4800/6995], lr: 0.000100, mv_avg_loss: 13.084137, loss: 10.670876
22:57:00.624315 Step[4900/6995], lr: 0.000100, mv_avg_loss: 14.356223, loss: 18.129768
22:57:05.392913 Step[5000/6995], lr: 0.000100, mv_avg_loss: 13.573332, loss: 7.301904
22:57:10.152187 Step[5100/6995], lr: 0.000100, mv_avg_loss: 14.371746, loss: 17.406187
22:57:14.918058 Step[5200/6995], lr: 0.000100, mv_avg_loss: 14.491470, loss: 10.769468
22:57:19.693621 Step[5300/6995], lr: 0.000100, mv_avg_loss: 14.085978, loss: 16.500084
22:57:24.474020 Step[5400/6995], lr: 0.000100, mv_avg_loss: 14.108529, loss: 19.063107
22:57:29.252470 Step[5500/6995], lr: 0.000100, mv_avg_loss: 13.664376, loss: 13.932285
22:57:34.027845 Step[5600/6995], lr: 0.000100, mv_avg_loss: 14.693282, loss: 10.969130
22:57:38.794514 Step[5700/6995], lr: 0.000100, mv_avg_loss: 13.176245, loss: 15.136688
22:57:43.566502 Step[5800/6995], lr: 0.000100, mv_avg_loss: 12.637638, loss: 15.013548
22:57:48.348041 Step[5900/6995], lr: 0.000100, mv_avg_loss: 14.349972, loss: 26.667913
22:57:53.114705 Step[6000/6995], lr: 0.000100, mv_avg_loss: 15.460533, loss: 19.094173
22:57:57.885866 Step[6100/6995], lr: 0.000100, mv_avg_loss: 13.429134, loss: 12.674980
22:58:02.663572 Step[6200/6995], lr: 0.000100, mv_avg_loss: 14.195562, loss: 7.492982
22:58:07.437359 Step[6300/6995], lr: 0.000100, mv_avg_loss: 13.433894, loss: 12.454332
22:58:12.204632 Step[6400/6995], lr: 0.000100, mv_avg_loss: 13.231820, loss: 10.204577
22:58:16.977815 Step[6500/6995], lr: 0.000100, mv_avg_loss: 13.467909, loss: 10.929236
22:58:21.749683 Step[6600/6995], lr: 0.000100, mv_avg_loss: 13.620431, loss: 9.369975
22:58:26.533428 Step[6700/6995], lr: 0.000100, mv_avg_loss: 13.625416, loss: 13.326309
22:58:31.298769 Step[6800/6995], lr: 0.000100, mv_avg_loss: 12.882919, loss: 13.814020
22:58:36.075774 Step[6900/6995], lr: 0.000100, mv_avg_loss: 13.524041, loss: 14.017939
Testing for epoch: 61
Average test PNSR is 25.353747 for 500 images
Start to train epoch 62
22:59:02.884458 Step[0/6995], lr: 0.000100, mv_avg_loss: 12.623417, loss: 13.317205
22:59:07.640663 Step[100/6995], lr: 0.000100, mv_avg_loss: 13.368797, loss: 13.494090
22:59:12.417267 Step[200/6995], lr: 0.000100, mv_avg_loss: 13.451145, loss: 11.651968
22:59:17.180730 Step[300/6995], lr: 0.000100, mv_avg_loss: 13.127173, loss: 24.196510
22:59:21.938836 Step[400/6995], lr: 0.000100, mv_avg_loss: 13.644912, loss: 18.061447
22:59:26.712182 Step[500/6995], lr: 0.000100, mv_avg_loss: 13.519235, loss: 14.935217
22:59:31.465803 Step[600/6995], lr: 0.000100, mv_avg_loss: 14.530794, loss: 20.151421
22:59:36.216903 Step[700/6995], lr: 0.000100, mv_avg_loss: 13.306076, loss: 9.310479
22:59:40.975798 Step[800/6995], lr: 0.000100, mv_avg_loss: 14.668784, loss: 8.712921
22:59:45.743493 Step[900/6995], lr: 0.000100, mv_avg_loss: 13.884249, loss: 14.150412
22:59:50.516525 Step[1000/6995], lr: 0.000100, mv_avg_loss: 13.862164, loss: 11.379080
22:59:55.294215 Step[1100/6995], lr: 0.000100, mv_avg_loss: 13.248795, loss: 13.985861
23:00:00.059979 Step[1200/6995], lr: 0.000100, mv_avg_loss: 13.407589, loss: 14.001637
23:00:04.807581 Step[1300/6995], lr: 0.000100, mv_avg_loss: 14.400746, loss: 20.793644
23:00:09.600315 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.466965, loss: 20.717051
23:00:14.372589 Step[1500/6995], lr: 0.000100, mv_avg_loss: 13.655540, loss: 13.634436
23:00:19.148442 Step[1600/6995], lr: 0.000100, mv_avg_loss: 14.229693, loss: 10.967915
23:00:23.919463 Step[1700/6995], lr: 0.000100, mv_avg_loss: 13.942279, loss: 10.075785
23:00:28.689470 Step[1800/6995], lr: 0.000100, mv_avg_loss: 14.439216, loss: 13.105021
23:00:33.446634 Step[1900/6995], lr: 0.000100, mv_avg_loss: 13.188383, loss: 9.820914
23:00:38.213094 Step[2000/6995], lr: 0.000100, mv_avg_loss: 13.566669, loss: 13.516571
23:00:42.985603 Step[2100/6995], lr: 0.000100, mv_avg_loss: 12.817030, loss: 17.452806
23:00:47.758352 Step[2200/6995], lr: 0.000100, mv_avg_loss: 14.042416, loss: 13.664658
23:00:52.530409 Step[2300/6995], lr: 0.000100, mv_avg_loss: 13.567452, loss: 6.669048
23:00:57.299534 Step[2400/6995], lr: 0.000100, mv_avg_loss: 14.731318, loss: 16.163620
23:01:02.071080 Step[2500/6995], lr: 0.000100, mv_avg_loss: 13.508213, loss: 14.631645
23:01:06.843352 Step[2600/6995], lr: 0.000100, mv_avg_loss: 12.397524, loss: 9.938993
23:01:11.602658 Step[2700/6995], lr: 0.000100, mv_avg_loss: 14.257913, loss: 10.785601
23:01:16.371615 Step[2800/6995], lr: 0.000100, mv_avg_loss: 14.376860, loss: 11.007370
23:01:21.144421 Step[2900/6995], lr: 0.000100, mv_avg_loss: 12.903085, loss: 8.282513
23:01:25.913800 Step[3000/6995], lr: 0.000100, mv_avg_loss: 14.848167, loss: 14.055222
23:01:30.677961 Step[3100/6995], lr: 0.000100, mv_avg_loss: 13.996506, loss: 13.465869
23:01:35.441098 Step[3200/6995], lr: 0.000100, mv_avg_loss: 13.187010, loss: 9.941990
23:01:40.214463 Step[3300/6995], lr: 0.000100, mv_avg_loss: 13.988051, loss: 11.201324
23:01:44.980774 Step[3400/6995], lr: 0.000100, mv_avg_loss: 14.305135, loss: 14.572940
23:01:49.745779 Step[3500/6995], lr: 0.000100, mv_avg_loss: 14.194731, loss: 8.014183
23:01:54.514705 Step[3600/6995], lr: 0.000100, mv_avg_loss: 14.638076, loss: 24.723110
23:01:59.276447 Step[3700/6995], lr: 0.000100, mv_avg_loss: 14.848858, loss: 12.504942
23:02:04.039620 Step[3800/6995], lr: 0.000100, mv_avg_loss: 14.548244, loss: 11.532759
23:02:08.810423 Step[3900/6995], lr: 0.000100, mv_avg_loss: 13.426372, loss: 9.896499
23:02:13.572418 Step[4000/6995], lr: 0.000100, mv_avg_loss: 12.946756, loss: 13.204948
23:02:18.338213 Step[4100/6995], lr: 0.000100, mv_avg_loss: 13.655715, loss: 13.453403
23:02:23.112032 Step[4200/6995], lr: 0.000100, mv_avg_loss: 12.495370, loss: 11.746780
23:02:27.878271 Step[4300/6995], lr: 0.000100, mv_avg_loss: 13.015326, loss: 15.539233
23:02:32.643266 Step[4400/6995], lr: 0.000100, mv_avg_loss: 14.544317, loss: 11.239340
23:02:37.399254 Step[4500/6995], lr: 0.000100, mv_avg_loss: 13.172153, loss: 15.778795
23:02:42.155961 Step[4600/6995], lr: 0.000100, mv_avg_loss: 12.430243, loss: 7.574808
23:02:46.913239 Step[4700/6995], lr: 0.000100, mv_avg_loss: 14.113322, loss: 14.946648
23:02:51.662262 Step[4800/6995], lr: 0.000100, mv_avg_loss: 12.728306, loss: 10.721506
23:02:56.424828 Step[4900/6995], lr: 0.000100, mv_avg_loss: 13.939205, loss: 11.298488
23:03:01.184616 Step[5000/6995], lr: 0.000100, mv_avg_loss: 14.742899, loss: 10.933140
23:03:05.942057 Step[5100/6995], lr: 0.000100, mv_avg_loss: 12.559121, loss: 12.328998
23:03:10.700250 Step[5200/6995], lr: 0.000100, mv_avg_loss: 14.817059, loss: 22.332634
23:03:15.471561 Step[5300/6995], lr: 0.000100, mv_avg_loss: 13.468080, loss: 13.509708
23:03:20.234101 Step[5400/6995], lr: 0.000100, mv_avg_loss: 14.589334, loss: 19.905537
23:03:24.981204 Step[5500/6995], lr: 0.000100, mv_avg_loss: 13.992353, loss: 18.903971
23:03:29.738481 Step[5600/6995], lr: 0.000100, mv_avg_loss: 13.818217, loss: 10.671383
23:03:34.496235 Step[5700/6995], lr: 0.000100, mv_avg_loss: 13.353704, loss: 14.094700
23:03:39.255051 Step[5800/6995], lr: 0.000100, mv_avg_loss: 14.074289, loss: 18.604494
23:03:44.008486 Step[5900/6995], lr: 0.000100, mv_avg_loss: 14.056373, loss: 25.236242
23:03:48.767350 Step[6000/6995], lr: 0.000100, mv_avg_loss: 13.811351, loss: 18.734434
23:03:53.530546 Step[6100/6995], lr: 0.000100, mv_avg_loss: 13.091701, loss: 9.739241
23:03:58.279759 Step[6200/6995], lr: 0.000100, mv_avg_loss: 13.645750, loss: 10.675433
23:04:03.037686 Step[6300/6995], lr: 0.000100, mv_avg_loss: 13.427107, loss: 15.860530
23:04:07.811563 Step[6400/6995], lr: 0.000100, mv_avg_loss: 13.790585, loss: 8.497150
23:04:12.565416 Step[6500/6995], lr: 0.000100, mv_avg_loss: 14.950007, loss: 19.336521
23:04:17.323932 Step[6600/6995], lr: 0.000100, mv_avg_loss: 13.177330, loss: 10.544898
23:04:22.075705 Step[6700/6995], lr: 0.000100, mv_avg_loss: 13.436585, loss: 14.003096
23:04:26.829433 Step[6800/6995], lr: 0.000100, mv_avg_loss: 13.299665, loss: 16.356171
23:04:31.582098 Step[6900/6995], lr: 0.000100, mv_avg_loss: 13.166096, loss: 14.401416
Testing for epoch: 62
Average test PNSR is 25.344962 for 500 images
Start to train epoch 63
23:04:58.052537 Step[0/6995], lr: 0.000100, mv_avg_loss: 13.461699, loss: 10.417706
23:05:02.800129 Step[100/6995], lr: 0.000100, mv_avg_loss: 13.880421, loss: 27.407804
23:05:07.530725 Step[200/6995], lr: 0.000100, mv_avg_loss: 13.261244, loss: 12.073940
23:05:12.270426 Step[300/6995], lr: 0.000100, mv_avg_loss: 13.554287, loss: 15.605713
23:05:17.012009 Step[400/6995], lr: 0.000100, mv_avg_loss: 12.363576, loss: 8.325827
23:05:21.766694 Step[500/6995], lr: 0.000100, mv_avg_loss: 13.486816, loss: 10.116234
23:05:26.515061 Step[600/6995], lr: 0.000100, mv_avg_loss: 13.565226, loss: 14.752159
23:05:31.258141 Step[700/6995], lr: 0.000100, mv_avg_loss: 14.272483, loss: 10.695369
23:05:36.001656 Step[800/6995], lr: 0.000100, mv_avg_loss: 14.030129, loss: 8.016275
23:05:40.753253 Step[900/6995], lr: 0.000100, mv_avg_loss: 13.550874, loss: 11.386940
23:05:45.502155 Step[1000/6995], lr: 0.000100, mv_avg_loss: 14.045508, loss: 8.673626
23:05:50.243483 Step[1100/6995], lr: 0.000100, mv_avg_loss: 12.375996, loss: 9.956122
23:05:54.995929 Step[1200/6995], lr: 0.000100, mv_avg_loss: 13.578755, loss: 14.360435
23:05:59.757268 Step[1300/6995], lr: 0.000100, mv_avg_loss: 12.701756, loss: 13.365332
23:06:04.505542 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.229211, loss: 13.555354
23:06:09.259098 Step[1500/6995], lr: 0.000100, mv_avg_loss: 13.489840, loss: 17.346233
23:06:14.007213 Step[1600/6995], lr: 0.000100, mv_avg_loss: 14.326608, loss: 10.563953
23:06:18.750064 Step[1700/6995], lr: 0.000100, mv_avg_loss: 14.027197, loss: 15.133060
23:06:23.519874 Step[1800/6995], lr: 0.000100, mv_avg_loss: 13.057387, loss: 23.669136
23:06:28.273024 Step[1900/6995], lr: 0.000100, mv_avg_loss: 13.454538, loss: 12.846751
23:06:33.017762 Step[2000/6995], lr: 0.000100, mv_avg_loss: 12.962463, loss: 13.220721
23:06:37.770114 Step[2100/6995], lr: 0.000100, mv_avg_loss: 13.090095, loss: 8.954293
23:06:42.524576 Step[2200/6995], lr: 0.000100, mv_avg_loss: 12.674283, loss: 12.972450
23:06:47.274920 Step[2300/6995], lr: 0.000100, mv_avg_loss: 13.331554, loss: 10.562693
23:06:52.026884 Step[2400/6995], lr: 0.000100, mv_avg_loss: 14.745543, loss: 8.942832
23:06:56.785864 Step[2500/6995], lr: 0.000100, mv_avg_loss: 14.615934, loss: 14.828212
23:07:01.548315 Step[2600/6995], lr: 0.000100, mv_avg_loss: 13.250732, loss: 10.223906
23:07:06.291266 Step[2700/6995], lr: 0.000100, mv_avg_loss: 13.884257, loss: 12.361801
23:07:11.048239 Step[2800/6995], lr: 0.000100, mv_avg_loss: 14.695068, loss: 12.537909
23:07:15.803826 Step[2900/6995], lr: 0.000100, mv_avg_loss: 13.541566, loss: 13.962831
23:07:20.564473 Step[3000/6995], lr: 0.000100, mv_avg_loss: 12.722431, loss: 14.147304
23:07:25.310526 Step[3100/6995], lr: 0.000100, mv_avg_loss: 13.524190, loss: 13.020950
23:07:30.059207 Step[3200/6995], lr: 0.000100, mv_avg_loss: 14.784739, loss: 9.081205
23:07:34.812874 Step[3300/6995], lr: 0.000100, mv_avg_loss: 13.740507, loss: 7.564860
23:07:39.570094 Step[3400/6995], lr: 0.000100, mv_avg_loss: 14.723577, loss: 16.892099
23:07:44.325186 Step[3500/6995], lr: 0.000100, mv_avg_loss: 13.609525, loss: 11.565646
23:07:49.086240 Step[3600/6995], lr: 0.000100, mv_avg_loss: 13.633633, loss: 18.641497
23:07:53.844710 Step[3700/6995], lr: 0.000100, mv_avg_loss: 14.683718, loss: 21.526640
23:07:58.610055 Step[3800/6995], lr: 0.000100, mv_avg_loss: 14.181100, loss: 11.644515
23:08:03.366828 Step[3900/6995], lr: 0.000100, mv_avg_loss: 14.779100, loss: 25.140783
23:08:08.125337 Step[4000/6995], lr: 0.000100, mv_avg_loss: 12.727497, loss: 14.428825
23:08:12.881545 Step[4100/6995], lr: 0.000100, mv_avg_loss: 14.945692, loss: 10.842142
23:08:17.639509 Step[4200/6995], lr: 0.000100, mv_avg_loss: 12.783639, loss: 11.981013
23:08:22.404890 Step[4300/6995], lr: 0.000100, mv_avg_loss: 13.448268, loss: 11.520590
23:08:27.188899 Step[4400/6995], lr: 0.000100, mv_avg_loss: 13.801060, loss: 15.727621
23:08:31.952113 Step[4500/6995], lr: 0.000100, mv_avg_loss: 14.200070, loss: 11.930199
23:08:36.710406 Step[4600/6995], lr: 0.000100, mv_avg_loss: 14.510398, loss: 10.318710
23:08:41.478387 Step[4700/6995], lr: 0.000100, mv_avg_loss: 13.643817, loss: 19.067738
23:08:46.231753 Step[4800/6995], lr: 0.000100, mv_avg_loss: 13.628499, loss: 19.432775
23:08:50.994948 Step[4900/6995], lr: 0.000100, mv_avg_loss: 13.331327, loss: 18.397503
23:08:55.757071 Step[5000/6995], lr: 0.000100, mv_avg_loss: 14.341005, loss: 13.306150
23:09:00.514357 Step[5100/6995], lr: 0.000100, mv_avg_loss: 14.716274, loss: 15.650394
23:09:05.267996 Step[5200/6995], lr: 0.000100, mv_avg_loss: 12.874159, loss: 18.353970
23:09:10.024514 Step[5300/6995], lr: 0.000100, mv_avg_loss: 13.019293, loss: 17.166883
23:09:14.776885 Step[5400/6995], lr: 0.000100, mv_avg_loss: 13.679696, loss: 7.864831
23:09:19.533353 Step[5500/6995], lr: 0.000100, mv_avg_loss: 13.702707, loss: 13.964615
23:09:24.281388 Step[5600/6995], lr: 0.000100, mv_avg_loss: 13.460598, loss: 24.058500
23:09:29.048537 Step[5700/6995], lr: 0.000100, mv_avg_loss: 13.907014, loss: 12.484413
23:09:33.813271 Step[5800/6995], lr: 0.000100, mv_avg_loss: 15.424983, loss: 11.038853
23:09:38.574184 Step[5900/6995], lr: 0.000100, mv_avg_loss: 13.283151, loss: 13.504461
23:09:43.329288 Step[6000/6995], lr: 0.000100, mv_avg_loss: 13.295824, loss: 10.497870
23:09:48.091725 Step[6100/6995], lr: 0.000100, mv_avg_loss: 13.318588, loss: 16.674440
23:09:52.857742 Step[6200/6995], lr: 0.000100, mv_avg_loss: 13.298589, loss: 9.114227
23:09:57.622204 Step[6300/6995], lr: 0.000100, mv_avg_loss: 14.084584, loss: 12.126007
23:10:02.390523 Step[6400/6995], lr: 0.000100, mv_avg_loss: 13.392470, loss: 9.856615
23:10:07.152239 Step[6500/6995], lr: 0.000100, mv_avg_loss: 14.053508, loss: 9.434669
23:10:11.912343 Step[6600/6995], lr: 0.000100, mv_avg_loss: 12.545392, loss: 8.530309
23:10:16.681496 Step[6700/6995], lr: 0.000100, mv_avg_loss: 13.603425, loss: 17.723022
23:10:21.452403 Step[6800/6995], lr: 0.000100, mv_avg_loss: 13.023030, loss: 17.245161
23:10:26.222979 Step[6900/6995], lr: 0.000100, mv_avg_loss: 14.062612, loss: 16.034348
Testing for epoch: 63
Average test PNSR is 25.353781 for 500 images
Start to train epoch 64
23:10:52.987489 Step[0/6995], lr: 0.000100, mv_avg_loss: 14.076643, loss: 10.151352
23:10:57.726184 Step[100/6995], lr: 0.000100, mv_avg_loss: 12.371233, loss: 13.944626
23:11:02.477210 Step[200/6995], lr: 0.000100, mv_avg_loss: 13.506596, loss: 8.550743
23:11:07.223847 Step[300/6995], lr: 0.000100, mv_avg_loss: 14.464665, loss: 9.093214
23:11:11.976821 Step[400/6995], lr: 0.000100, mv_avg_loss: 12.814179, loss: 14.916080
23:11:16.727784 Step[500/6995], lr: 0.000100, mv_avg_loss: 14.828039, loss: 12.407896
23:11:21.489096 Step[600/6995], lr: 0.000100, mv_avg_loss: 13.657341, loss: 12.870494
23:11:26.261972 Step[700/6995], lr: 0.000100, mv_avg_loss: 14.205656, loss: 11.928179
23:11:31.034953 Step[800/6995], lr: 0.000100, mv_avg_loss: 14.003327, loss: 11.949898
23:11:35.803114 Step[900/6995], lr: 0.000100, mv_avg_loss: 13.413280, loss: 10.554180
23:11:40.572193 Step[1000/6995], lr: 0.000100, mv_avg_loss: 14.225721, loss: 15.406265
23:11:45.356054 Step[1100/6995], lr: 0.000100, mv_avg_loss: 12.535377, loss: 11.847410
23:11:50.120074 Step[1200/6995], lr: 0.000100, mv_avg_loss: 12.788471, loss: 9.965000
23:11:54.885822 Step[1300/6995], lr: 0.000100, mv_avg_loss: 13.413616, loss: 15.698685
23:11:59.655937 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.100649, loss: 9.722193
23:12:04.427472 Step[1500/6995], lr: 0.000100, mv_avg_loss: 13.922879, loss: 31.341228
23:12:09.196035 Step[1600/6995], lr: 0.000100, mv_avg_loss: 13.529638, loss: 9.744062
23:12:13.970381 Step[1700/6995], lr: 0.000100, mv_avg_loss: 13.219908, loss: 18.916945
23:12:18.730380 Step[1800/6995], lr: 0.000100, mv_avg_loss: 12.779253, loss: 10.029446
23:12:23.500875 Step[1900/6995], lr: 0.000100, mv_avg_loss: 13.282292, loss: 7.477785
23:12:28.267561 Step[2000/6995], lr: 0.000100, mv_avg_loss: 13.941115, loss: 17.103188
23:12:33.037214 Step[2100/6995], lr: 0.000100, mv_avg_loss: 13.044583, loss: 9.760716
23:12:37.801100 Step[2200/6995], lr: 0.000100, mv_avg_loss: 13.438333, loss: 15.260248
23:12:42.564563 Step[2300/6995], lr: 0.000100, mv_avg_loss: 13.027842, loss: 13.485090
23:12:47.362758 Step[2400/6995], lr: 0.000100, mv_avg_loss: 14.288649, loss: 24.084974
23:12:52.126687 Step[2500/6995], lr: 0.000100, mv_avg_loss: 13.452231, loss: 9.747505
23:12:56.898145 Step[2600/6995], lr: 0.000100, mv_avg_loss: 13.827696, loss: 12.670992
23:13:01.666431 Step[2700/6995], lr: 0.000100, mv_avg_loss: 14.644809, loss: 11.408295
23:13:06.428848 Step[2800/6995], lr: 0.000100, mv_avg_loss: 12.133889, loss: 8.393543
23:13:11.205427 Step[2900/6995], lr: 0.000100, mv_avg_loss: 15.352302, loss: 15.089521
23:13:15.980210 Step[3000/6995], lr: 0.000100, mv_avg_loss: 14.511684, loss: 15.040942
23:13:20.745741 Step[3100/6995], lr: 0.000100, mv_avg_loss: 13.454514, loss: 16.611420
23:13:25.535835 Step[3200/6995], lr: 0.000100, mv_avg_loss: 14.101245, loss: 16.720724
23:13:30.326841 Step[3300/6995], lr: 0.000100, mv_avg_loss: 13.883485, loss: 11.998092
23:13:35.098142 Step[3400/6995], lr: 0.000100, mv_avg_loss: 14.373142, loss: 12.986823
23:13:39.870929 Step[3500/6995], lr: 0.000100, mv_avg_loss: 13.638412, loss: 9.286491
23:13:44.636997 Step[3600/6995], lr: 0.000100, mv_avg_loss: 13.308711, loss: 10.787452
23:13:49.404347 Step[3700/6995], lr: 0.000100, mv_avg_loss: 12.979881, loss: 10.671741
23:13:54.161903 Step[3800/6995], lr: 0.000100, mv_avg_loss: 12.811164, loss: 18.572659
23:13:58.933338 Step[3900/6995], lr: 0.000100, mv_avg_loss: 14.648677, loss: 16.970554
23:14:03.701435 Step[4000/6995], lr: 0.000100, mv_avg_loss: 13.835120, loss: 20.214188
23:14:08.470805 Step[4100/6995], lr: 0.000100, mv_avg_loss: 13.427836, loss: 8.244238
23:14:13.232760 Step[4200/6995], lr: 0.000100, mv_avg_loss: 13.322103, loss: 20.046459
23:14:17.990506 Step[4300/6995], lr: 0.000100, mv_avg_loss: 13.682807, loss: 15.695266
23:14:22.758815 Step[4400/6995], lr: 0.000100, mv_avg_loss: 14.058084, loss: 12.910866
23:14:27.521785 Step[4500/6995], lr: 0.000100, mv_avg_loss: 13.478360, loss: 10.224265
23:14:32.292857 Step[4600/6995], lr: 0.000100, mv_avg_loss: 13.742894, loss: 8.702853
23:14:37.063088 Step[4700/6995], lr: 0.000100, mv_avg_loss: 13.265095, loss: 12.765270
23:14:41.832024 Step[4800/6995], lr: 0.000100, mv_avg_loss: 12.809160, loss: 10.467564
23:14:46.610270 Step[4900/6995], lr: 0.000100, mv_avg_loss: 13.850957, loss: 15.179020
23:14:51.380921 Step[5000/6995], lr: 0.000100, mv_avg_loss: 13.371902, loss: 19.722034
23:14:56.150944 Step[5100/6995], lr: 0.000100, mv_avg_loss: 13.688513, loss: 9.256353
23:15:00.929488 Step[5200/6995], lr: 0.000100, mv_avg_loss: 14.316565, loss: 13.596553
23:15:05.703105 Step[5300/6995], lr: 0.000100, mv_avg_loss: 13.262831, loss: 10.246902
23:15:10.482472 Step[5400/6995], lr: 0.000100, mv_avg_loss: 13.355183, loss: 10.554358
23:15:15.251580 Step[5500/6995], lr: 0.000100, mv_avg_loss: 13.736167, loss: 11.147850
23:15:20.015933 Step[5600/6995], lr: 0.000100, mv_avg_loss: 14.343824, loss: 11.155261
23:15:24.792542 Step[5700/6995], lr: 0.000100, mv_avg_loss: 13.803643, loss: 11.729897
23:15:29.569737 Step[5800/6995], lr: 0.000100, mv_avg_loss: 12.866753, loss: 18.212900
23:15:34.336386 Step[5900/6995], lr: 0.000100, mv_avg_loss: 14.102820, loss: 28.881706
23:15:39.107910 Step[6000/6995], lr: 0.000100, mv_avg_loss: 12.444777, loss: 19.266785
23:15:43.880625 Step[6100/6995], lr: 0.000100, mv_avg_loss: 13.885380, loss: 11.693527
23:15:48.647138 Step[6200/6995], lr: 0.000100, mv_avg_loss: 13.810203, loss: 21.211287
23:15:53.428595 Step[6300/6995], lr: 0.000100, mv_avg_loss: 14.212652, loss: 17.236418
23:15:58.195796 Step[6400/6995], lr: 0.000100, mv_avg_loss: 14.156045, loss: 9.015924
23:16:02.970938 Step[6500/6995], lr: 0.000100, mv_avg_loss: 13.648935, loss: 13.961608
23:16:07.746087 Step[6600/6995], lr: 0.000100, mv_avg_loss: 13.258857, loss: 9.052384
23:16:12.513084 Step[6700/6995], lr: 0.000100, mv_avg_loss: 13.480572, loss: 18.336895
23:16:17.272898 Step[6800/6995], lr: 0.000100, mv_avg_loss: 13.335626, loss: 13.447001
23:16:22.044623 Step[6900/6995], lr: 0.000100, mv_avg_loss: 13.773836, loss: 14.813400
saving model for epoch 64
Testing for epoch: 64
Average test PNSR is 25.358974 for 500 images
Start to train epoch 65
23:16:49.097515 Step[0/6995], lr: 0.000100, mv_avg_loss: 14.436748, loss: 13.222018
23:16:53.842435 Step[100/6995], lr: 0.000100, mv_avg_loss: 12.720288, loss: 14.711020
23:16:58.581811 Step[200/6995], lr: 0.000100, mv_avg_loss: 14.306767, loss: 10.997703
23:17:03.325321 Step[300/6995], lr: 0.000100, mv_avg_loss: 13.051245, loss: 18.002018
23:17:08.097885 Step[400/6995], lr: 0.000100, mv_avg_loss: 14.108662, loss: 9.057046
23:17:12.848507 Step[500/6995], lr: 0.000100, mv_avg_loss: 13.335683, loss: 8.785633
23:17:17.597678 Step[600/6995], lr: 0.000100, mv_avg_loss: 13.207324, loss: 14.898623
23:17:22.349815 Step[700/6995], lr: 0.000100, mv_avg_loss: 12.873637, loss: 9.620842
23:17:27.124991 Step[800/6995], lr: 0.000100, mv_avg_loss: 13.535320, loss: 13.428112
23:17:31.875442 Step[900/6995], lr: 0.000100, mv_avg_loss: 13.814907, loss: 14.746378
23:17:36.632286 Step[1000/6995], lr: 0.000100, mv_avg_loss: 13.836422, loss: 13.839946
23:17:41.363819 Step[1100/6995], lr: 0.000100, mv_avg_loss: 13.344977, loss: 9.358653
23:17:46.114764 Step[1200/6995], lr: 0.000100, mv_avg_loss: 13.525679, loss: 10.641327
23:17:50.872340 Step[1300/6995], lr: 0.000100, mv_avg_loss: 12.246797, loss: 14.542547
23:17:55.620913 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.490501, loss: 11.341064
23:18:00.378084 Step[1500/6995], lr: 0.000100, mv_avg_loss: 13.716656, loss: 16.106918
23:18:05.128518 Step[1600/6995], lr: 0.000100, mv_avg_loss: 13.127378, loss: 16.196989
23:18:09.874877 Step[1700/6995], lr: 0.000100, mv_avg_loss: 13.303541, loss: 8.386189
23:18:14.622920 Step[1800/6995], lr: 0.000100, mv_avg_loss: 13.056758, loss: 8.791639
23:18:19.377532 Step[1900/6995], lr: 0.000100, mv_avg_loss: 12.511388, loss: 10.459546
23:18:24.135310 Step[2000/6995], lr: 0.000100, mv_avg_loss: 13.472652, loss: 17.210987
23:18:28.899457 Step[2100/6995], lr: 0.000100, mv_avg_loss: 14.405723, loss: 8.514893
23:18:33.658162 Step[2200/6995], lr: 0.000100, mv_avg_loss: 13.755516, loss: 12.676481
23:18:38.416790 Step[2300/6995], lr: 0.000100, mv_avg_loss: 14.405619, loss: 12.527049
23:18:43.171496 Step[2400/6995], lr: 0.000100, mv_avg_loss: 13.375648, loss: 18.838400
23:18:47.939773 Step[2500/6995], lr: 0.000100, mv_avg_loss: 13.340058, loss: 11.859675
23:18:52.695533 Step[2600/6995], lr: 0.000100, mv_avg_loss: 13.371819, loss: 13.250400
23:18:57.462472 Step[2700/6995], lr: 0.000100, mv_avg_loss: 13.865819, loss: 10.632250
23:19:02.217985 Step[2800/6995], lr: 0.000100, mv_avg_loss: 14.464432, loss: 11.864231
23:19:06.969396 Step[2900/6995], lr: 0.000100, mv_avg_loss: 13.424841, loss: 12.505023
23:19:11.715665 Step[3000/6995], lr: 0.000100, mv_avg_loss: 14.065683, loss: 17.299879
23:19:16.460709 Step[3100/6995], lr: 0.000100, mv_avg_loss: 13.918925, loss: 16.787453
23:19:21.210014 Step[3200/6995], lr: 0.000100, mv_avg_loss: 13.094827, loss: 6.580270
23:19:25.961502 Step[3300/6995], lr: 0.000100, mv_avg_loss: 13.636112, loss: 9.475565
23:19:30.727556 Step[3400/6995], lr: 0.000100, mv_avg_loss: 12.845773, loss: 11.517281
23:19:35.468907 Step[3500/6995], lr: 0.000100, mv_avg_loss: 13.293647, loss: 10.152114
23:19:40.210501 Step[3600/6995], lr: 0.000100, mv_avg_loss: 13.743773, loss: 13.042899
23:19:44.970173 Step[3700/6995], lr: 0.000100, mv_avg_loss: 13.470793, loss: 12.261511
23:19:49.723992 Step[3800/6995], lr: 0.000100, mv_avg_loss: 13.762957, loss: 14.055231
23:19:54.476817 Step[3900/6995], lr: 0.000100, mv_avg_loss: 13.916248, loss: 11.267202
23:19:59.236568 Step[4000/6995], lr: 0.000100, mv_avg_loss: 13.332432, loss: 10.524469
23:20:03.985180 Step[4100/6995], lr: 0.000100, mv_avg_loss: 13.402446, loss: 8.911745
23:20:08.738766 Step[4200/6995], lr: 0.000100, mv_avg_loss: 13.307210, loss: 10.558907
23:20:13.488549 Step[4300/6995], lr: 0.000100, mv_avg_loss: 13.628402, loss: 18.244980
23:20:18.225345 Step[4400/6995], lr: 0.000100, mv_avg_loss: 13.767349, loss: 17.464172
23:20:22.979501 Step[4500/6995], lr: 0.000100, mv_avg_loss: 15.377677, loss: 11.323618
23:20:27.745265 Step[4600/6995], lr: 0.000100, mv_avg_loss: 13.345856, loss: 9.448799
23:20:32.501291 Step[4700/6995], lr: 0.000100, mv_avg_loss: 13.533780, loss: 10.872299
23:20:37.247305 Step[4800/6995], lr: 0.000100, mv_avg_loss: 12.961635, loss: 8.188884
23:20:42.002660 Step[4900/6995], lr: 0.000100, mv_avg_loss: 13.016612, loss: 21.421803
23:20:46.769491 Step[5000/6995], lr: 0.000100, mv_avg_loss: 14.333784, loss: 8.452013
23:20:51.527467 Step[5100/6995], lr: 0.000100, mv_avg_loss: 13.782335, loss: 20.809538
23:20:56.283842 Step[5200/6995], lr: 0.000100, mv_avg_loss: 13.385605, loss: 16.528118
23:21:01.037091 Step[5300/6995], lr: 0.000100, mv_avg_loss: 13.495331, loss: 14.683384
23:21:05.826598 Step[5400/6995], lr: 0.000100, mv_avg_loss: 14.088082, loss: 11.686568
23:21:10.577715 Step[5500/6995], lr: 0.000100, mv_avg_loss: 13.412358, loss: 15.533867
23:21:15.328549 Step[5600/6995], lr: 0.000100, mv_avg_loss: 14.875491, loss: 10.443348
23:21:20.073084 Step[5700/6995], lr: 0.000100, mv_avg_loss: 13.759794, loss: 13.949547
23:21:24.822673 Step[5800/6995], lr: 0.000100, mv_avg_loss: 13.482228, loss: 11.068115
23:21:29.579641 Step[5900/6995], lr: 0.000100, mv_avg_loss: 13.095936, loss: 8.523699
23:21:34.320553 Step[6000/6995], lr: 0.000100, mv_avg_loss: 13.740117, loss: 12.209608
23:21:39.075862 Step[6100/6995], lr: 0.000100, mv_avg_loss: 13.168387, loss: 20.967442
23:21:43.827931 Step[6200/6995], lr: 0.000100, mv_avg_loss: 12.889123, loss: 11.516852
23:21:48.579379 Step[6300/6995], lr: 0.000100, mv_avg_loss: 13.520879, loss: 20.653999
23:21:53.334792 Step[6400/6995], lr: 0.000100, mv_avg_loss: 14.805634, loss: 8.822375
23:21:58.090774 Step[6500/6995], lr: 0.000100, mv_avg_loss: 13.177997, loss: 10.629177
23:22:02.843657 Step[6600/6995], lr: 0.000100, mv_avg_loss: 12.975313, loss: 12.927928
23:22:07.601271 Step[6700/6995], lr: 0.000100, mv_avg_loss: 14.539577, loss: 11.974684
23:22:12.351125 Step[6800/6995], lr: 0.000100, mv_avg_loss: 13.477842, loss: 16.943056
23:22:17.108120 Step[6900/6995], lr: 0.000100, mv_avg_loss: 13.732295, loss: 9.793223
Testing for epoch: 65
Average test PNSR is 25.350292 for 500 images
Start to train epoch 66
23:22:43.953334 Step[0/6995], lr: 0.000100, mv_avg_loss: 13.152220, loss: 15.747547
23:22:48.697172 Step[100/6995], lr: 0.000100, mv_avg_loss: 13.224862, loss: 9.341436
23:22:53.447482 Step[200/6995], lr: 0.000100, mv_avg_loss: 14.002474, loss: 15.963558
23:22:58.192037 Step[300/6995], lr: 0.000100, mv_avg_loss: 12.104568, loss: 14.774563
23:23:02.957350 Step[400/6995], lr: 0.000100, mv_avg_loss: 13.161272, loss: 13.664261
23:23:07.713098 Step[500/6995], lr: 0.000100, mv_avg_loss: 13.142175, loss: 8.076206
23:23:12.470521 Step[600/6995], lr: 0.000100, mv_avg_loss: 12.988697, loss: 16.317726
23:23:17.228346 Step[700/6995], lr: 0.000100, mv_avg_loss: 13.515474, loss: 10.482660
23:23:21.985208 Step[800/6995], lr: 0.000100, mv_avg_loss: 12.169702, loss: 16.867039
23:23:26.738636 Step[900/6995], lr: 0.000100, mv_avg_loss: 14.833258, loss: 18.707851
23:23:31.558010 Step[1000/6995], lr: 0.000100, mv_avg_loss: 14.182549, loss: 16.964802
23:23:36.305875 Step[1100/6995], lr: 0.000100, mv_avg_loss: 12.508903, loss: 17.521751
23:23:41.071628 Step[1200/6995], lr: 0.000100, mv_avg_loss: 13.539656, loss: 11.946742
23:23:45.848374 Step[1300/6995], lr: 0.000100, mv_avg_loss: 13.497463, loss: 9.592098
23:23:50.604194 Step[1400/6995], lr: 0.000100, mv_avg_loss: 14.438522, loss: 16.190191
23:23:55.361047 Step[1500/6995], lr: 0.000100, mv_avg_loss: 13.210026, loss: 14.716693
23:24:00.115380 Step[1600/6995], lr: 0.000100, mv_avg_loss: 14.093830, loss: 12.954901
23:24:04.880944 Step[1700/6995], lr: 0.000100, mv_avg_loss: 14.395309, loss: 11.514662
23:24:09.648445 Step[1800/6995], lr: 0.000100, mv_avg_loss: 12.642265, loss: 14.541805
23:24:14.418888 Step[1900/6995], lr: 0.000100, mv_avg_loss: 14.463525, loss: 11.589343
23:24:19.170247 Step[2000/6995], lr: 0.000100, mv_avg_loss: 14.124076, loss: 14.775232
23:24:23.929246 Step[2100/6995], lr: 0.000100, mv_avg_loss: 13.975874, loss: 10.854601
23:24:28.684603 Step[2200/6995], lr: 0.000100, mv_avg_loss: 13.744760, loss: 13.476011
23:24:33.448747 Step[2300/6995], lr: 0.000100, mv_avg_loss: 14.237690, loss: 17.862511
23:24:38.203524 Step[2400/6995], lr: 0.000100, mv_avg_loss: 13.173458, loss: 9.973705
23:24:42.961476 Step[2500/6995], lr: 0.000100, mv_avg_loss: 13.328249, loss: 11.898884
23:24:47.723211 Step[2600/6995], lr: 0.000100, mv_avg_loss: 13.720844, loss: 14.274002
23:24:52.484037 Step[2700/6995], lr: 0.000100, mv_avg_loss: 12.484908, loss: 19.019474
23:24:57.252019 Step[2800/6995], lr: 0.000100, mv_avg_loss: 12.799330, loss: 13.321463
23:25:02.021500 Step[2900/6995], lr: 0.000100, mv_avg_loss: 12.260373, loss: 11.288588
23:25:06.797910 Step[3000/6995], lr: 0.000100, mv_avg_loss: 13.375700, loss: 16.843082
23:25:11.576450 Step[3100/6995], lr: 0.000100, mv_avg_loss: 13.260565, loss: 16.944801
23:25:16.343515 Step[3200/6995], lr: 0.000100, mv_avg_loss: 13.260095, loss: 22.224998
23:25:21.106894 Step[3300/6995], lr: 0.000100, mv_avg_loss: 13.347801, loss: 11.138990
23:25:25.899717 Step[3400/6995], lr: 0.000100, mv_avg_loss: 14.200685, loss: 18.981697
23:25:30.669379 Step[3500/6995], lr: 0.000100, mv_avg_loss: 14.189768, loss: 9.586387
23:25:35.438646 Step[3600/6995], lr: 0.000100, mv_avg_loss: 13.959618, loss: 7.442913
23:25:40.212964 Step[3700/6995], lr: 0.000100, mv_avg_loss: 13.058665, loss: 9.396969
23:25:44.989179 Step[3800/6995], lr: 0.000100, mv_avg_loss: 13.561602, loss: 18.291298
23:25:49.770505 Step[3900/6995], lr: 0.000100, mv_avg_loss: 12.734351, loss: 17.747726
23:25:54.538170 Step[4000/6995], lr: 0.000100, mv_avg_loss: 13.502587, loss: 13.885745
23:25:59.315221 Step[4100/6995], lr: 0.000100, mv_avg_loss: 12.656223, loss: 13.898707
23:26:04.082479 Step[4200/6995], lr: 0.000100, mv_avg_loss: 14.126297, loss: 9.086955
23:26:08.856035 Step[4300/6995], lr: 0.000100, mv_avg_loss: 13.468860, loss: 18.570976
23:26:13.624615 Step[4400/6995], lr: 0.000100, mv_avg_loss: 13.623664, loss: 12.000163
23:26:18.393394 Step[4500/6995], lr: 0.000100, mv_avg_loss: 13.420615, loss: 11.841757
23:26:23.154852 Step[4600/6995], lr: 0.000100, mv_avg_loss: 13.023268, loss: 11.821811
23:26:27.928489 Step[4700/6995], lr: 0.000100, mv_avg_loss: 13.636015, loss: 15.564413
23:26:32.707450 Step[4800/6995], lr: 0.000100, mv_avg_loss: 13.298793, loss: 8.630877
23:26:37.482001 Step[4900/6995], lr: 0.000100, mv_avg_loss: 13.747800, loss: 13.662197
23:26:42.263892 Step[5000/6995], lr: 0.000100, mv_avg_loss: 13.131501, loss: 9.396447
23:26:47.043785 Step[5100/6995], lr: 0.000100, mv_avg_loss: 14.141202, loss: 8.369201
23:26:51.820853 Step[5200/6995], lr: 0.000100, mv_avg_loss: 14.561730, loss: 20.042746
23:26:56.597586 Step[5300/6995], lr: 0.000100, mv_avg_loss: 12.804183, loss: 13.834079
23:27:01.375972 Step[5400/6995], lr: 0.000100, mv_avg_loss: 13.127051, loss: 11.849331
23:27:06.154422 Step[5500/6995], lr: 0.000100, mv_avg_loss: 13.150187, loss: 11.937132
23:27:10.934603 Step[5600/6995], lr: 0.000100, mv_avg_loss: 13.174534, loss: 13.030714
23:27:15.707010 Step[5700/6995], lr: 0.000100, mv_avg_loss: 12.619081, loss: 11.645495
23:27:20.486454 Step[5800/6995], lr: 0.000100, mv_avg_loss: 13.671745, loss: 10.811670
23:27:25.275493 Step[5900/6995], lr: 0.000100, mv_avg_loss: 13.425564, loss: 11.340736
23:27:30.063800 Step[6000/6995], lr: 0.000100, mv_avg_loss: 14.133336, loss: 14.291702
23:27:34.842840 Step[6100/6995], lr: 0.000100, mv_avg_loss: 13.369305, loss: 11.275515
23:27:39.615955 Step[6200/6995], lr: 0.000100, mv_avg_loss: 14.218328, loss: 10.870936
23:27:44.398409 Step[6300/6995], lr: 0.000100, mv_avg_loss: 13.430037, loss: 12.748301
23:27:49.175115 Step[6400/6995], lr: 0.000100, mv_avg_loss: 13.937788, loss: 9.121051
23:27:53.964915 Step[6500/6995], lr: 0.000100, mv_avg_loss: 13.112464, loss: 11.194641
23:27:58.751962 Step[6600/6995], lr: 0.000100, mv_avg_loss: 12.765570, loss: 8.705957
23:28:03.531470 Step[6700/6995], lr: 0.000100, mv_avg_loss: 14.184940, loss: 13.320366
23:28:08.305475 Step[6800/6995], lr: 0.000100, mv_avg_loss: 13.474841, loss: 14.773983
23:28:13.093857 Step[6900/6995], lr: 0.000100, mv_avg_loss: 14.204023, loss: 33.656647
Testing for epoch: 66
Average test PNSR is 25.336787 for 500 images
Start to train epoch 67
23:28:39.673717 Step[0/6995], lr: 0.000100, mv_avg_loss: 13.690248, loss: 19.817459
23:28:44.428283 Step[100/6995], lr: 0.000100, mv_avg_loss: 13.510868, loss: 14.361944
23:28:49.189311 Step[200/6995], lr: 0.000100, mv_avg_loss: 12.408223, loss: 13.626482
23:28:53.953809 Step[300/6995], lr: 0.000100, mv_avg_loss: 14.470504, loss: 18.706327
23:28:58.722597 Step[400/6995], lr: 0.000100, mv_avg_loss: 13.595860, loss: 9.661898
23:29:03.488505 Step[500/6995], lr: 0.000100, mv_avg_loss: 13.271404, loss: 9.289325
23:29:08.256636 Step[600/6995], lr: 0.000100, mv_avg_loss: 12.646655, loss: 12.177630
23:29:13.023859 Step[700/6995], lr: 0.000100, mv_avg_loss: 13.942292, loss: 15.300793
23:29:17.785569 Step[800/6995], lr: 0.000100, mv_avg_loss: 13.837955, loss: 14.983379
23:29:22.552638 Step[900/6995], lr: 0.000100, mv_avg_loss: 13.467252, loss: 8.315046
23:29:27.309523 Step[1000/6995], lr: 0.000100, mv_avg_loss: 12.812973, loss: 16.164391
23:29:32.084652 Step[1100/6995], lr: 0.000100, mv_avg_loss: 13.756497, loss: 9.037706
23:29:36.849825 Step[1200/6995], lr: 0.000100, mv_avg_loss: 13.299881, loss: 11.058162
23:29:41.599710 Step[1300/6995], lr: 0.000100, mv_avg_loss: 13.632341, loss: 10.746985
23:29:46.387882 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.636114, loss: 11.045179
23:29:51.162497 Step[1500/6995], lr: 0.000100, mv_avg_loss: 13.046261, loss: 8.041714
23:29:55.929040 Step[1600/6995], lr: 0.000100, mv_avg_loss: 13.697680, loss: 16.268723
23:30:00.695914 Step[1700/6995], lr: 0.000100, mv_avg_loss: 13.320011, loss: 14.632833
23:30:05.463124 Step[1800/6995], lr: 0.000100, mv_avg_loss: 14.135408, loss: 16.807981
23:30:10.228059 Step[1900/6995], lr: 0.000100, mv_avg_loss: 13.612766, loss: 12.367877
23:30:14.997382 Step[2000/6995], lr: 0.000100, mv_avg_loss: 12.812450, loss: 19.939253
23:30:19.772904 Step[2100/6995], lr: 0.000100, mv_avg_loss: 13.753289, loss: 9.194300
23:30:24.546580 Step[2200/6995], lr: 0.000100, mv_avg_loss: 13.258589, loss: 17.708942
23:30:29.321078 Step[2300/6995], lr: 0.000100, mv_avg_loss: 12.876357, loss: 11.197532
23:30:34.096964 Step[2400/6995], lr: 0.000100, mv_avg_loss: 13.492748, loss: 16.233459
23:30:38.870674 Step[2500/6995], lr: 0.000100, mv_avg_loss: 13.311189, loss: 9.069223
23:30:43.638390 Step[2600/6995], lr: 0.000100, mv_avg_loss: 13.371614, loss: 11.418349
23:30:48.404056 Step[2700/6995], lr: 0.000100, mv_avg_loss: 14.341167, loss: 11.434463
23:30:53.164203 Step[2800/6995], lr: 0.000100, mv_avg_loss: 13.242833, loss: 7.986457
23:30:57.936651 Step[2900/6995], lr: 0.000100, mv_avg_loss: 14.056346, loss: 8.347556
23:31:02.702353 Step[3000/6995], lr: 0.000100, mv_avg_loss: 13.058399, loss: 13.814215
23:31:07.467588 Step[3100/6995], lr: 0.000100, mv_avg_loss: 13.208754, loss: 17.348196
23:31:12.228599 Step[3200/6995], lr: 0.000100, mv_avg_loss: 14.210050, loss: 15.948344
23:31:16.995836 Step[3300/6995], lr: 0.000100, mv_avg_loss: 14.501842, loss: 9.629491
23:31:21.751307 Step[3400/6995], lr: 0.000100, mv_avg_loss: 13.463650, loss: 17.869892
23:31:26.518150 Step[3500/6995], lr: 0.000100, mv_avg_loss: 13.854798, loss: 10.016342
23:31:31.285281 Step[3600/6995], lr: 0.000100, mv_avg_loss: 13.749568, loss: 9.506092
23:31:36.048473 Step[3700/6995], lr: 0.000100, mv_avg_loss: 13.618566, loss: 12.340622
23:31:40.821869 Step[3800/6995], lr: 0.000100, mv_avg_loss: 13.043071, loss: 7.834901
23:31:45.579164 Step[3900/6995], lr: 0.000100, mv_avg_loss: 14.142767, loss: 15.080923
23:31:50.353140 Step[4000/6995], lr: 0.000100, mv_avg_loss: 13.355597, loss: 10.790290
23:31:55.116424 Step[4100/6995], lr: 0.000100, mv_avg_loss: 14.382070, loss: 12.429076
23:31:59.889605 Step[4200/6995], lr: 0.000100, mv_avg_loss: 13.749291, loss: 18.659966
23:32:04.668457 Step[4300/6995], lr: 0.000100, mv_avg_loss: 14.674403, loss: 21.449957
23:32:09.431937 Step[4400/6995], lr: 0.000100, mv_avg_loss: 12.633813, loss: 9.094559
23:32:14.189687 Step[4500/6995], lr: 0.000100, mv_avg_loss: 13.169714, loss: 9.996552
23:32:18.943520 Step[4600/6995], lr: 0.000100, mv_avg_loss: 13.972445, loss: 12.954422
23:32:23.701279 Step[4700/6995], lr: 0.000100, mv_avg_loss: 13.472696, loss: 11.759626
23:32:28.465837 Step[4800/6995], lr: 0.000100, mv_avg_loss: 13.418580, loss: 9.462432
23:32:33.217119 Step[4900/6995], lr: 0.000100, mv_avg_loss: 12.778743, loss: 8.207137
23:32:37.966772 Step[5000/6995], lr: 0.000100, mv_avg_loss: 13.384670, loss: 19.741985
23:32:42.734130 Step[5100/6995], lr: 0.000100, mv_avg_loss: 13.823933, loss: 14.316931
23:32:47.502776 Step[5200/6995], lr: 0.000100, mv_avg_loss: 13.943470, loss: 12.937132
23:32:52.251681 Step[5300/6995], lr: 0.000100, mv_avg_loss: 12.957764, loss: 13.502199
23:32:56.996221 Step[5400/6995], lr: 0.000100, mv_avg_loss: 14.532372, loss: 18.395378
23:33:01.752425 Step[5500/6995], lr: 0.000100, mv_avg_loss: 13.587556, loss: 14.907302
23:33:06.509860 Step[5600/6995], lr: 0.000100, mv_avg_loss: 13.164036, loss: 13.809340
23:33:11.268474 Step[5700/6995], lr: 0.000100, mv_avg_loss: 12.954598, loss: 10.813059
23:33:16.024319 Step[5800/6995], lr: 0.000100, mv_avg_loss: 13.651008, loss: 11.510630
23:33:20.776610 Step[5900/6995], lr: 0.000100, mv_avg_loss: 12.969685, loss: 10.538235
23:33:25.529880 Step[6000/6995], lr: 0.000100, mv_avg_loss: 13.887928, loss: 17.663212
23:33:30.281740 Step[6100/6995], lr: 0.000100, mv_avg_loss: 13.304027, loss: 14.968761
23:33:35.039004 Step[6200/6995], lr: 0.000100, mv_avg_loss: 13.658793, loss: 11.664939
23:33:39.792660 Step[6300/6995], lr: 0.000100, mv_avg_loss: 13.384525, loss: 12.716876
23:33:44.572032 Step[6400/6995], lr: 0.000100, mv_avg_loss: 13.731544, loss: 14.992424
23:33:49.330950 Step[6500/6995], lr: 0.000100, mv_avg_loss: 13.510756, loss: 10.942448
23:33:54.084674 Step[6600/6995], lr: 0.000100, mv_avg_loss: 13.462814, loss: 22.628918
23:33:58.846273 Step[6700/6995], lr: 0.000100, mv_avg_loss: 12.800182, loss: 11.711128
23:34:03.606379 Step[6800/6995], lr: 0.000100, mv_avg_loss: 13.190774, loss: 13.951017
23:34:08.362556 Step[6900/6995], lr: 0.000100, mv_avg_loss: 12.539974, loss: 10.272756
Testing for epoch: 67
Average test PNSR is 25.358516 for 500 images
Start to train epoch 68
23:34:35.331587 Step[0/6995], lr: 0.000100, mv_avg_loss: 13.292312, loss: 9.214521
23:34:40.052975 Step[100/6995], lr: 0.000100, mv_avg_loss: 14.123342, loss: 11.078432
23:34:44.771372 Step[200/6995], lr: 0.000100, mv_avg_loss: 12.783502, loss: 12.483829
23:34:49.506261 Step[300/6995], lr: 0.000100, mv_avg_loss: 12.505049, loss: 6.830939
23:34:54.239505 Step[400/6995], lr: 0.000100, mv_avg_loss: 13.154577, loss: 11.404483
23:34:58.972997 Step[500/6995], lr: 0.000100, mv_avg_loss: 13.279846, loss: 15.301422
23:35:03.694142 Step[600/6995], lr: 0.000100, mv_avg_loss: 13.012604, loss: 11.039218
23:35:08.417390 Step[700/6995], lr: 0.000100, mv_avg_loss: 12.759772, loss: 15.862098
23:35:13.147757 Step[800/6995], lr: 0.000100, mv_avg_loss: 13.890900, loss: 20.808037
23:35:17.880271 Step[900/6995], lr: 0.000100, mv_avg_loss: 14.761497, loss: 10.522357
23:35:22.622199 Step[1000/6995], lr: 0.000100, mv_avg_loss: 13.567816, loss: 9.288755
23:35:27.353937 Step[1100/6995], lr: 0.000100, mv_avg_loss: 12.692452, loss: 20.387424
23:35:32.104561 Step[1200/6995], lr: 0.000100, mv_avg_loss: 14.824413, loss: 7.761735
23:35:36.860768 Step[1300/6995], lr: 0.000100, mv_avg_loss: 12.944367, loss: 16.255102
23:35:41.599497 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.420258, loss: 14.782999
23:35:46.340571 Step[1500/6995], lr: 0.000100, mv_avg_loss: 12.372227, loss: 17.792925
23:35:51.090312 Step[1600/6995], lr: 0.000100, mv_avg_loss: 14.348392, loss: 15.933608
23:35:55.837564 Step[1700/6995], lr: 0.000100, mv_avg_loss: 14.050077, loss: 12.581154
23:36:00.571108 Step[1800/6995], lr: 0.000100, mv_avg_loss: 13.490834, loss: 17.058077
23:36:05.315532 Step[1900/6995], lr: 0.000100, mv_avg_loss: 14.228726, loss: 12.640543
23:36:10.059483 Step[2000/6995], lr: 0.000100, mv_avg_loss: 13.735182, loss: 18.396757
23:36:14.798411 Step[2100/6995], lr: 0.000100, mv_avg_loss: 13.617252, loss: 11.211081
23:36:19.541198 Step[2200/6995], lr: 0.000100, mv_avg_loss: 13.281932, loss: 15.633533
23:36:24.280985 Step[2300/6995], lr: 0.000100, mv_avg_loss: 13.338214, loss: 12.092810
23:36:29.032269 Step[2400/6995], lr: 0.000100, mv_avg_loss: 13.434579, loss: 18.546631
23:36:33.781287 Step[2500/6995], lr: 0.000100, mv_avg_loss: 14.484516, loss: 12.093292
23:36:38.521798 Step[2600/6995], lr: 0.000100, mv_avg_loss: 12.961797, loss: 8.376738
23:36:43.267546 Step[2700/6995], lr: 0.000100, mv_avg_loss: 14.326417, loss: 9.831409
23:36:48.021757 Step[2800/6995], lr: 0.000100, mv_avg_loss: 12.845978, loss: 13.849730
23:36:52.783150 Step[2900/6995], lr: 0.000100, mv_avg_loss: 12.918734, loss: 16.435616
23:36:57.533724 Step[3000/6995], lr: 0.000100, mv_avg_loss: 13.880277, loss: 14.745317
23:37:02.287845 Step[3100/6995], lr: 0.000100, mv_avg_loss: 13.344975, loss: 14.846104
23:37:07.041643 Step[3200/6995], lr: 0.000100, mv_avg_loss: 13.292723, loss: 16.641239
23:37:11.800052 Step[3300/6995], lr: 0.000100, mv_avg_loss: 13.892075, loss: 12.507322
23:37:16.554392 Step[3400/6995], lr: 0.000100, mv_avg_loss: 13.363557, loss: 7.198965
23:37:21.307685 Step[3500/6995], lr: 0.000100, mv_avg_loss: 13.039993, loss: 10.911289
23:37:26.068428 Step[3600/6995], lr: 0.000100, mv_avg_loss: 13.812877, loss: 23.720573
23:37:30.822275 Step[3700/6995], lr: 0.000100, mv_avg_loss: 13.240147, loss: 10.273404
23:37:35.576976 Step[3800/6995], lr: 0.000100, mv_avg_loss: 12.809788, loss: 8.295511
23:37:40.325400 Step[3900/6995], lr: 0.000100, mv_avg_loss: 12.706758, loss: 12.008324
23:37:45.076302 Step[4000/6995], lr: 0.000100, mv_avg_loss: 12.758187, loss: 9.880133
23:37:49.842317 Step[4100/6995], lr: 0.000100, mv_avg_loss: 14.093282, loss: 15.852256
23:37:54.611657 Step[4200/6995], lr: 0.000100, mv_avg_loss: 14.863785, loss: 12.561905
23:37:59.368961 Step[4300/6995], lr: 0.000100, mv_avg_loss: 13.017833, loss: 11.089546
23:38:04.148810 Step[4400/6995], lr: 0.000100, mv_avg_loss: 13.894423, loss: 12.483704
23:38:08.911584 Step[4500/6995], lr: 0.000100, mv_avg_loss: 14.284478, loss: 11.896892
23:38:13.674664 Step[4600/6995], lr: 0.000100, mv_avg_loss: 13.253943, loss: 14.166945
23:38:18.436492 Step[4700/6995], lr: 0.000100, mv_avg_loss: 14.387994, loss: 9.017364
23:38:23.203825 Step[4800/6995], lr: 0.000100, mv_avg_loss: 12.971815, loss: 23.987389
23:38:27.967976 Step[4900/6995], lr: 0.000100, mv_avg_loss: 13.225745, loss: 12.507809
23:38:32.725142 Step[5000/6995], lr: 0.000100, mv_avg_loss: 13.851956, loss: 12.247905
23:38:37.487213 Step[5100/6995], lr: 0.000100, mv_avg_loss: 13.048835, loss: 10.530386
23:38:42.241915 Step[5200/6995], lr: 0.000100, mv_avg_loss: 13.706492, loss: 11.241939
23:38:47.003047 Step[5300/6995], lr: 0.000100, mv_avg_loss: 12.920129, loss: 20.332382
23:38:51.759726 Step[5400/6995], lr: 0.000100, mv_avg_loss: 13.352956, loss: 11.908671
23:38:56.529223 Step[5500/6995], lr: 0.000100, mv_avg_loss: 13.567324, loss: 21.388123
23:39:01.285651 Step[5600/6995], lr: 0.000100, mv_avg_loss: 13.344419, loss: 21.447104
23:39:06.043461 Step[5700/6995], lr: 0.000100, mv_avg_loss: 12.093994, loss: 18.760651
23:39:10.811707 Step[5800/6995], lr: 0.000100, mv_avg_loss: 13.565791, loss: 11.644344
23:39:15.574258 Step[5900/6995], lr: 0.000100, mv_avg_loss: 13.689770, loss: 7.832683
23:39:20.331573 Step[6000/6995], lr: 0.000100, mv_avg_loss: 12.323619, loss: 8.452653
23:39:25.092052 Step[6100/6995], lr: 0.000100, mv_avg_loss: 12.741867, loss: 10.738329
23:39:29.859122 Step[6200/6995], lr: 0.000100, mv_avg_loss: 13.859423, loss: 7.257574
23:39:34.619902 Step[6300/6995], lr: 0.000100, mv_avg_loss: 13.356647, loss: 16.005928
23:39:39.382472 Step[6400/6995], lr: 0.000100, mv_avg_loss: 13.691342, loss: 13.315582
23:39:44.139670 Step[6500/6995], lr: 0.000100, mv_avg_loss: 12.348899, loss: 13.571890
23:39:48.900792 Step[6600/6995], lr: 0.000100, mv_avg_loss: 13.509698, loss: 11.064198
23:39:53.673513 Step[6700/6995], lr: 0.000100, mv_avg_loss: 13.750541, loss: 10.013019
23:39:58.431528 Step[6800/6995], lr: 0.000100, mv_avg_loss: 12.689978, loss: 10.372658
23:40:03.201056 Step[6900/6995], lr: 0.000100, mv_avg_loss: 13.115162, loss: 10.445263
Testing for epoch: 68
Average test PNSR is 25.376814 for 500 images
Start to train epoch 69
23:40:30.070104 Step[0/6995], lr: 0.000100, mv_avg_loss: 13.431070, loss: 10.937115
23:40:34.815063 Step[100/6995], lr: 0.000100, mv_avg_loss: 12.818346, loss: 18.662416
23:40:39.577670 Step[200/6995], lr: 0.000100, mv_avg_loss: 13.392378, loss: 18.419399
23:40:44.333072 Step[300/6995], lr: 0.000100, mv_avg_loss: 12.847047, loss: 10.306114
23:40:49.084385 Step[400/6995], lr: 0.000100, mv_avg_loss: 14.072995, loss: 10.397842
23:40:53.852643 Step[500/6995], lr: 0.000100, mv_avg_loss: 13.481611, loss: 18.379948
23:40:58.599123 Step[600/6995], lr: 0.000100, mv_avg_loss: 14.870018, loss: 16.799202
23:41:03.361026 Step[700/6995], lr: 0.000100, mv_avg_loss: 14.174595, loss: 17.393188
23:41:08.136273 Step[800/6995], lr: 0.000100, mv_avg_loss: 12.388667, loss: 12.324885
23:41:12.904692 Step[900/6995], lr: 0.000100, mv_avg_loss: 13.098101, loss: 15.467392
23:41:17.667346 Step[1000/6995], lr: 0.000100, mv_avg_loss: 13.786594, loss: 23.231321
23:41:22.427463 Step[1100/6995], lr: 0.000100, mv_avg_loss: 13.778077, loss: 20.309635
23:41:27.198576 Step[1200/6995], lr: 0.000100, mv_avg_loss: 13.408575, loss: 14.230103
23:41:31.958387 Step[1300/6995], lr: 0.000100, mv_avg_loss: 13.250315, loss: 11.531288
23:41:36.726125 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.285193, loss: 10.362090
23:41:41.492640 Step[1500/6995], lr: 0.000100, mv_avg_loss: 13.726717, loss: 16.098770
23:41:46.259735 Step[1600/6995], lr: 0.000100, mv_avg_loss: 13.862123, loss: 8.515060
23:41:51.032461 Step[1700/6995], lr: 0.000100, mv_avg_loss: 12.780499, loss: 8.901857
23:41:55.814032 Step[1800/6995], lr: 0.000100, mv_avg_loss: 13.425410, loss: 13.400167
23:42:00.583027 Step[1900/6995], lr: 0.000100, mv_avg_loss: 13.257538, loss: 11.675016
23:42:05.357547 Step[2000/6995], lr: 0.000100, mv_avg_loss: 12.202148, loss: 15.270004
23:42:10.132828 Step[2100/6995], lr: 0.000100, mv_avg_loss: 13.029441, loss: 12.640400
23:42:14.911907 Step[2200/6995], lr: 0.000100, mv_avg_loss: 12.800795, loss: 14.157004
23:42:19.682881 Step[2300/6995], lr: 0.000100, mv_avg_loss: 13.017646, loss: 13.356406
23:42:24.479617 Step[2400/6995], lr: 0.000100, mv_avg_loss: 13.394301, loss: 22.869774
23:42:29.266532 Step[2500/6995], lr: 0.000100, mv_avg_loss: 13.065532, loss: 13.317122
23:42:34.049588 Step[2600/6995], lr: 0.000100, mv_avg_loss: 12.585284, loss: 18.765007
23:42:38.823544 Step[2700/6995], lr: 0.000100, mv_avg_loss: 12.651371, loss: 14.514799
23:42:43.602612 Step[2800/6995], lr: 0.000100, mv_avg_loss: 13.361356, loss: 15.071459
23:42:48.383465 Step[2900/6995], lr: 0.000100, mv_avg_loss: 13.628579, loss: 14.819781
23:42:53.165266 Step[3000/6995], lr: 0.000100, mv_avg_loss: 13.932644, loss: 11.830886
23:42:57.931433 Step[3100/6995], lr: 0.000100, mv_avg_loss: 12.527174, loss: 8.924223
23:43:02.702667 Step[3200/6995], lr: 0.000100, mv_avg_loss: 15.367864, loss: 11.711157
23:43:07.464878 Step[3300/6995], lr: 0.000100, mv_avg_loss: 14.154231, loss: 12.399496
23:43:12.239030 Step[3400/6995], lr: 0.000100, mv_avg_loss: 12.413655, loss: 8.971897
23:43:17.004524 Step[3500/6995], lr: 0.000100, mv_avg_loss: 13.366680, loss: 8.638433
23:43:21.773688 Step[3600/6995], lr: 0.000100, mv_avg_loss: 13.171746, loss: 16.905401
23:43:26.542156 Step[3700/6995], lr: 0.000100, mv_avg_loss: 13.567816, loss: 10.996101
23:43:31.316848 Step[3800/6995], lr: 0.000100, mv_avg_loss: 12.079155, loss: 10.842606
23:43:36.091721 Step[3900/6995], lr: 0.000100, mv_avg_loss: 13.210068, loss: 15.235298
23:43:40.866565 Step[4000/6995], lr: 0.000100, mv_avg_loss: 12.728621, loss: 21.680828
23:43:45.635779 Step[4100/6995], lr: 0.000100, mv_avg_loss: 13.553165, loss: 7.502422
23:43:50.400894 Step[4200/6995], lr: 0.000100, mv_avg_loss: 12.312504, loss: 11.604492
23:43:55.179196 Step[4300/6995], lr: 0.000100, mv_avg_loss: 12.580614, loss: 11.148521
23:43:59.948434 Step[4400/6995], lr: 0.000100, mv_avg_loss: 13.973217, loss: 11.562101
23:44:04.720103 Step[4500/6995], lr: 0.000100, mv_avg_loss: 13.036708, loss: 10.858513
23:44:09.488127 Step[4600/6995], lr: 0.000100, mv_avg_loss: 12.241468, loss: 11.800667
23:44:14.251464 Step[4700/6995], lr: 0.000100, mv_avg_loss: 13.146299, loss: 7.573558
23:44:19.001674 Step[4800/6995], lr: 0.000100, mv_avg_loss: 13.061996, loss: 12.582190
23:44:23.772092 Step[4900/6995], lr: 0.000100, mv_avg_loss: 12.213277, loss: 8.656568
23:44:28.562109 Step[5000/6995], lr: 0.000100, mv_avg_loss: 13.287944, loss: 8.443838
23:44:33.340978 Step[5100/6995], lr: 0.000100, mv_avg_loss: 13.911896, loss: 12.671623
23:44:38.117826 Step[5200/6995], lr: 0.000100, mv_avg_loss: 13.841762, loss: 20.458473
23:44:42.889820 Step[5300/6995], lr: 0.000100, mv_avg_loss: 12.748583, loss: 11.499414
23:44:47.647197 Step[5400/6995], lr: 0.000100, mv_avg_loss: 13.482388, loss: 7.830723
23:44:52.428251 Step[5500/6995], lr: 0.000100, mv_avg_loss: 12.882638, loss: 12.230919
23:44:57.195761 Step[5600/6995], lr: 0.000100, mv_avg_loss: 12.476830, loss: 11.991415
23:45:01.972813 Step[5700/6995], lr: 0.000100, mv_avg_loss: 12.742254, loss: 20.256092
23:45:06.741508 Step[5800/6995], lr: 0.000100, mv_avg_loss: 12.805290, loss: 11.101810
23:45:11.509746 Step[5900/6995], lr: 0.000100, mv_avg_loss: 12.861090, loss: 14.315734
23:45:16.282739 Step[6000/6995], lr: 0.000100, mv_avg_loss: 12.839983, loss: 12.844489
23:45:21.059837 Step[6100/6995], lr: 0.000100, mv_avg_loss: 13.028693, loss: 19.059162
23:45:25.833206 Step[6200/6995], lr: 0.000100, mv_avg_loss: 13.025614, loss: 10.861588
23:45:30.604514 Step[6300/6995], lr: 0.000100, mv_avg_loss: 12.416958, loss: 9.862788
23:45:35.355256 Step[6400/6995], lr: 0.000100, mv_avg_loss: 13.217809, loss: 11.060161
23:45:40.118225 Step[6500/6995], lr: 0.000100, mv_avg_loss: 13.604931, loss: 12.169783
23:45:44.886528 Step[6600/6995], lr: 0.000100, mv_avg_loss: 13.290903, loss: 19.020149
23:45:49.645398 Step[6700/6995], lr: 0.000100, mv_avg_loss: 14.361246, loss: 18.448412
23:45:54.407640 Step[6800/6995], lr: 0.000100, mv_avg_loss: 12.897833, loss: 8.778367
23:45:59.177288 Step[6900/6995], lr: 0.000100, mv_avg_loss: 14.127633, loss: 7.914738
saving model for epoch 69
Testing for epoch: 69
Average test PNSR is 25.371627 for 500 images
Start to train epoch 70
23:46:25.745693 Step[0/6995], lr: 0.000100, mv_avg_loss: 12.554304, loss: 18.301605
23:46:30.489936 Step[100/6995], lr: 0.000100, mv_avg_loss: 13.585067, loss: 10.247862
23:46:35.235992 Step[200/6995], lr: 0.000100, mv_avg_loss: 13.334719, loss: 10.497494
23:46:39.985363 Step[300/6995], lr: 0.000100, mv_avg_loss: 12.444612, loss: 7.821138
23:46:44.774468 Step[400/6995], lr: 0.000100, mv_avg_loss: 14.187341, loss: 10.327724
23:46:49.513806 Step[500/6995], lr: 0.000100, mv_avg_loss: 13.137618, loss: 14.329634
23:46:54.256031 Step[600/6995], lr: 0.000100, mv_avg_loss: 13.423798, loss: 17.435818
23:46:59.003092 Step[700/6995], lr: 0.000100, mv_avg_loss: 12.864866, loss: 6.791280
23:47:03.774186 Step[800/6995], lr: 0.000100, mv_avg_loss: 12.935123, loss: 15.153008
23:47:08.519681 Step[900/6995], lr: 0.000100, mv_avg_loss: 13.856199, loss: 12.364276
23:47:13.271835 Step[1000/6995], lr: 0.000100, mv_avg_loss: 13.577394, loss: 15.122906
23:47:18.021327 Step[1100/6995], lr: 0.000100, mv_avg_loss: 13.021343, loss: 7.943603
23:47:22.784506 Step[1200/6995], lr: 0.000100, mv_avg_loss: 12.926393, loss: 7.958964
23:47:27.541037 Step[1300/6995], lr: 0.000100, mv_avg_loss: 13.129724, loss: 23.211325
23:47:32.293774 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.696445, loss: 16.933477
23:47:37.046010 Step[1500/6995], lr: 0.000100, mv_avg_loss: 12.847849, loss: 16.737782
23:47:41.804019 Step[1600/6995], lr: 0.000100, mv_avg_loss: 12.516276, loss: 18.844772
23:47:46.561469 Step[1700/6995], lr: 0.000100, mv_avg_loss: 13.131481, loss: 13.117183
23:47:51.309446 Step[1800/6995], lr: 0.000100, mv_avg_loss: 12.645840, loss: 14.353607
23:47:56.071799 Step[1900/6995], lr: 0.000100, mv_avg_loss: 12.584080, loss: 10.387146
23:48:00.840316 Step[2000/6995], lr: 0.000100, mv_avg_loss: 13.445597, loss: 21.464191
23:48:05.597068 Step[2100/6995], lr: 0.000100, mv_avg_loss: 13.182641, loss: 12.574176
23:48:10.354442 Step[2200/6995], lr: 0.000100, mv_avg_loss: 13.777317, loss: 20.902754
23:48:15.116747 Step[2300/6995], lr: 0.000100, mv_avg_loss: 13.202418, loss: 22.932325
23:48:19.860305 Step[2400/6995], lr: 0.000100, mv_avg_loss: 14.092997, loss: 10.604564
23:48:24.615276 Step[2500/6995], lr: 0.000100, mv_avg_loss: 12.121090, loss: 15.185816
23:48:29.377977 Step[2600/6995], lr: 0.000100, mv_avg_loss: 12.660074, loss: 14.600178
23:48:34.111483 Step[2700/6995], lr: 0.000100, mv_avg_loss: 13.443300, loss: 8.766085
23:48:38.850193 Step[2800/6995], lr: 0.000100, mv_avg_loss: 12.997953, loss: 12.182300
23:48:43.602468 Step[2900/6995], lr: 0.000100, mv_avg_loss: 13.336404, loss: 14.976894
23:48:48.343182 Step[3000/6995], lr: 0.000100, mv_avg_loss: 12.324265, loss: 10.085648
23:48:53.064152 Step[3100/6995], lr: 0.000100, mv_avg_loss: 14.596688, loss: 15.477272
23:48:57.806607 Step[3200/6995], lr: 0.000100, mv_avg_loss: 13.644890, loss: 9.196609
23:49:02.555746 Step[3300/6995], lr: 0.000100, mv_avg_loss: 12.888781, loss: 12.336619
23:49:07.300343 Step[3400/6995], lr: 0.000100, mv_avg_loss: 14.431851, loss: 19.519361
23:49:12.039383 Step[3500/6995], lr: 0.000100, mv_avg_loss: 13.376963, loss: 16.531363
23:49:16.764528 Step[3600/6995], lr: 0.000100, mv_avg_loss: 14.742776, loss: 16.480640
23:49:21.498557 Step[3700/6995], lr: 0.000100, mv_avg_loss: 12.468616, loss: 15.601667
23:49:26.241730 Step[3800/6995], lr: 0.000100, mv_avg_loss: 12.526927, loss: 12.584690
23:49:30.993510 Step[3900/6995], lr: 0.000100, mv_avg_loss: 13.433583, loss: 14.766596
23:49:35.740389 Step[4000/6995], lr: 0.000100, mv_avg_loss: 12.564964, loss: 15.134418
23:49:40.466237 Step[4100/6995], lr: 0.000100, mv_avg_loss: 12.492043, loss: 15.318817
23:49:45.200677 Step[4200/6995], lr: 0.000100, mv_avg_loss: 13.922112, loss: 13.475200
23:49:49.928176 Step[4300/6995], lr: 0.000100, mv_avg_loss: 13.352750, loss: 12.609068
23:49:54.660197 Step[4400/6995], lr: 0.000100, mv_avg_loss: 13.019745, loss: 11.377372
23:49:59.395721 Step[4500/6995], lr: 0.000100, mv_avg_loss: 14.045119, loss: 17.140219
23:50:04.125476 Step[4600/6995], lr: 0.000100, mv_avg_loss: 13.779138, loss: 8.600037
23:50:08.856649 Step[4700/6995], lr: 0.000100, mv_avg_loss: 13.780725, loss: 13.438372
23:50:13.563681 Step[4800/6995], lr: 0.000100, mv_avg_loss: 12.941513, loss: 24.395269
23:50:18.296629 Step[4900/6995], lr: 0.000100, mv_avg_loss: 13.369127, loss: 12.874823
23:50:23.032283 Step[5000/6995], lr: 0.000100, mv_avg_loss: 13.269432, loss: 8.932123
23:50:27.765299 Step[5100/6995], lr: 0.000100, mv_avg_loss: 12.807341, loss: 10.355824
23:50:32.495093 Step[5200/6995], lr: 0.000100, mv_avg_loss: 13.185686, loss: 13.447929
23:50:37.233793 Step[5300/6995], lr: 0.000100, mv_avg_loss: 12.079621, loss: 11.396500
23:50:42.017059 Step[5400/6995], lr: 0.000100, mv_avg_loss: 12.292890, loss: 12.913366
23:50:46.741018 Step[5500/6995], lr: 0.000100, mv_avg_loss: 12.983270, loss: 8.773705
23:50:51.485531 Step[5600/6995], lr: 0.000100, mv_avg_loss: 14.050609, loss: 14.031428
23:50:56.213973 Step[5700/6995], lr: 0.000100, mv_avg_loss: 13.756219, loss: 13.593687
23:51:00.951564 Step[5800/6995], lr: 0.000100, mv_avg_loss: 12.717710, loss: 13.698417
23:51:05.683250 Step[5900/6995], lr: 0.000100, mv_avg_loss: 13.368923, loss: 8.074160
23:51:10.418494 Step[6000/6995], lr: 0.000100, mv_avg_loss: 12.900871, loss: 13.705233
23:51:15.167086 Step[6100/6995], lr: 0.000100, mv_avg_loss: 12.968831, loss: 10.171133
23:51:19.899183 Step[6200/6995], lr: 0.000100, mv_avg_loss: 13.062125, loss: 10.803706
23:51:24.636598 Step[6300/6995], lr: 0.000100, mv_avg_loss: 12.507277, loss: 14.767929
23:51:29.416061 Step[6400/6995], lr: 0.000100, mv_avg_loss: 13.263112, loss: 14.104156
23:51:34.194363 Step[6500/6995], lr: 0.000100, mv_avg_loss: 13.287273, loss: 12.608066
23:51:38.930228 Step[6600/6995], lr: 0.000100, mv_avg_loss: 13.221516, loss: 8.871117
23:51:43.662028 Step[6700/6995], lr: 0.000100, mv_avg_loss: 12.460137, loss: 12.419596
23:51:48.408104 Step[6800/6995], lr: 0.000100, mv_avg_loss: 14.286941, loss: 9.505900
23:51:53.143508 Step[6900/6995], lr: 0.000100, mv_avg_loss: 13.535784, loss: 13.688747
Testing for epoch: 70
Average test PNSR is 25.366010 for 500 images
Start to train epoch 71
23:52:19.789528 Step[0/6995], lr: 0.000100, mv_avg_loss: 12.690491, loss: 10.374130
23:52:24.517365 Step[100/6995], lr: 0.000100, mv_avg_loss: 13.345417, loss: 14.556582
23:52:29.246343 Step[200/6995], lr: 0.000100, mv_avg_loss: 13.161937, loss: 13.050792
23:52:33.985123 Step[300/6995], lr: 0.000100, mv_avg_loss: 12.824413, loss: 14.128264
23:52:38.714335 Step[400/6995], lr: 0.000100, mv_avg_loss: 12.915577, loss: 11.729088
23:52:43.455954 Step[500/6995], lr: 0.000100, mv_avg_loss: 13.329247, loss: 14.865924
23:52:48.190026 Step[600/6995], lr: 0.000100, mv_avg_loss: 12.880111, loss: 8.927345
23:52:52.917323 Step[700/6995], lr: 0.000100, mv_avg_loss: 13.017134, loss: 10.557404
23:52:57.650894 Step[800/6995], lr: 0.000100, mv_avg_loss: 13.009624, loss: 8.917480
23:53:02.381343 Step[900/6995], lr: 0.000100, mv_avg_loss: 12.825022, loss: 11.328302
23:53:07.120336 Step[1000/6995], lr: 0.000100, mv_avg_loss: 12.336609, loss: 8.933547
23:53:11.853639 Step[1100/6995], lr: 0.000100, mv_avg_loss: 13.622167, loss: 12.081131
23:53:16.592408 Step[1200/6995], lr: 0.000100, mv_avg_loss: 13.160537, loss: 16.758476
23:53:21.326499 Step[1300/6995], lr: 0.000100, mv_avg_loss: 12.796036, loss: 9.018604
23:53:26.038052 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.712197, loss: 18.472603
23:53:30.776324 Step[1500/6995], lr: 0.000100, mv_avg_loss: 13.400308, loss: 18.137396
23:53:35.517424 Step[1600/6995], lr: 0.000100, mv_avg_loss: 12.413278, loss: 14.029108
23:53:40.262745 Step[1700/6995], lr: 0.000100, mv_avg_loss: 13.132991, loss: 9.697944
23:53:45.007881 Step[1800/6995], lr: 0.000100, mv_avg_loss: 11.769097, loss: 9.245678
23:53:49.742832 Step[1900/6995], lr: 0.000100, mv_avg_loss: 12.652893, loss: 12.333853
23:53:54.485929 Step[2000/6995], lr: 0.000100, mv_avg_loss: 12.443248, loss: 13.419540
23:53:59.235286 Step[2100/6995], lr: 0.000100, mv_avg_loss: 13.117198, loss: 10.184374
23:54:03.969156 Step[2200/6995], lr: 0.000100, mv_avg_loss: 12.297614, loss: 8.837872
23:54:08.726745 Step[2300/6995], lr: 0.000100, mv_avg_loss: 13.017685, loss: 10.184467
23:54:13.474504 Step[2400/6995], lr: 0.000100, mv_avg_loss: 12.511751, loss: 10.123560
23:54:18.220853 Step[2500/6995], lr: 0.000100, mv_avg_loss: 14.413723, loss: 24.300873
23:54:22.960780 Step[2600/6995], lr: 0.000100, mv_avg_loss: 13.859173, loss: 23.920080
23:54:27.698844 Step[2700/6995], lr: 0.000100, mv_avg_loss: 12.921722, loss: 12.314532
23:54:32.436128 Step[2800/6995], lr: 0.000100, mv_avg_loss: 12.895514, loss: 12.649107
23:54:37.178521 Step[2900/6995], lr: 0.000100, mv_avg_loss: 12.932719, loss: 10.643107
23:54:41.922259 Step[3000/6995], lr: 0.000100, mv_avg_loss: 13.150117, loss: 9.516867
23:54:46.687398 Step[3100/6995], lr: 0.000100, mv_avg_loss: 12.958810, loss: 11.301768
23:54:51.450554 Step[3200/6995], lr: 0.000100, mv_avg_loss: 11.983962, loss: 13.046897
23:54:56.202889 Step[3300/6995], lr: 0.000100, mv_avg_loss: 13.246632, loss: 6.898922
23:55:00.973543 Step[3400/6995], lr: 0.000100, mv_avg_loss: 12.446054, loss: 11.064460
23:55:05.715519 Step[3500/6995], lr: 0.000100, mv_avg_loss: 12.639884, loss: 10.302761
23:55:10.463209 Step[3600/6995], lr: 0.000100, mv_avg_loss: 12.355817, loss: 13.160534
23:55:15.216410 Step[3700/6995], lr: 0.000100, mv_avg_loss: 13.193163, loss: 16.192577
23:55:19.961034 Step[3800/6995], lr: 0.000100, mv_avg_loss: 12.903173, loss: 13.281764
23:55:24.710786 Step[3900/6995], lr: 0.000100, mv_avg_loss: 12.658275, loss: 18.454136
23:55:29.462984 Step[4000/6995], lr: 0.000100, mv_avg_loss: 13.063182, loss: 11.084745
23:55:34.212009 Step[4100/6995], lr: 0.000100, mv_avg_loss: 13.270675, loss: 13.519226
23:55:38.958632 Step[4200/6995], lr: 0.000100, mv_avg_loss: 13.556161, loss: 18.170692
23:55:43.712318 Step[4300/6995], lr: 0.000100, mv_avg_loss: 13.247593, loss: 20.914513
23:55:48.462181 Step[4400/6995], lr: 0.000100, mv_avg_loss: 13.365014, loss: 11.259130
23:55:53.201520 Step[4500/6995], lr: 0.000100, mv_avg_loss: 13.271916, loss: 13.824028
23:55:57.952959 Step[4600/6995], lr: 0.000100, mv_avg_loss: 13.380569, loss: 18.127991
23:56:02.706634 Step[4700/6995], lr: 0.000100, mv_avg_loss: 14.020676, loss: 18.015539
23:56:07.460717 Step[4800/6995], lr: 0.000100, mv_avg_loss: 13.060534, loss: 8.680315
23:56:12.216067 Step[4900/6995], lr: 0.000100, mv_avg_loss: 13.300865, loss: 9.819841
23:56:16.960874 Step[5000/6995], lr: 0.000100, mv_avg_loss: 12.319647, loss: 10.261301
23:56:21.836676 Step[5100/6995], lr: 0.000100, mv_avg_loss: 13.084188, loss: 10.191833
23:56:26.583020 Step[5200/6995], lr: 0.000100, mv_avg_loss: 13.467623, loss: 9.881828
23:56:31.344480 Step[5300/6995], lr: 0.000100, mv_avg_loss: 12.696752, loss: 9.120745
23:56:36.094276 Step[5400/6995], lr: 0.000100, mv_avg_loss: 13.369544, loss: 9.475823
23:56:40.852874 Step[5500/6995], lr: 0.000100, mv_avg_loss: 13.269279, loss: 10.481661
23:56:45.605706 Step[5600/6995], lr: 0.000100, mv_avg_loss: 12.062437, loss: 11.472248
23:56:50.346470 Step[5700/6995], lr: 0.000100, mv_avg_loss: 14.401546, loss: 9.459391
23:56:55.097737 Step[5800/6995], lr: 0.000100, mv_avg_loss: 14.892275, loss: 20.328833
23:56:59.848854 Step[5900/6995], lr: 0.000100, mv_avg_loss: 13.681921, loss: 11.106441
23:57:04.611648 Step[6000/6995], lr: 0.000100, mv_avg_loss: 13.624487, loss: 14.636292
23:57:09.366962 Step[6100/6995], lr: 0.000100, mv_avg_loss: 13.038849, loss: 15.488551
23:57:14.118315 Step[6200/6995], lr: 0.000100, mv_avg_loss: 12.762888, loss: 30.165792
23:57:18.869586 Step[6300/6995], lr: 0.000100, mv_avg_loss: 13.194190, loss: 16.370878
23:57:23.619727 Step[6400/6995], lr: 0.000100, mv_avg_loss: 13.167884, loss: 10.985947
23:57:28.367636 Step[6500/6995], lr: 0.000100, mv_avg_loss: 12.839173, loss: 8.666567
23:57:33.112118 Step[6600/6995], lr: 0.000100, mv_avg_loss: 12.823698, loss: 18.977224
23:57:37.867709 Step[6700/6995], lr: 0.000100, mv_avg_loss: 13.557265, loss: 11.211128
23:57:42.617152 Step[6800/6995], lr: 0.000100, mv_avg_loss: 13.030384, loss: 8.049610
23:57:47.370277 Step[6900/6995], lr: 0.000100, mv_avg_loss: 13.312406, loss: 8.305850
Testing for epoch: 71
Average test PNSR is 25.341398 for 500 images
Start to train epoch 72
23:58:13.597370 Step[0/6995], lr: 0.000100, mv_avg_loss: 13.240828, loss: 11.131885
23:58:18.335964 Step[100/6995], lr: 0.000100, mv_avg_loss: 11.823619, loss: 13.176046
23:58:23.079836 Step[200/6995], lr: 0.000100, mv_avg_loss: 13.376225, loss: 26.185080
23:58:27.812605 Step[300/6995], lr: 0.000100, mv_avg_loss: 13.128912, loss: 12.206955
23:58:32.549958 Step[400/6995], lr: 0.000100, mv_avg_loss: 12.984629, loss: 10.429538
23:58:37.284003 Step[500/6995], lr: 0.000100, mv_avg_loss: 13.760878, loss: 15.309268
23:58:42.012696 Step[600/6995], lr: 0.000100, mv_avg_loss: 12.616582, loss: 14.546661
23:58:46.756999 Step[700/6995], lr: 0.000100, mv_avg_loss: 14.350683, loss: 15.140262
23:58:51.494604 Step[800/6995], lr: 0.000100, mv_avg_loss: 12.609415, loss: 16.968550
23:58:56.227330 Step[900/6995], lr: 0.000100, mv_avg_loss: 13.830211, loss: 21.176319
23:59:00.964990 Step[1000/6995], lr: 0.000100, mv_avg_loss: 13.292564, loss: 11.664226
23:59:05.701466 Step[1100/6995], lr: 0.000100, mv_avg_loss: 12.995877, loss: 15.477751
23:59:10.428207 Step[1200/6995], lr: 0.000100, mv_avg_loss: 12.588673, loss: 9.057388
23:59:15.156844 Step[1300/6995], lr: 0.000100, mv_avg_loss: 13.010188, loss: 8.536537
23:59:19.907575 Step[1400/6995], lr: 0.000100, mv_avg_loss: 12.691535, loss: 21.567772
23:59:24.638210 Step[1500/6995], lr: 0.000100, mv_avg_loss: 12.735353, loss: 9.668909
23:59:29.376089 Step[1600/6995], lr: 0.000100, mv_avg_loss: 12.424779, loss: 15.403773
23:59:34.110448 Step[1700/6995], lr: 0.000100, mv_avg_loss: 13.256350, loss: 15.113838
23:59:38.837742 Step[1800/6995], lr: 0.000100, mv_avg_loss: 13.143807, loss: 19.404911
23:59:43.563728 Step[1900/6995], lr: 0.000100, mv_avg_loss: 13.554874, loss: 10.004930
23:59:48.294145 Step[2000/6995], lr: 0.000100, mv_avg_loss: 12.964489, loss: 16.696781
23:59:53.029898 Step[2100/6995], lr: 0.000100, mv_avg_loss: 13.284444, loss: 9.823861
23:59:57.759840 Step[2200/6995], lr: 0.000100, mv_avg_loss: 13.168788, loss: 9.921368
00:00:02.496479 Step[2300/6995], lr: 0.000100, mv_avg_loss: 13.517701, loss: 12.767649
00:00:07.227655 Step[2400/6995], lr: 0.000100, mv_avg_loss: 11.259801, loss: 8.906397
00:00:11.976834 Step[2500/6995], lr: 0.000100, mv_avg_loss: 12.275220, loss: 8.882666
00:00:16.710010 Step[2600/6995], lr: 0.000100, mv_avg_loss: 12.059170, loss: 11.211729
00:00:21.432418 Step[2700/6995], lr: 0.000100, mv_avg_loss: 13.498652, loss: 10.461440
00:00:26.149611 Step[2800/6995], lr: 0.000100, mv_avg_loss: 12.980464, loss: 12.860891
00:00:30.870789 Step[2900/6995], lr: 0.000100, mv_avg_loss: 13.949556, loss: 24.613033
00:00:35.594350 Step[3000/6995], lr: 0.000100, mv_avg_loss: 13.162006, loss: 13.039455
00:00:40.318338 Step[3100/6995], lr: 0.000100, mv_avg_loss: 12.929200, loss: 8.571245
00:00:45.048188 Step[3200/6995], lr: 0.000100, mv_avg_loss: 13.278857, loss: 19.437973
00:00:49.789702 Step[3300/6995], lr: 0.000100, mv_avg_loss: 12.619846, loss: 9.245372
00:00:54.526273 Step[3400/6995], lr: 0.000100, mv_avg_loss: 12.632192, loss: 11.988127
00:00:59.255608 Step[3500/6995], lr: 0.000100, mv_avg_loss: 13.108456, loss: 10.998554
00:01:03.994563 Step[3600/6995], lr: 0.000100, mv_avg_loss: 12.349812, loss: 11.100444
00:01:08.727130 Step[3700/6995], lr: 0.000100, mv_avg_loss: 12.525388, loss: 10.980494
00:01:13.461091 Step[3800/6995], lr: 0.000100, mv_avg_loss: 12.844867, loss: 11.486011
00:01:18.208629 Step[3900/6995], lr: 0.000100, mv_avg_loss: 13.991618, loss: 12.296176
00:01:22.946544 Step[4000/6995], lr: 0.000100, mv_avg_loss: 12.483524, loss: 7.540476
00:01:27.675885 Step[4100/6995], lr: 0.000100, mv_avg_loss: 12.892667, loss: 9.533278
00:01:32.403715 Step[4200/6995], lr: 0.000100, mv_avg_loss: 12.056096, loss: 11.210714
00:01:37.132380 Step[4300/6995], lr: 0.000100, mv_avg_loss: 12.463922, loss: 7.919624
00:01:41.852556 Step[4400/6995], lr: 0.000100, mv_avg_loss: 12.774604, loss: 9.514584
00:01:46.593290 Step[4500/6995], lr: 0.000100, mv_avg_loss: 12.570039, loss: 11.123343
00:01:51.316681 Step[4600/6995], lr: 0.000100, mv_avg_loss: 14.209269, loss: 12.631247
00:01:56.051038 Step[4700/6995], lr: 0.000100, mv_avg_loss: 13.248318, loss: 18.490993
00:02:00.783830 Step[4800/6995], lr: 0.000100, mv_avg_loss: 13.845622, loss: 14.239956
00:02:05.508830 Step[4900/6995], lr: 0.000100, mv_avg_loss: 12.722043, loss: 13.773692
00:02:10.241898 Step[5000/6995], lr: 0.000100, mv_avg_loss: 13.468202, loss: 12.186890
00:02:14.979217 Step[5100/6995], lr: 0.000100, mv_avg_loss: 12.692510, loss: 14.873913
00:02:19.709769 Step[5200/6995], lr: 0.000100, mv_avg_loss: 13.893794, loss: 10.367648
00:02:24.443713 Step[5300/6995], lr: 0.000100, mv_avg_loss: 12.835649, loss: 9.928305
00:02:29.171219 Step[5400/6995], lr: 0.000100, mv_avg_loss: 13.419087, loss: 9.243536
00:02:33.910392 Step[5500/6995], lr: 0.000100, mv_avg_loss: 11.984240, loss: 8.297846
00:02:38.655457 Step[5600/6995], lr: 0.000100, mv_avg_loss: 12.631414, loss: 8.489128
00:02:43.385181 Step[5700/6995], lr: 0.000100, mv_avg_loss: 13.948742, loss: 8.520212
00:02:48.116769 Step[5800/6995], lr: 0.000100, mv_avg_loss: 13.356049, loss: 9.194425
00:02:52.843129 Step[5900/6995], lr: 0.000100, mv_avg_loss: 12.133385, loss: 10.063065
00:02:57.571057 Step[6000/6995], lr: 0.000100, mv_avg_loss: 12.974200, loss: 14.879344
00:03:02.312833 Step[6100/6995], lr: 0.000100, mv_avg_loss: 12.852928, loss: 9.836743
00:03:07.048585 Step[6200/6995], lr: 0.000100, mv_avg_loss: 12.108601, loss: 10.877930
00:03:11.787793 Step[6300/6995], lr: 0.000100, mv_avg_loss: 12.983346, loss: 16.734695
00:03:16.540672 Step[6400/6995], lr: 0.000100, mv_avg_loss: 13.491555, loss: 10.977747
00:03:21.288123 Step[6500/6995], lr: 0.000100, mv_avg_loss: 12.891201, loss: 11.107184
00:03:26.020330 Step[6600/6995], lr: 0.000100, mv_avg_loss: 13.988605, loss: 10.707867
00:03:30.753972 Step[6700/6995], lr: 0.000100, mv_avg_loss: 12.937881, loss: 12.134460
00:03:35.483557 Step[6800/6995], lr: 0.000100, mv_avg_loss: 11.854813, loss: 12.440660
00:03:40.221261 Step[6900/6995], lr: 0.000100, mv_avg_loss: 12.479624, loss: 11.900246
Testing for epoch: 72
Average test PNSR is 25.332895 for 500 images
Start to train epoch 73
00:04:06.346342 Step[0/6995], lr: 0.000100, mv_avg_loss: 13.558231, loss: 20.927855
00:04:11.045007 Step[100/6995], lr: 0.000100, mv_avg_loss: 12.818666, loss: 12.011915
00:04:15.758212 Step[200/6995], lr: 0.000100, mv_avg_loss: 12.331928, loss: 11.042685
00:04:20.463672 Step[300/6995], lr: 0.000100, mv_avg_loss: 12.667966, loss: 15.479793
00:04:25.193157 Step[400/6995], lr: 0.000100, mv_avg_loss: 13.082750, loss: 14.665244
00:04:29.912416 Step[500/6995], lr: 0.000100, mv_avg_loss: 12.642656, loss: 12.368191
00:04:34.623799 Step[600/6995], lr: 0.000100, mv_avg_loss: 12.104925, loss: 8.676448
00:04:39.334061 Step[700/6995], lr: 0.000100, mv_avg_loss: 12.822020, loss: 12.863643
00:04:44.048144 Step[800/6995], lr: 0.000100, mv_avg_loss: 13.172087, loss: 13.641851
00:04:48.755352 Step[900/6995], lr: 0.000100, mv_avg_loss: 12.318430, loss: 12.044544
00:04:53.457174 Step[1000/6995], lr: 0.000100, mv_avg_loss: 12.193174, loss: 13.626163
00:04:58.183715 Step[1100/6995], lr: 0.000100, mv_avg_loss: 13.507854, loss: 10.838069
00:05:02.881598 Step[1200/6995], lr: 0.000100, mv_avg_loss: 13.659777, loss: 14.161674
00:05:07.592759 Step[1300/6995], lr: 0.000100, mv_avg_loss: 14.263718, loss: 17.053318
00:05:12.305777 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.832246, loss: 14.708967
00:05:17.024996 Step[1500/6995], lr: 0.000100, mv_avg_loss: 12.949902, loss: 12.408984
00:05:21.759245 Step[1600/6995], lr: 0.000100, mv_avg_loss: 11.925351, loss: 7.650354
00:05:26.487866 Step[1700/6995], lr: 0.000100, mv_avg_loss: 13.022786, loss: 8.589315
00:05:31.227236 Step[1800/6995], lr: 0.000100, mv_avg_loss: 11.880805, loss: 18.167112
00:05:35.961102 Step[1900/6995], lr: 0.000100, mv_avg_loss: 13.632472, loss: 17.080177
00:05:40.697639 Step[2000/6995], lr: 0.000100, mv_avg_loss: 13.768596, loss: 10.719104
00:05:45.437068 Step[2100/6995], lr: 0.000100, mv_avg_loss: 13.536530, loss: 13.362648
00:05:50.157311 Step[2200/6995], lr: 0.000100, mv_avg_loss: 13.494433, loss: 13.897220
00:05:54.892630 Step[2300/6995], lr: 0.000100, mv_avg_loss: 14.262218, loss: 27.454006
00:05:59.630475 Step[2400/6995], lr: 0.000100, mv_avg_loss: 12.622826, loss: 20.039894
00:06:04.365182 Step[2500/6995], lr: 0.000100, mv_avg_loss: 12.454268, loss: 12.224853
00:06:09.096323 Step[2600/6995], lr: 0.000100, mv_avg_loss: 14.352254, loss: 10.965590
00:06:13.838580 Step[2700/6995], lr: 0.000100, mv_avg_loss: 13.573939, loss: 8.342028
00:06:18.579997 Step[2800/6995], lr: 0.000100, mv_avg_loss: 12.619272, loss: 8.593380
00:06:23.308224 Step[2900/6995], lr: 0.000100, mv_avg_loss: 13.587407, loss: 11.363120
00:06:28.024151 Step[3000/6995], lr: 0.000100, mv_avg_loss: 12.886257, loss: 9.697024
00:06:32.753046 Step[3100/6995], lr: 0.000100, mv_avg_loss: 12.171038, loss: 12.329542
00:06:37.492718 Step[3200/6995], lr: 0.000100, mv_avg_loss: 13.483387, loss: 17.424503
00:06:42.214659 Step[3300/6995], lr: 0.000100, mv_avg_loss: 12.599641, loss: 11.131224
00:06:46.932856 Step[3400/6995], lr: 0.000100, mv_avg_loss: 14.004811, loss: 8.162613
00:06:51.665796 Step[3500/6995], lr: 0.000100, mv_avg_loss: 12.611329, loss: 13.487621
00:06:56.396262 Step[3600/6995], lr: 0.000100, mv_avg_loss: 12.902903, loss: 11.332479
00:07:01.140464 Step[3700/6995], lr: 0.000100, mv_avg_loss: 13.325154, loss: 11.505430
00:07:05.875141 Step[3800/6995], lr: 0.000100, mv_avg_loss: 12.022191, loss: 10.658142
00:07:10.610951 Step[3900/6995], lr: 0.000100, mv_avg_loss: 12.701157, loss: 14.343336
00:07:15.340914 Step[4000/6995], lr: 0.000100, mv_avg_loss: 12.746632, loss: 16.248318
00:07:20.067019 Step[4100/6995], lr: 0.000100, mv_avg_loss: 12.844213, loss: 12.829021
00:07:24.798701 Step[4200/6995], lr: 0.000100, mv_avg_loss: 13.396598, loss: 13.700541
00:07:29.551478 Step[4300/6995], lr: 0.000100, mv_avg_loss: 12.492926, loss: 8.653644
00:07:34.320324 Step[4400/6995], lr: 0.000100, mv_avg_loss: 12.147079, loss: 12.266692
00:07:39.060258 Step[4500/6995], lr: 0.000100, mv_avg_loss: 13.099135, loss: 16.982964
00:07:43.792696 Step[4600/6995], lr: 0.000100, mv_avg_loss: 13.281398, loss: 10.254967
00:07:48.526891 Step[4700/6995], lr: 0.000100, mv_avg_loss: 14.250798, loss: 16.783516
00:07:53.285555 Step[4800/6995], lr: 0.000100, mv_avg_loss: 13.052407, loss: 21.289490
00:07:58.027953 Step[4900/6995], lr: 0.000100, mv_avg_loss: 12.660449, loss: 16.171219
00:08:02.763830 Step[5000/6995], lr: 0.000100, mv_avg_loss: 12.450869, loss: 12.577539
00:08:07.503591 Step[5100/6995], lr: 0.000100, mv_avg_loss: 12.488508, loss: 10.191064
00:08:12.238125 Step[5200/6995], lr: 0.000100, mv_avg_loss: 12.321124, loss: 8.407284
00:08:16.968421 Step[5300/6995], lr: 0.000100, mv_avg_loss: 12.796674, loss: 7.926517
00:08:21.710656 Step[5400/6995], lr: 0.000100, mv_avg_loss: 13.201612, loss: 17.041618
00:08:26.453590 Step[5500/6995], lr: 0.000100, mv_avg_loss: 12.399724, loss: 12.601896
00:08:31.193368 Step[5600/6995], lr: 0.000100, mv_avg_loss: 13.975784, loss: 16.176731
00:08:35.925975 Step[5700/6995], lr: 0.000100, mv_avg_loss: 12.709564, loss: 11.713331
00:08:40.653180 Step[5800/6995], lr: 0.000100, mv_avg_loss: 13.538319, loss: 11.881537
00:08:45.406157 Step[5900/6995], lr: 0.000100, mv_avg_loss: 13.318154, loss: 17.739601
00:08:50.157289 Step[6000/6995], lr: 0.000100, mv_avg_loss: 12.533230, loss: 12.559343
00:08:54.907912 Step[6100/6995], lr: 0.000100, mv_avg_loss: 13.005439, loss: 10.236353
00:08:59.648422 Step[6200/6995], lr: 0.000100, mv_avg_loss: 13.116398, loss: 13.286598
00:09:04.391945 Step[6300/6995], lr: 0.000100, mv_avg_loss: 13.864696, loss: 9.596587
00:09:09.128288 Step[6400/6995], lr: 0.000100, mv_avg_loss: 12.777607, loss: 11.477398
00:09:13.870676 Step[6500/6995], lr: 0.000100, mv_avg_loss: 13.235233, loss: 16.797920
00:09:18.619491 Step[6600/6995], lr: 0.000100, mv_avg_loss: 12.474943, loss: 17.062592
00:09:23.368596 Step[6700/6995], lr: 0.000100, mv_avg_loss: 13.462633, loss: 9.255602
00:09:28.124312 Step[6800/6995], lr: 0.000100, mv_avg_loss: 13.162910, loss: 9.566302
00:09:32.870071 Step[6900/6995], lr: 0.000100, mv_avg_loss: 14.025630, loss: 16.428322
Testing for epoch: 73
Average test PNSR is 25.354530 for 500 images
Start to train epoch 74
00:09:58.987144 Step[0/6995], lr: 0.000100, mv_avg_loss: 13.096973, loss: 10.190630
00:10:03.713201 Step[100/6995], lr: 0.000100, mv_avg_loss: 14.068283, loss: 10.319484
00:10:08.450698 Step[200/6995], lr: 0.000100, mv_avg_loss: 13.061482, loss: 19.363529
00:10:13.180249 Step[300/6995], lr: 0.000100, mv_avg_loss: 13.221385, loss: 11.861031
00:10:17.935537 Step[400/6995], lr: 0.000100, mv_avg_loss: 13.441587, loss: 21.459326
00:10:22.674843 Step[500/6995], lr: 0.000100, mv_avg_loss: 12.228156, loss: 17.833593
00:10:27.422143 Step[600/6995], lr: 0.000100, mv_avg_loss: 13.572715, loss: 7.985076
00:10:32.167708 Step[700/6995], lr: 0.000100, mv_avg_loss: 12.934060, loss: 16.931286
00:10:36.908830 Step[800/6995], lr: 0.000100, mv_avg_loss: 12.841276, loss: 11.534709
00:10:41.639988 Step[900/6995], lr: 0.000100, mv_avg_loss: 12.339081, loss: 10.424076
00:10:46.375656 Step[1000/6995], lr: 0.000100, mv_avg_loss: 12.945818, loss: 8.302299
00:10:51.126108 Step[1100/6995], lr: 0.000100, mv_avg_loss: 13.311333, loss: 13.959576
00:10:55.876852 Step[1200/6995], lr: 0.000100, mv_avg_loss: 12.944952, loss: 8.694419
00:11:00.624960 Step[1300/6995], lr: 0.000100, mv_avg_loss: 12.902211, loss: 11.607699
00:11:05.355858 Step[1400/6995], lr: 0.000100, mv_avg_loss: 12.071913, loss: 10.351853
00:11:10.096675 Step[1500/6995], lr: 0.000100, mv_avg_loss: 13.366037, loss: 10.123449
00:11:14.851465 Step[1600/6995], lr: 0.000100, mv_avg_loss: 12.965394, loss: 12.637660
00:11:19.612277 Step[1700/6995], lr: 0.000100, mv_avg_loss: 12.400837, loss: 15.455070
00:11:24.350167 Step[1800/6995], lr: 0.000100, mv_avg_loss: 13.575332, loss: 15.119722
00:11:29.106589 Step[1900/6995], lr: 0.000100, mv_avg_loss: 12.640913, loss: 10.337877
00:11:33.857371 Step[2000/6995], lr: 0.000100, mv_avg_loss: 12.389855, loss: 22.318327
00:11:38.619677 Step[2100/6995], lr: 0.000100, mv_avg_loss: 12.642905, loss: 9.444617
00:11:43.369777 Step[2200/6995], lr: 0.000100, mv_avg_loss: 12.758811, loss: 14.042969
00:11:48.115453 Step[2300/6995], lr: 0.000100, mv_avg_loss: 13.535448, loss: 12.821211
00:11:52.896013 Step[2400/6995], lr: 0.000100, mv_avg_loss: 12.455621, loss: 9.425174
00:11:57.651945 Step[2500/6995], lr: 0.000100, mv_avg_loss: 14.054723, loss: 18.532927
00:12:02.403986 Step[2600/6995], lr: 0.000100, mv_avg_loss: 13.037565, loss: 8.881510
00:12:07.148523 Step[2700/6995], lr: 0.000100, mv_avg_loss: 13.692842, loss: 17.246046
00:12:11.904157 Step[2800/6995], lr: 0.000100, mv_avg_loss: 12.874217, loss: 11.741615
00:12:16.643112 Step[2900/6995], lr: 0.000100, mv_avg_loss: 13.337897, loss: 11.265536
00:12:21.383418 Step[3000/6995], lr: 0.000100, mv_avg_loss: 13.136991, loss: 14.229625
00:12:26.127594 Step[3100/6995], lr: 0.000100, mv_avg_loss: 13.771804, loss: 9.085436
00:12:30.876767 Step[3200/6995], lr: 0.000100, mv_avg_loss: 13.270362, loss: 11.976856
00:12:35.616785 Step[3300/6995], lr: 0.000100, mv_avg_loss: 13.448078, loss: 9.214466
00:12:40.363498 Step[3400/6995], lr: 0.000100, mv_avg_loss: 14.374408, loss: 9.113153
00:12:45.111337 Step[3500/6995], lr: 0.000100, mv_avg_loss: 11.956189, loss: 9.080690
00:12:49.852329 Step[3600/6995], lr: 0.000100, mv_avg_loss: 12.968724, loss: 12.542284
00:12:54.595490 Step[3700/6995], lr: 0.000100, mv_avg_loss: 12.622565, loss: 11.194231
00:12:59.338522 Step[3800/6995], lr: 0.000100, mv_avg_loss: 13.134838, loss: 9.167057
00:13:04.092261 Step[3900/6995], lr: 0.000100, mv_avg_loss: 12.512695, loss: 8.496075
00:13:08.843197 Step[4000/6995], lr: 0.000100, mv_avg_loss: 13.701112, loss: 11.116043
00:13:13.579090 Step[4100/6995], lr: 0.000100, mv_avg_loss: 13.649843, loss: 17.561749
00:13:18.314482 Step[4200/6995], lr: 0.000100, mv_avg_loss: 13.349843, loss: 14.661147
00:13:23.060205 Step[4300/6995], lr: 0.000100, mv_avg_loss: 13.462475, loss: 9.490241
00:13:27.797825 Step[4400/6995], lr: 0.000100, mv_avg_loss: 12.855953, loss: 10.372341
00:13:32.542646 Step[4500/6995], lr: 0.000100, mv_avg_loss: 13.944063, loss: 11.984190
00:13:37.293117 Step[4600/6995], lr: 0.000100, mv_avg_loss: 12.179206, loss: 11.373663
00:13:42.039653 Step[4700/6995], lr: 0.000100, mv_avg_loss: 12.680382, loss: 21.214571
00:13:46.772796 Step[4800/6995], lr: 0.000100, mv_avg_loss: 12.533063, loss: 12.605925
00:13:51.510913 Step[4900/6995], lr: 0.000100, mv_avg_loss: 11.882971, loss: 10.508219
00:13:56.252719 Step[5000/6995], lr: 0.000100, mv_avg_loss: 13.232311, loss: 14.219429
00:14:00.987851 Step[5100/6995], lr: 0.000100, mv_avg_loss: 12.684026, loss: 18.569538
00:14:05.725534 Step[5200/6995], lr: 0.000100, mv_avg_loss: 12.820086, loss: 11.413046
00:14:10.471629 Step[5300/6995], lr: 0.000100, mv_avg_loss: 12.869308, loss: 10.652009
00:14:15.207774 Step[5400/6995], lr: 0.000100, mv_avg_loss: 12.618195, loss: 10.820461
00:14:19.948699 Step[5500/6995], lr: 0.000100, mv_avg_loss: 13.862724, loss: 12.403027
00:14:24.657936 Step[5600/6995], lr: 0.000100, mv_avg_loss: 12.129598, loss: 14.030379
00:14:29.390587 Step[5700/6995], lr: 0.000100, mv_avg_loss: 14.011817, loss: 10.557482
00:14:34.128322 Step[5800/6995], lr: 0.000100, mv_avg_loss: 12.482393, loss: 12.466379
00:14:38.850530 Step[5900/6995], lr: 0.000100, mv_avg_loss: 13.408896, loss: 10.204571
00:14:43.585569 Step[6000/6995], lr: 0.000100, mv_avg_loss: 13.646781, loss: 14.132469
00:14:48.324241 Step[6100/6995], lr: 0.000100, mv_avg_loss: 12.451551, loss: 17.189404
00:14:53.050275 Step[6200/6995], lr: 0.000100, mv_avg_loss: 13.568303, loss: 11.430834
00:14:57.793358 Step[6300/6995], lr: 0.000100, mv_avg_loss: 12.106604, loss: 9.741061
00:15:02.526258 Step[6400/6995], lr: 0.000100, mv_avg_loss: 13.907116, loss: 14.545773
00:15:07.246653 Step[6500/6995], lr: 0.000100, mv_avg_loss: 12.980116, loss: 8.392911
00:15:11.978933 Step[6600/6995], lr: 0.000100, mv_avg_loss: 12.046515, loss: 8.917038
00:15:16.710696 Step[6700/6995], lr: 0.000100, mv_avg_loss: 13.077357, loss: 12.058482
00:15:21.451114 Step[6800/6995], lr: 0.000100, mv_avg_loss: 12.468534, loss: 13.262220
00:15:26.177498 Step[6900/6995], lr: 0.000100, mv_avg_loss: 12.417686, loss: 14.247848
saving model for epoch 74
Testing for epoch: 74
Average test PNSR is 25.346726 for 500 images
Start to train epoch 75
00:15:52.398840 Step[0/6995], lr: 0.000100, mv_avg_loss: 12.911922, loss: 12.156500
00:15:57.112294 Step[100/6995], lr: 0.000100, mv_avg_loss: 12.838279, loss: 11.021757
00:16:01.850105 Step[200/6995], lr: 0.000100, mv_avg_loss: 14.036426, loss: 14.869597
00:16:06.580610 Step[300/6995], lr: 0.000100, mv_avg_loss: 13.494419, loss: 13.127846
00:16:11.341079 Step[400/6995], lr: 0.000100, mv_avg_loss: 12.428967, loss: 24.464369
00:16:16.070721 Step[500/6995], lr: 0.000100, mv_avg_loss: 11.617250, loss: 10.881104
00:16:20.801191 Step[600/6995], lr: 0.000100, mv_avg_loss: 12.985851, loss: 11.640776
00:16:25.544097 Step[700/6995], lr: 0.000100, mv_avg_loss: 13.551969, loss: 12.451927
00:16:30.258714 Step[800/6995], lr: 0.000100, mv_avg_loss: 12.049711, loss: 13.115562
00:16:34.985548 Step[900/6995], lr: 0.000100, mv_avg_loss: 13.242481, loss: 13.032832
00:16:39.712802 Step[1000/6995], lr: 0.000100, mv_avg_loss: 12.448381, loss: 14.804291
00:16:44.444588 Step[1100/6995], lr: 0.000100, mv_avg_loss: 13.741930, loss: 12.110292
00:16:49.165842 Step[1200/6995], lr: 0.000100, mv_avg_loss: 12.702455, loss: 14.622046
00:16:53.896055 Step[1300/6995], lr: 0.000100, mv_avg_loss: 12.832507, loss: 15.249451
00:16:58.612364 Step[1400/6995], lr: 0.000100, mv_avg_loss: 12.713895, loss: 11.947908
00:17:03.345945 Step[1500/6995], lr: 0.000100, mv_avg_loss: 12.532920, loss: 14.638475
00:17:08.071653 Step[1600/6995], lr: 0.000100, mv_avg_loss: 13.275622, loss: 12.471991
00:17:12.796607 Step[1700/6995], lr: 0.000100, mv_avg_loss: 12.363095, loss: 8.654986
00:17:17.515361 Step[1800/6995], lr: 0.000100, mv_avg_loss: 13.199259, loss: 15.722633
00:17:22.237828 Step[1900/6995], lr: 0.000100, mv_avg_loss: 12.528058, loss: 12.291670
00:17:26.951203 Step[2000/6995], lr: 0.000100, mv_avg_loss: 11.649790, loss: 11.582980
00:17:31.669360 Step[2100/6995], lr: 0.000100, mv_avg_loss: 12.338951, loss: 23.257757
00:17:36.406495 Step[2200/6995], lr: 0.000100, mv_avg_loss: 13.036626, loss: 13.022484
00:17:41.140034 Step[2300/6995], lr: 0.000100, mv_avg_loss: 12.294304, loss: 13.982047
00:17:45.870373 Step[2400/6995], lr: 0.000100, mv_avg_loss: 12.981977, loss: 8.781581
00:17:50.598725 Step[2500/6995], lr: 0.000100, mv_avg_loss: 13.053347, loss: 14.144374
00:17:55.326939 Step[2600/6995], lr: 0.000100, mv_avg_loss: 13.281138, loss: 10.032877
00:18:00.070425 Step[2700/6995], lr: 0.000100, mv_avg_loss: 13.065309, loss: 9.882841
00:18:04.804336 Step[2800/6995], lr: 0.000100, mv_avg_loss: 13.951233, loss: 7.146743
00:18:09.526617 Step[2900/6995], lr: 0.000100, mv_avg_loss: 13.514592, loss: 21.081171
00:18:14.266203 Step[3000/6995], lr: 0.000100, mv_avg_loss: 12.458779, loss: 10.321581
00:18:19.004518 Step[3100/6995], lr: 0.000100, mv_avg_loss: 12.458797, loss: 17.396236
00:18:23.740968 Step[3200/6995], lr: 0.000100, mv_avg_loss: 13.838679, loss: 9.835663
00:18:28.490367 Step[3300/6995], lr: 0.000100, mv_avg_loss: 12.153100, loss: 19.968748
00:18:33.230014 Step[3400/6995], lr: 0.000100, mv_avg_loss: 12.643326, loss: 11.126616
00:18:37.961760 Step[3500/6995], lr: 0.000100, mv_avg_loss: 12.596444, loss: 11.894083
00:18:42.681070 Step[3600/6995], lr: 0.000100, mv_avg_loss: 14.299709, loss: 11.456810
00:18:47.405057 Step[3700/6995], lr: 0.000100, mv_avg_loss: 12.354271, loss: 11.785900
00:18:52.135113 Step[3800/6995], lr: 0.000100, mv_avg_loss: 12.739827, loss: 7.830791
00:18:56.862614 Step[3900/6995], lr: 0.000100, mv_avg_loss: 12.164250, loss: 7.938561
00:19:01.593929 Step[4000/6995], lr: 0.000100, mv_avg_loss: 13.239592, loss: 13.637098
00:19:06.327893 Step[4100/6995], lr: 0.000100, mv_avg_loss: 13.098927, loss: 14.762489
00:19:11.066208 Step[4200/6995], lr: 0.000100, mv_avg_loss: 13.138508, loss: 12.408371
00:19:15.805039 Step[4300/6995], lr: 0.000100, mv_avg_loss: 12.996396, loss: 9.438342
00:19:20.546362 Step[4400/6995], lr: 0.000100, mv_avg_loss: 12.145706, loss: 16.593586
00:19:25.284992 Step[4500/6995], lr: 0.000100, mv_avg_loss: 12.390629, loss: 11.577415
00:19:30.023150 Step[4600/6995], lr: 0.000100, mv_avg_loss: 13.113113, loss: 14.019168
00:19:34.743289 Step[4700/6995], lr: 0.000100, mv_avg_loss: 13.629418, loss: 14.286489
00:19:39.467713 Step[4800/6995], lr: 0.000100, mv_avg_loss: 13.077026, loss: 11.085161
00:19:44.199157 Step[4900/6995], lr: 0.000100, mv_avg_loss: 11.977406, loss: 10.244368
00:19:48.933173 Step[5000/6995], lr: 0.000100, mv_avg_loss: 12.295992, loss: 9.812564
00:19:53.663484 Step[5100/6995], lr: 0.000100, mv_avg_loss: 13.276258, loss: 9.338207
00:19:58.395796 Step[5200/6995], lr: 0.000100, mv_avg_loss: 12.178528, loss: 8.420815
00:20:03.124144 Step[5300/6995], lr: 0.000100, mv_avg_loss: 12.572699, loss: 24.722273
00:20:07.885767 Step[5400/6995], lr: 0.000100, mv_avg_loss: 12.347980, loss: 11.509140
00:20:12.608755 Step[5500/6995], lr: 0.000100, mv_avg_loss: 13.030217, loss: 13.484619
00:20:17.340730 Step[5600/6995], lr: 0.000100, mv_avg_loss: 12.767650, loss: 11.697739
00:20:22.059581 Step[5700/6995], lr: 0.000100, mv_avg_loss: 12.418152, loss: 13.126918
00:20:26.787814 Step[5800/6995], lr: 0.000100, mv_avg_loss: 13.709237, loss: 18.005548
00:20:31.523262 Step[5900/6995], lr: 0.000100, mv_avg_loss: 12.094968, loss: 9.212921
00:20:36.256297 Step[6000/6995], lr: 0.000100, mv_avg_loss: 13.927796, loss: 14.373817
00:20:40.995457 Step[6100/6995], lr: 0.000100, mv_avg_loss: 11.576589, loss: 11.569358
00:20:45.730625 Step[6200/6995], lr: 0.000100, mv_avg_loss: 13.543459, loss: 11.288788
00:20:50.461209 Step[6300/6995], lr: 0.000100, mv_avg_loss: 12.906928, loss: 18.220192
00:20:55.192306 Step[6400/6995], lr: 0.000100, mv_avg_loss: 13.011349, loss: 15.211535
00:20:59.926989 Step[6500/6995], lr: 0.000100, mv_avg_loss: 13.379730, loss: 15.946200
00:21:04.662477 Step[6600/6995], lr: 0.000100, mv_avg_loss: 12.766686, loss: 12.766479
00:21:09.394010 Step[6700/6995], lr: 0.000100, mv_avg_loss: 13.291560, loss: 10.162399
00:21:14.132575 Step[6800/6995], lr: 0.000100, mv_avg_loss: 12.879509, loss: 19.967415
00:21:18.867294 Step[6900/6995], lr: 0.000100, mv_avg_loss: 12.654511, loss: 8.178925
Testing for epoch: 75
Average test PNSR is 25.311637 for 500 images
Start to train epoch 76
00:21:45.613797 Step[0/6995], lr: 0.000100, mv_avg_loss: 12.950479, loss: 7.828317
00:21:50.346063 Step[100/6995], lr: 0.000100, mv_avg_loss: 12.355985, loss: 15.461879
00:21:55.074127 Step[200/6995], lr: 0.000100, mv_avg_loss: 12.123244, loss: 13.301546
00:21:59.786916 Step[300/6995], lr: 0.000100, mv_avg_loss: 12.079094, loss: 12.596079
00:22:04.502089 Step[400/6995], lr: 0.000100, mv_avg_loss: 13.438829, loss: 11.869823
00:22:09.230047 Step[500/6995], lr: 0.000100, mv_avg_loss: 12.878715, loss: 8.704713
00:22:13.971491 Step[600/6995], lr: 0.000100, mv_avg_loss: 13.172403, loss: 16.627195
00:22:18.707512 Step[700/6995], lr: 0.000100, mv_avg_loss: 13.010692, loss: 12.784676
00:22:23.451663 Step[800/6995], lr: 0.000100, mv_avg_loss: 13.233674, loss: 12.250656
00:22:28.193221 Step[900/6995], lr: 0.000100, mv_avg_loss: 12.577001, loss: 12.218497
00:22:32.925865 Step[1000/6995], lr: 0.000100, mv_avg_loss: 11.929657, loss: 12.622742
00:22:37.665803 Step[1100/6995], lr: 0.000100, mv_avg_loss: 13.104494, loss: 12.088240
00:22:42.397890 Step[1200/6995], lr: 0.000100, mv_avg_loss: 13.472839, loss: 10.085346
00:22:47.125202 Step[1300/6995], lr: 0.000100, mv_avg_loss: 12.923141, loss: 20.678877
00:22:51.844862 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.346012, loss: 11.057261
00:22:56.570723 Step[1500/6995], lr: 0.000100, mv_avg_loss: 12.784066, loss: 9.278238
00:23:01.305467 Step[1600/6995], lr: 0.000100, mv_avg_loss: 12.519862, loss: 8.509972
00:23:06.050873 Step[1700/6995], lr: 0.000100, mv_avg_loss: 12.489224, loss: 7.661456
00:23:10.783494 Step[1800/6995], lr: 0.000100, mv_avg_loss: 13.522897, loss: 19.950380
00:23:15.518593 Step[1900/6995], lr: 0.000100, mv_avg_loss: 12.569426, loss: 8.022175
00:23:20.257013 Step[2000/6995], lr: 0.000100, mv_avg_loss: 13.674537, loss: 9.276532
00:23:24.991724 Step[2100/6995], lr: 0.000100, mv_avg_loss: 13.078227, loss: 14.865103
00:23:29.736184 Step[2200/6995], lr: 0.000100, mv_avg_loss: 13.576611, loss: 9.995991
00:23:34.480158 Step[2300/6995], lr: 0.000100, mv_avg_loss: 13.405212, loss: 13.908419
00:23:39.210513 Step[2400/6995], lr: 0.000100, mv_avg_loss: 12.502756, loss: 9.684254
00:23:43.955931 Step[2500/6995], lr: 0.000100, mv_avg_loss: 13.853076, loss: 15.051216
00:23:48.693830 Step[2600/6995], lr: 0.000100, mv_avg_loss: 11.935344, loss: 23.424994
00:23:53.443989 Step[2700/6995], lr: 0.000100, mv_avg_loss: 13.038833, loss: 11.413058
00:23:58.197992 Step[2800/6995], lr: 0.000100, mv_avg_loss: 12.615596, loss: 11.186596
00:24:02.958580 Step[2900/6995], lr: 0.000100, mv_avg_loss: 13.109619, loss: 11.239856
00:24:07.686520 Step[3000/6995], lr: 0.000100, mv_avg_loss: 12.190602, loss: 20.607113
00:24:12.441225 Step[3100/6995], lr: 0.000100, mv_avg_loss: 12.336124, loss: 24.938026
00:24:17.178564 Step[3200/6995], lr: 0.000100, mv_avg_loss: 12.642072, loss: 18.507839
00:24:21.936472 Step[3300/6995], lr: 0.000100, mv_avg_loss: 13.031494, loss: 12.557155
00:24:26.717925 Step[3400/6995], lr: 0.000100, mv_avg_loss: 12.720507, loss: 10.940967
00:24:31.451858 Step[3500/6995], lr: 0.000100, mv_avg_loss: 12.957895, loss: 13.513949
00:24:36.189945 Step[3600/6995], lr: 0.000100, mv_avg_loss: 13.276975, loss: 12.356901
00:24:40.929652 Step[3700/6995], lr: 0.000100, mv_avg_loss: 12.180686, loss: 10.646670
00:24:45.670710 Step[3800/6995], lr: 0.000100, mv_avg_loss: 12.560689, loss: 9.948145
00:24:50.418059 Step[3900/6995], lr: 0.000100, mv_avg_loss: 12.470996, loss: 11.259712
00:24:55.144323 Step[4000/6995], lr: 0.000100, mv_avg_loss: 11.285797, loss: 6.968652
00:24:59.882138 Step[4100/6995], lr: 0.000100, mv_avg_loss: 13.944760, loss: 7.580578
00:25:04.620828 Step[4200/6995], lr: 0.000100, mv_avg_loss: 13.798332, loss: 13.639407
00:25:09.336254 Step[4300/6995], lr: 0.000100, mv_avg_loss: 12.375283, loss: 20.202019
00:25:14.086321 Step[4400/6995], lr: 0.000100, mv_avg_loss: 12.950475, loss: 18.170559
00:25:18.833517 Step[4500/6995], lr: 0.000100, mv_avg_loss: 12.574031, loss: 12.951317
00:25:23.584998 Step[4600/6995], lr: 0.000100, mv_avg_loss: 12.260169, loss: 8.023452
00:25:28.337405 Step[4700/6995], lr: 0.000100, mv_avg_loss: 13.347499, loss: 20.955873
00:25:33.090767 Step[4800/6995], lr: 0.000100, mv_avg_loss: 12.909617, loss: 8.082005
00:25:37.823824 Step[4900/6995], lr: 0.000100, mv_avg_loss: 12.827757, loss: 9.223145
00:25:42.565249 Step[5000/6995], lr: 0.000100, mv_avg_loss: 12.432196, loss: 13.583023
00:25:47.305001 Step[5100/6995], lr: 0.000100, mv_avg_loss: 12.936497, loss: 5.527217
00:25:52.061768 Step[5200/6995], lr: 0.000100, mv_avg_loss: 11.819569, loss: 10.489985
00:25:56.803567 Step[5300/6995], lr: 0.000100, mv_avg_loss: 13.493914, loss: 15.865814
00:26:01.545495 Step[5400/6995], lr: 0.000100, mv_avg_loss: 12.996641, loss: 8.817063
00:26:06.292441 Step[5500/6995], lr: 0.000100, mv_avg_loss: 13.624058, loss: 12.817001
00:26:11.046706 Step[5600/6995], lr: 0.000100, mv_avg_loss: 12.658223, loss: 10.400434
00:26:15.790426 Step[5700/6995], lr: 0.000100, mv_avg_loss: 12.832150, loss: 12.271610
00:26:20.529758 Step[5800/6995], lr: 0.000100, mv_avg_loss: 12.873070, loss: 28.801846
00:26:25.271396 Step[5900/6995], lr: 0.000100, mv_avg_loss: 13.202424, loss: 10.223498
00:26:30.016592 Step[6000/6995], lr: 0.000100, mv_avg_loss: 11.755033, loss: 17.397465
00:26:34.761852 Step[6100/6995], lr: 0.000100, mv_avg_loss: 13.509369, loss: 23.381836
00:26:39.506624 Step[6200/6995], lr: 0.000100, mv_avg_loss: 12.834787, loss: 14.491881
00:26:44.242642 Step[6300/6995], lr: 0.000100, mv_avg_loss: 12.739739, loss: 8.681603
00:26:48.974977 Step[6400/6995], lr: 0.000100, mv_avg_loss: 12.973341, loss: 17.928280
00:26:53.714819 Step[6500/6995], lr: 0.000100, mv_avg_loss: 12.899268, loss: 20.521460
00:26:58.457298 Step[6600/6995], lr: 0.000100, mv_avg_loss: 13.014798, loss: 23.175648
00:27:03.199185 Step[6700/6995], lr: 0.000100, mv_avg_loss: 13.208529, loss: 13.041235
00:27:07.946622 Step[6800/6995], lr: 0.000100, mv_avg_loss: 12.732706, loss: 13.075397
00:27:12.689243 Step[6900/6995], lr: 0.000100, mv_avg_loss: 13.232644, loss: 11.846235
Testing for epoch: 76
Average test PNSR is 25.330392 for 500 images
Start to train epoch 77
00:27:39.200391 Step[0/6995], lr: 0.000100, mv_avg_loss: 12.653563, loss: 10.271237
00:27:43.929527 Step[100/6995], lr: 0.000100, mv_avg_loss: 12.763131, loss: 8.504549
00:27:48.665864 Step[200/6995], lr: 0.000100, mv_avg_loss: 12.978812, loss: 12.308167
00:27:53.392939 Step[300/6995], lr: 0.000100, mv_avg_loss: 11.815487, loss: 8.458280
00:27:58.116578 Step[400/6995], lr: 0.000100, mv_avg_loss: 13.491481, loss: 11.536213
00:28:02.863984 Step[500/6995], lr: 0.000100, mv_avg_loss: 13.087352, loss: 7.929153
00:28:07.576293 Step[600/6995], lr: 0.000100, mv_avg_loss: 12.307323, loss: 11.657980
00:28:12.313738 Step[700/6995], lr: 0.000100, mv_avg_loss: 13.480235, loss: 13.585177
00:28:17.056966 Step[800/6995], lr: 0.000100, mv_avg_loss: 12.944510, loss: 16.767048
00:28:21.782413 Step[900/6995], lr: 0.000100, mv_avg_loss: 13.618807, loss: 12.458696
00:28:26.534160 Step[1000/6995], lr: 0.000100, mv_avg_loss: 12.190120, loss: 18.003979
00:28:31.266886 Step[1100/6995], lr: 0.000100, mv_avg_loss: 12.689060, loss: 16.563091
00:28:35.992414 Step[1200/6995], lr: 0.000100, mv_avg_loss: 13.824363, loss: 13.929774
00:28:40.735431 Step[1300/6995], lr: 0.000100, mv_avg_loss: 12.995520, loss: 11.016002
00:28:45.494403 Step[1400/6995], lr: 0.000100, mv_avg_loss: 12.571309, loss: 12.561478
00:28:50.225684 Step[1500/6995], lr: 0.000100, mv_avg_loss: 12.891783, loss: 14.179382
00:28:54.955196 Step[1600/6995], lr: 0.000100, mv_avg_loss: 12.905604, loss: 21.057592
00:28:59.680256 Step[1700/6995], lr: 0.000100, mv_avg_loss: 11.613194, loss: 12.719255
00:29:04.405605 Step[1800/6995], lr: 0.000100, mv_avg_loss: 11.826758, loss: 11.040760
00:29:09.132532 Step[1900/6995], lr: 0.000100, mv_avg_loss: 12.896006, loss: 7.999902
00:29:13.948071 Step[2000/6995], lr: 0.000100, mv_avg_loss: 13.410771, loss: 6.977222
00:29:18.682064 Step[2100/6995], lr: 0.000100, mv_avg_loss: 12.677058, loss: 15.452137
00:29:23.420408 Step[2200/6995], lr: 0.000100, mv_avg_loss: 12.614294, loss: 16.195934
00:29:28.167045 Step[2300/6995], lr: 0.000100, mv_avg_loss: 12.479457, loss: 15.927040
00:29:32.902613 Step[2400/6995], lr: 0.000100, mv_avg_loss: 14.264017, loss: 10.302169
00:29:37.617669 Step[2500/6995], lr: 0.000100, mv_avg_loss: 12.500595, loss: 7.809574
00:29:42.334192 Step[2600/6995], lr: 0.000100, mv_avg_loss: 13.131065, loss: 16.236416
00:29:47.058284 Step[2700/6995], lr: 0.000100, mv_avg_loss: 12.576735, loss: 14.026894
00:29:51.785001 Step[2800/6995], lr: 0.000100, mv_avg_loss: 12.444983, loss: 15.139938
00:29:56.518345 Step[2900/6995], lr: 0.000100, mv_avg_loss: 13.093226, loss: 19.836205
00:30:01.250426 Step[3000/6995], lr: 0.000100, mv_avg_loss: 12.753513, loss: 12.605543
00:30:05.990778 Step[3100/6995], lr: 0.000100, mv_avg_loss: 12.141222, loss: 8.370727
00:30:10.715517 Step[3200/6995], lr: 0.000100, mv_avg_loss: 13.523414, loss: 19.006798
00:30:15.456337 Step[3300/6995], lr: 0.000100, mv_avg_loss: 11.989146, loss: 13.141016
00:30:20.202490 Step[3400/6995], lr: 0.000100, mv_avg_loss: 13.330720, loss: 13.385838
00:30:24.936693 Step[3500/6995], lr: 0.000100, mv_avg_loss: 12.658207, loss: 15.121379
00:30:29.669394 Step[3600/6995], lr: 0.000100, mv_avg_loss: 11.864800, loss: 7.382898
00:30:34.381367 Step[3700/6995], lr: 0.000100, mv_avg_loss: 12.249077, loss: 11.968120
00:30:39.113747 Step[3800/6995], lr: 0.000100, mv_avg_loss: 12.595725, loss: 12.122316
00:30:43.825179 Step[3900/6995], lr: 0.000100, mv_avg_loss: 12.836260, loss: 10.827853
00:30:48.549797 Step[4000/6995], lr: 0.000100, mv_avg_loss: 12.072791, loss: 12.530859
00:30:53.277291 Step[4100/6995], lr: 0.000100, mv_avg_loss: 12.377706, loss: 9.380156
00:30:58.001478 Step[4200/6995], lr: 0.000100, mv_avg_loss: 12.557863, loss: 9.780680
00:31:02.738248 Step[4300/6995], lr: 0.000100, mv_avg_loss: 12.822174, loss: 10.581646
00:31:07.475457 Step[4400/6995], lr: 0.000100, mv_avg_loss: 13.387551, loss: 9.791792
00:31:12.217817 Step[4500/6995], lr: 0.000100, mv_avg_loss: 13.328386, loss: 43.014343
00:31:16.941556 Step[4600/6995], lr: 0.000100, mv_avg_loss: 13.217740, loss: 12.433195
00:31:21.664680 Step[4700/6995], lr: 0.000100, mv_avg_loss: 12.287186, loss: 15.621498
00:31:26.408729 Step[4800/6995], lr: 0.000100, mv_avg_loss: 12.156364, loss: 17.216118
00:31:31.135532 Step[4900/6995], lr: 0.000100, mv_avg_loss: 12.562164, loss: 19.309921
00:31:35.858743 Step[5000/6995], lr: 0.000100, mv_avg_loss: 12.141563, loss: 13.029075
00:31:40.581568 Step[5100/6995], lr: 0.000100, mv_avg_loss: 13.138692, loss: 24.018599
00:31:45.314785 Step[5200/6995], lr: 0.000100, mv_avg_loss: 13.423737, loss: 10.239184
00:31:50.055088 Step[5300/6995], lr: 0.000100, mv_avg_loss: 12.395765, loss: 8.206185
00:31:54.782716 Step[5400/6995], lr: 0.000100, mv_avg_loss: 12.920547, loss: 13.268028
00:31:59.505399 Step[5500/6995], lr: 0.000100, mv_avg_loss: 12.274005, loss: 11.140354
00:32:04.250224 Step[5600/6995], lr: 0.000100, mv_avg_loss: 12.313985, loss: 22.840942
00:32:08.998963 Step[5700/6995], lr: 0.000100, mv_avg_loss: 12.978882, loss: 17.486361
00:32:13.729921 Step[5800/6995], lr: 0.000100, mv_avg_loss: 12.616581, loss: 8.769844
00:32:18.473269 Step[5900/6995], lr: 0.000100, mv_avg_loss: 13.191123, loss: 13.632391
00:32:23.216854 Step[6000/6995], lr: 0.000100, mv_avg_loss: 12.672634, loss: 23.644812
00:32:27.949189 Step[6100/6995], lr: 0.000100, mv_avg_loss: 13.105855, loss: 10.743385
00:32:32.673541 Step[6200/6995], lr: 0.000100, mv_avg_loss: 13.250232, loss: 8.458302
00:32:37.403061 Step[6300/6995], lr: 0.000100, mv_avg_loss: 11.884238, loss: 12.110763
00:32:42.144572 Step[6400/6995], lr: 0.000100, mv_avg_loss: 13.354258, loss: 15.401419
00:32:46.862341 Step[6500/6995], lr: 0.000100, mv_avg_loss: 12.563518, loss: 15.534377
00:32:51.583864 Step[6600/6995], lr: 0.000100, mv_avg_loss: 13.183321, loss: 14.637942
00:32:56.306902 Step[6700/6995], lr: 0.000100, mv_avg_loss: 13.158993, loss: 10.828852
00:33:01.048685 Step[6800/6995], lr: 0.000100, mv_avg_loss: 13.339909, loss: 14.677839
00:33:05.752496 Step[6900/6995], lr: 0.000100, mv_avg_loss: 12.847499, loss: 14.421705
Testing for epoch: 77
Average test PNSR is 25.314200 for 500 images
Start to train epoch 78
00:33:31.556636 Step[0/6995], lr: 0.000100, mv_avg_loss: 13.022282, loss: 11.297919
00:33:36.268866 Step[100/6995], lr: 0.000100, mv_avg_loss: 13.377068, loss: 11.256124
00:33:40.984720 Step[200/6995], lr: 0.000100, mv_avg_loss: 12.963764, loss: 9.508492
00:33:45.712872 Step[300/6995], lr: 0.000100, mv_avg_loss: 12.095910, loss: 7.576063
00:33:50.433628 Step[400/6995], lr: 0.000100, mv_avg_loss: 11.872725, loss: 8.851863
00:33:55.147642 Step[500/6995], lr: 0.000100, mv_avg_loss: 12.035485, loss: 15.232521
00:33:59.883788 Step[600/6995], lr: 0.000100, mv_avg_loss: 12.412231, loss: 12.357067
00:34:04.612619 Step[700/6995], lr: 0.000100, mv_avg_loss: 12.431109, loss: 17.036970
00:34:09.337716 Step[800/6995], lr: 0.000100, mv_avg_loss: 13.318465, loss: 10.620895
00:34:14.067082 Step[900/6995], lr: 0.000100, mv_avg_loss: 14.272491, loss: 12.896925
00:34:18.805312 Step[1000/6995], lr: 0.000100, mv_avg_loss: 11.874731, loss: 11.714399
00:34:23.539108 Step[1100/6995], lr: 0.000100, mv_avg_loss: 12.038296, loss: 16.180656
00:34:28.252558 Step[1200/6995], lr: 0.000100, mv_avg_loss: 12.469745, loss: 11.347040
00:34:32.983947 Step[1300/6995], lr: 0.000100, mv_avg_loss: 12.569507, loss: 18.301243
00:34:37.720606 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.293508, loss: 12.148005
00:34:42.426729 Step[1500/6995], lr: 0.000100, mv_avg_loss: 13.580441, loss: 16.221630
00:34:47.138617 Step[1600/6995], lr: 0.000100, mv_avg_loss: 12.742421, loss: 14.670115
00:34:51.852820 Step[1700/6995], lr: 0.000100, mv_avg_loss: 12.425532, loss: 18.817633
00:34:56.575579 Step[1800/6995], lr: 0.000100, mv_avg_loss: 13.896049, loss: 10.257187
00:35:01.306259 Step[1900/6995], lr: 0.000100, mv_avg_loss: 11.988975, loss: 15.237083
00:35:06.034055 Step[2000/6995], lr: 0.000100, mv_avg_loss: 13.443645, loss: 9.466047
00:35:10.779428 Step[2100/6995], lr: 0.000100, mv_avg_loss: 12.081492, loss: 14.608194
00:35:15.523780 Step[2200/6995], lr: 0.000100, mv_avg_loss: 12.878720, loss: 10.219780
00:35:20.251709 Step[2300/6995], lr: 0.000100, mv_avg_loss: 12.805544, loss: 14.363937
00:35:24.979065 Step[2400/6995], lr: 0.000100, mv_avg_loss: 12.475539, loss: 13.866648
00:35:29.729027 Step[2500/6995], lr: 0.000100, mv_avg_loss: 13.223459, loss: 11.115409
00:35:34.462339 Step[2600/6995], lr: 0.000100, mv_avg_loss: 12.405498, loss: 11.426931
00:35:39.187681 Step[2700/6995], lr: 0.000100, mv_avg_loss: 12.740930, loss: 15.441750
00:35:43.930852 Step[2800/6995], lr: 0.000100, mv_avg_loss: 13.717588, loss: 9.616001
00:35:48.681242 Step[2900/6995], lr: 0.000100, mv_avg_loss: 13.046406, loss: 9.245152
00:35:53.399929 Step[3000/6995], lr: 0.000100, mv_avg_loss: 12.006781, loss: 8.925940
00:35:58.135194 Step[3100/6995], lr: 0.000100, mv_avg_loss: 12.732706, loss: 42.250732
00:36:02.855743 Step[3200/6995], lr: 0.000100, mv_avg_loss: 12.516622, loss: 11.944080
00:36:07.583275 Step[3300/6995], lr: 0.000100, mv_avg_loss: 13.096431, loss: 18.953869
00:36:12.313875 Step[3400/6995], lr: 0.000100, mv_avg_loss: 11.780618, loss: 10.966110
00:36:17.047955 Step[3500/6995], lr: 0.000100, mv_avg_loss: 12.588547, loss: 9.177780
00:36:21.784184 Step[3600/6995], lr: 0.000100, mv_avg_loss: 12.418955, loss: 11.264823
00:36:26.517544 Step[3700/6995], lr: 0.000100, mv_avg_loss: 12.701094, loss: 11.255416
00:36:31.263442 Step[3800/6995], lr: 0.000100, mv_avg_loss: 12.567594, loss: 13.934215
00:36:35.999015 Step[3900/6995], lr: 0.000100, mv_avg_loss: 12.323723, loss: 11.029438
00:36:40.734251 Step[4000/6995], lr: 0.000100, mv_avg_loss: 13.028196, loss: 8.773275
00:36:45.463976 Step[4100/6995], lr: 0.000100, mv_avg_loss: 13.207342, loss: 14.129687
00:36:50.203153 Step[4200/6995], lr: 0.000100, mv_avg_loss: 11.566681, loss: 8.804065
00:36:54.956285 Step[4300/6995], lr: 0.000100, mv_avg_loss: 12.043559, loss: 10.078640
00:36:59.714632 Step[4400/6995], lr: 0.000100, mv_avg_loss: 12.994093, loss: 12.192138
00:37:04.451999 Step[4500/6995], lr: 0.000100, mv_avg_loss: 13.550242, loss: 8.957326
00:37:09.190012 Step[4600/6995], lr: 0.000100, mv_avg_loss: 12.724618, loss: 18.491011
00:37:13.928405 Step[4700/6995], lr: 0.000100, mv_avg_loss: 11.777427, loss: 10.962294
00:37:18.678043 Step[4800/6995], lr: 0.000100, mv_avg_loss: 12.680470, loss: 5.561035
00:37:23.422482 Step[4900/6995], lr: 0.000100, mv_avg_loss: 12.848668, loss: 8.805152
00:37:28.166949 Step[5000/6995], lr: 0.000100, mv_avg_loss: 12.989221, loss: 17.063303
00:37:32.907051 Step[5100/6995], lr: 0.000100, mv_avg_loss: 13.208537, loss: 13.558390
00:37:37.666685 Step[5200/6995], lr: 0.000100, mv_avg_loss: 13.145514, loss: 15.947360
00:37:42.414160 Step[5300/6995], lr: 0.000100, mv_avg_loss: 13.455700, loss: 29.173073
00:37:47.162320 Step[5400/6995], lr: 0.000100, mv_avg_loss: 13.285610, loss: 19.629505
00:37:51.914649 Step[5500/6995], lr: 0.000100, mv_avg_loss: 12.185135, loss: 9.936174
00:37:56.653819 Step[5600/6995], lr: 0.000100, mv_avg_loss: 12.349689, loss: 9.995214
00:38:01.394270 Step[5700/6995], lr: 0.000100, mv_avg_loss: 13.121985, loss: 19.416847
00:38:06.131027 Step[5800/6995], lr: 0.000100, mv_avg_loss: 13.135957, loss: 15.444327
00:38:10.864958 Step[5900/6995], lr: 0.000100, mv_avg_loss: 13.451400, loss: 9.707546
00:38:15.592215 Step[6000/6995], lr: 0.000100, mv_avg_loss: 12.452176, loss: 7.055388
00:38:20.336214 Step[6100/6995], lr: 0.000100, mv_avg_loss: 12.610688, loss: 11.836712
00:38:25.069116 Step[6200/6995], lr: 0.000100, mv_avg_loss: 12.443546, loss: 8.423533
00:38:29.799157 Step[6300/6995], lr: 0.000100, mv_avg_loss: 12.994946, loss: 13.755260
00:38:34.541704 Step[6400/6995], lr: 0.000100, mv_avg_loss: 12.880469, loss: 11.708537
00:38:39.271222 Step[6500/6995], lr: 0.000100, mv_avg_loss: 13.143966, loss: 10.366229
00:38:44.020849 Step[6600/6995], lr: 0.000100, mv_avg_loss: 13.349908, loss: 9.618199
00:38:48.772795 Step[6700/6995], lr: 0.000100, mv_avg_loss: 13.200218, loss: 18.483761
00:38:53.524320 Step[6800/6995], lr: 0.000100, mv_avg_loss: 11.957587, loss: 8.747844
00:38:58.294096 Step[6900/6995], lr: 0.000100, mv_avg_loss: 13.694901, loss: 25.713516
Testing for epoch: 78
Average test PNSR is 25.330200 for 500 images
Start to train epoch 79
00:39:24.747134 Step[0/6995], lr: 0.000100, mv_avg_loss: 12.690583, loss: 7.378381
00:39:29.504735 Step[100/6995], lr: 0.000100, mv_avg_loss: 12.266960, loss: 9.792491
00:39:34.242175 Step[200/6995], lr: 0.000100, mv_avg_loss: 13.335263, loss: 9.293972
00:39:38.991335 Step[300/6995], lr: 0.000100, mv_avg_loss: 12.707870, loss: 10.227341
00:39:43.722705 Step[400/6995], lr: 0.000100, mv_avg_loss: 12.935268, loss: 20.111458
00:39:48.473530 Step[500/6995], lr: 0.000100, mv_avg_loss: 13.282181, loss: 9.175108
00:39:53.226736 Step[600/6995], lr: 0.000100, mv_avg_loss: 12.897492, loss: 14.601677
00:39:57.975132 Step[700/6995], lr: 0.000100, mv_avg_loss: 12.994136, loss: 10.908714
00:40:02.724316 Step[800/6995], lr: 0.000100, mv_avg_loss: 12.815383, loss: 8.916789
00:40:07.477346 Step[900/6995], lr: 0.000100, mv_avg_loss: 12.790303, loss: 11.596281
00:40:12.236991 Step[1000/6995], lr: 0.000100, mv_avg_loss: 12.331387, loss: 8.177668
00:40:16.984042 Step[1100/6995], lr: 0.000100, mv_avg_loss: 12.798021, loss: 13.282784
00:40:21.740913 Step[1200/6995], lr: 0.000100, mv_avg_loss: 11.813452, loss: 10.886745
00:40:26.488777 Step[1300/6995], lr: 0.000100, mv_avg_loss: 12.653945, loss: 12.390474
00:40:31.230536 Step[1400/6995], lr: 0.000100, mv_avg_loss: 13.624583, loss: 22.582851
00:40:35.994216 Step[1500/6995], lr: 0.000100, mv_avg_loss: 11.689338, loss: 15.878138
00:40:40.751863 Step[1600/6995], lr: 0.000100, mv_avg_loss: 12.034389, loss: 8.383322
00:40:45.503294 Step[1700/6995], lr: 0.000100, mv_avg_loss: 13.005210, loss: 12.119003
00:40:50.250125 Step[1800/6995], lr: 0.000100, mv_avg_loss: 12.382627, loss: 8.436026
00:40:54.962538 Step[1900/6995], lr: 0.000100, mv_avg_loss: 12.645323, loss: 22.645573
00:40:59.703507 Step[2000/6995], lr: 0.000100, mv_avg_loss: 12.692715, loss: 6.880104
00:41:04.457567 Step[2100/6995], lr: 0.000100, mv_avg_loss: 12.174542, loss: 7.719947
00:41:09.198591 Step[2200/6995], lr: 0.000100, mv_avg_loss: 11.851296, loss: 14.839416
00:41:13.948577 Step[2300/6995], lr: 0.000100, mv_avg_loss: 12.578436, loss: 8.228603
00:41:18.726413 Step[2400/6995], lr: 0.000100, mv_avg_loss: 13.240348, loss: 10.188972
00:41:23.472469 Step[2500/6995], lr: 0.000100, mv_avg_loss: 11.923593, loss: 14.953796
00:41:28.216442 Step[2600/6995], lr: 0.000100, mv_avg_loss: 13.067086, loss: 11.788160
00:41:32.952910 Step[2700/6995], lr: 0.000100, mv_avg_loss: 12.836202, loss: 13.703781
00:41:37.679908 Step[2800/6995], lr: 0.000100, mv_avg_loss: 13.338954, loss: 13.665774
00:41:42.426013 Step[2900/6995], lr: 0.000100, mv_avg_loss: 12.270631, loss: 13.237125
00:41:47.159410 Step[3000/6995], lr: 0.000100, mv_avg_loss: 12.761636, loss: 13.177877
00:41:51.903563 Step[3100/6995], lr: 0.000100, mv_avg_loss: 13.535976, loss: 16.167625
00:41:56.656601 Step[3200/6995], lr: 0.000100, mv_avg_loss: 12.227620, loss: 13.624423
00:42:01.393735 Step[3300/6995], lr: 0.000100, mv_avg_loss: 12.820951, loss: 9.688948
00:42:06.136603 Step[3400/6995], lr: 0.000100, mv_avg_loss: 11.964177, loss: 11.875149
00:42:10.870580 Step[3500/6995], lr: 0.000100, mv_avg_loss: 12.317270, loss: 15.987822
00:42:15.627041 Step[3600/6995], lr: 0.000100, mv_avg_loss: 13.946569, loss: 19.565418
00:42:20.362069 Step[3700/6995], lr: 0.000100, mv_avg_loss: 12.958160, loss: 19.242949
00:42:25.103558 Step[3800/6995], lr: 0.000100, mv_avg_loss: 13.475038, loss: 6.017108
00:42:29.835505 Step[3900/6995], lr: 0.000100, mv_avg_loss: 12.682373, loss: 15.346117
00:42:34.576196 Step[4000/6995], lr: 0.000100, mv_avg_loss: 12.057843, loss: 9.361513
00:42:39.318325 Step[4100/6995], lr: 0.000100, mv_avg_loss: 12.922283, loss: 13.100792
00:42:44.079679 Step[4200/6995], lr: 0.000100, mv_avg_loss: 13.032318, loss: 15.760231
00:42:48.815025 Step[4300/6995], lr: 0.000100, mv_avg_loss: 12.649756, loss: 11.198256
00:42:53.558130 Step[4400/6995], lr: 0.000100, mv_avg_loss: 12.239831, loss: 7.535753
00:42:58.301601 Step[4500/6995], lr: 0.000100, mv_avg_loss: 12.448885, loss: 6.660542
00:43:03.039844 Step[4600/6995], lr: 0.000100, mv_avg_loss: 12.923637, loss: 17.054806
00:43:07.782383 Step[4700/6995], lr: 0.000100, mv_avg_loss: 12.690732, loss: 15.623196
00:43:12.517414 Step[4800/6995], lr: 0.000100, mv_avg_loss: 12.606154, loss: 14.493747
00:43:17.257281 Step[4900/6995], lr: 0.000100, mv_avg_loss: 12.417616, loss: 10.687144
00:43:21.985074 Step[5000/6995], lr: 0.000100, mv_avg_loss: 12.591018, loss: 12.721149
00:43:26.732500 Step[5100/6995], lr: 0.000100, mv_avg_loss: 13.133648, loss: 7.267859
00:43:31.469646 Step[5200/6995], lr: 0.000100, mv_avg_loss: 13.586166, loss: 14.801677
00:43:36.198925 Step[5300/6995], lr: 0.000100, mv_avg_loss: 12.548512, loss: 8.676777
00:43:40.927936 Step[5400/6995], lr: 0.000100, mv_avg_loss: 14.310026, loss: 12.984224
00:43:45.669194 Step[5500/6995], lr: 0.000100, mv_avg_loss: 12.679367, loss: 20.005066
00:43:50.387536 Step[5600/6995], lr: 0.000100, mv_avg_loss: 12.452774, loss: 8.876605
00:43:55.103930 Step[5700/6995], lr: 0.000100, mv_avg_loss: 12.854263, loss: 8.059504
00:43:59.837544 Step[5800/6995], lr: 0.000100, mv_avg_loss: 13.666519, loss: 23.868164
00:44:04.569603 Step[5900/6995], lr: 0.000100, mv_avg_loss: 13.005605, loss: 17.107504
00:44:09.297169 Step[6000/6995], lr: 0.000100, mv_avg_loss: 13.083642, loss: 9.509356
00:44:14.024395 Step[6100/6995], lr: 0.000100, mv_avg_loss: 13.268482, loss: 10.245690
00:44:18.744425 Step[6200/6995], lr: 0.000100, mv_avg_loss: 12.386577, loss: 10.547625
00:44:23.461996 Step[6300/6995], lr: 0.000100, mv_avg_loss: 13.573702, loss: 8.198385
00:44:28.201023 Step[6400/6995], lr: 0.000100, mv_avg_loss: 12.054979, loss: 10.292665
00:44:32.925298 Step[6500/6995], lr: 0.000100, mv_avg_loss: 12.208030, loss: 8.589897
00:44:37.644188 Step[6600/6995], lr: 0.000100, mv_avg_loss: 11.958725, loss: 19.504585
00:44:42.368807 Step[6700/6995], lr: 0.000100, mv_avg_loss: 12.382952, loss: 33.250900
00:44:47.083225 Step[6800/6995], lr: 0.000100, mv_avg_loss: 12.327767, loss: 11.149291
00:44:51.819371 Step[6900/6995], lr: 0.000100, mv_avg_loss: 13.260635, loss: 8.977036
saving model for epoch 79
Testing for epoch: 79
Average test PNSR is 25.341748 for 500 images
Start to train epoch 80
00:45:18.109672 Step[0/6995], lr: 0.000001, mv_avg_loss: 12.235945, loss: 15.621168
00:45:22.827851 Step[100/6995], lr: 0.000001, mv_avg_loss: 13.535343, loss: 18.207376
00:45:27.540017 Step[200/6995], lr: 0.000001, mv_avg_loss: 12.817567, loss: 16.003319
00:45:32.236980 Step[300/6995], lr: 0.000001, mv_avg_loss: 12.298210, loss: 9.719448
00:45:36.957201 Step[400/6995], lr: 0.000001, mv_avg_loss: 11.106478, loss: 13.376049
00:45:41.665351 Step[500/6995], lr: 0.000001, mv_avg_loss: 12.994020, loss: 23.791893
00:45:46.384429 Step[600/6995], lr: 0.000001, mv_avg_loss: 12.027500, loss: 8.921434
00:45:51.088660 Step[700/6995], lr: 0.000001, mv_avg_loss: 12.969646, loss: 9.412378
00:45:55.803164 Step[800/6995], lr: 0.000001, mv_avg_loss: 12.833749, loss: 13.628883
00:46:00.512067 Step[900/6995], lr: 0.000001, mv_avg_loss: 11.851481, loss: 12.226381
00:46:05.228903 Step[1000/6995], lr: 0.000001, mv_avg_loss: 11.639462, loss: 10.269471
00:46:09.939895 Step[1100/6995], lr: 0.000001, mv_avg_loss: 12.213619, loss: 7.326454
00:46:14.646447 Step[1200/6995], lr: 0.000001, mv_avg_loss: 12.506085, loss: 14.689803
00:46:19.352625 Step[1300/6995], lr: 0.000001, mv_avg_loss: 12.365892, loss: 19.936598
00:46:24.050204 Step[1400/6995], lr: 0.000001, mv_avg_loss: 12.293179, loss: 6.960084
00:46:28.757263 Step[1500/6995], lr: 0.000001, mv_avg_loss: 12.267208, loss: 11.311640
00:46:33.464188 Step[1600/6995], lr: 0.000001, mv_avg_loss: 12.649299, loss: 26.993359
00:46:38.131754 Step[1700/6995], lr: 0.000001, mv_avg_loss: 12.090425, loss: 9.512166
00:46:42.839277 Step[1800/6995], lr: 0.000001, mv_avg_loss: 12.856404, loss: 6.689947
00:46:47.541380 Step[1900/6995], lr: 0.000001, mv_avg_loss: 11.360426, loss: 9.132060
00:46:52.257702 Step[2000/6995], lr: 0.000001, mv_avg_loss: 13.390235, loss: 10.504386
00:46:56.965741 Step[2100/6995], lr: 0.000001, mv_avg_loss: 12.776271, loss: 11.901196
00:47:01.668593 Step[2200/6995], lr: 0.000001, mv_avg_loss: 13.125505, loss: 22.328140
00:47:06.387143 Step[2300/6995], lr: 0.000001, mv_avg_loss: 12.407104, loss: 11.397716
00:47:11.106104 Step[2400/6995], lr: 0.000001, mv_avg_loss: 12.966686, loss: 17.182426
00:47:15.808929 Step[2500/6995], lr: 0.000001, mv_avg_loss: 11.990125, loss: 9.446662
00:47:20.520463 Step[2600/6995], lr: 0.000001, mv_avg_loss: 13.552806, loss: 15.465570
00:47:25.227193 Step[2700/6995], lr: 0.000001, mv_avg_loss: 13.072636, loss: 12.645372
00:47:29.941873 Step[2800/6995], lr: 0.000001, mv_avg_loss: 12.618115, loss: 7.633001
00:47:34.651531 Step[2900/6995], lr: 0.000001, mv_avg_loss: 12.958723, loss: 19.649595
00:47:39.366930 Step[3000/6995], lr: 0.000001, mv_avg_loss: 11.424518, loss: 6.234906
00:47:44.061934 Step[3100/6995], lr: 0.000001, mv_avg_loss: 12.504056, loss: 10.409410
00:47:48.758913 Step[3200/6995], lr: 0.000001, mv_avg_loss: 12.513385, loss: 18.249689
00:47:53.467400 Step[3300/6995], lr: 0.000001, mv_avg_loss: 12.589344, loss: 8.121836
00:47:58.173867 Step[3400/6995], lr: 0.000001, mv_avg_loss: 12.912560, loss: 12.675654
00:48:02.893094 Step[3500/6995], lr: 0.000001, mv_avg_loss: 12.065523, loss: 19.139936
00:48:07.605843 Step[3600/6995], lr: 0.000001, mv_avg_loss: 12.090178, loss: 7.839081
00:48:12.319092 Step[3700/6995], lr: 0.000001, mv_avg_loss: 11.878276, loss: 12.171737
00:48:17.045177 Step[3800/6995], lr: 0.000001, mv_avg_loss: 13.490463, loss: 10.146896
00:48:21.768955 Step[3900/6995], lr: 0.000001, mv_avg_loss: 12.724380, loss: 9.843563
00:48:26.491587 Step[4000/6995], lr: 0.000001, mv_avg_loss: 12.466459, loss: 15.369629
00:48:31.221529 Step[4100/6995], lr: 0.000001, mv_avg_loss: 12.435479, loss: 8.034788
00:48:35.955536 Step[4200/6995], lr: 0.000001, mv_avg_loss: 12.301173, loss: 13.015387
00:48:40.698595 Step[4300/6995], lr: 0.000001, mv_avg_loss: 12.710890, loss: 13.211195
00:48:45.430348 Step[4400/6995], lr: 0.000001, mv_avg_loss: 12.753868, loss: 15.213800
00:48:50.153761 Step[4500/6995], lr: 0.000001, mv_avg_loss: 11.680403, loss: 10.148102
00:48:54.878623 Step[4600/6995], lr: 0.000001, mv_avg_loss: 12.068539, loss: 15.712096
00:48:59.594957 Step[4700/6995], lr: 0.000001, mv_avg_loss: 12.367905, loss: 15.365295
00:49:04.332114 Step[4800/6995], lr: 0.000001, mv_avg_loss: 11.736378, loss: 10.220328
00:49:09.048809 Step[4900/6995], lr: 0.000001, mv_avg_loss: 12.877995, loss: 13.651202
00:49:13.784915 Step[5000/6995], lr: 0.000001, mv_avg_loss: 11.809897, loss: 6.937279
00:49:18.511659 Step[5100/6995], lr: 0.000001, mv_avg_loss: 11.945283, loss: 6.293399
00:49:23.205657 Step[5200/6995], lr: 0.000001, mv_avg_loss: 12.891401, loss: 18.531687
00:49:27.925301 Step[5300/6995], lr: 0.000001, mv_avg_loss: 12.385663, loss: 10.991282
00:49:32.671812 Step[5400/6995], lr: 0.000001, mv_avg_loss: 11.673599, loss: 21.648560
00:49:37.393519 Step[5500/6995], lr: 0.000001, mv_avg_loss: 11.672002, loss: 12.794163
00:49:42.117843 Step[5600/6995], lr: 0.000001, mv_avg_loss: 11.751740, loss: 9.676445
00:49:46.845235 Step[5700/6995], lr: 0.000001, mv_avg_loss: 12.479621, loss: 11.047122
00:49:51.550083 Step[5800/6995], lr: 0.000001, mv_avg_loss: 12.813765, loss: 18.731466
00:49:56.282921 Step[5900/6995], lr: 0.000001, mv_avg_loss: 12.640234, loss: 20.815613
00:50:01.013940 Step[6000/6995], lr: 0.000001, mv_avg_loss: 12.449425, loss: 8.408310
00:50:05.754171 Step[6100/6995], lr: 0.000001, mv_avg_loss: 12.403804, loss: 11.903644
00:50:10.485751 Step[6200/6995], lr: 0.000001, mv_avg_loss: 12.352314, loss: 12.881472
00:50:15.229363 Step[6300/6995], lr: 0.000001, mv_avg_loss: 13.048231, loss: 14.537318
00:50:19.954776 Step[6400/6995], lr: 0.000001, mv_avg_loss: 11.532494, loss: 13.350296
00:50:24.679734 Step[6500/6995], lr: 0.000001, mv_avg_loss: 11.088189, loss: 7.313488
00:50:29.408138 Step[6600/6995], lr: 0.000001, mv_avg_loss: 12.706007, loss: 9.572878
00:50:34.136634 Step[6700/6995], lr: 0.000001, mv_avg_loss: 12.494267, loss: 10.897083
00:50:38.862196 Step[6800/6995], lr: 0.000001, mv_avg_loss: 13.012590, loss: 8.744666
00:50:43.594419 Step[6900/6995], lr: 0.000001, mv_avg_loss: 12.068800, loss: 7.275617
Testing for epoch: 80
Average test PNSR is 25.323454 for 500 images
Start to train epoch 81
00:51:10.039663 Step[0/6995], lr: 0.000010, mv_avg_loss: 13.639011, loss: 15.442558
00:51:14.771268 Step[100/6995], lr: 0.000010, mv_avg_loss: 12.600132, loss: 13.607802
00:51:19.491671 Step[200/6995], lr: 0.000010, mv_avg_loss: 11.954264, loss: 17.069252
00:51:24.225050 Step[300/6995], lr: 0.000010, mv_avg_loss: 12.225178, loss: 14.654308
00:51:28.978202 Step[400/6995], lr: 0.000010, mv_avg_loss: 11.944536, loss: 19.870386
00:51:33.717791 Step[500/6995], lr: 0.000010, mv_avg_loss: 12.097704, loss: 19.786932
00:51:38.463709 Step[600/6995], lr: 0.000010, mv_avg_loss: 12.561872, loss: 12.883492
00:51:43.201368 Step[700/6995], lr: 0.000010, mv_avg_loss: 12.734223, loss: 11.734851
00:51:47.943910 Step[800/6995], lr: 0.000010, mv_avg_loss: 12.172265, loss: 7.593565
00:51:52.681615 Step[900/6995], lr: 0.000010, mv_avg_loss: 11.786043, loss: 8.504217
00:51:57.421284 Step[1000/6995], lr: 0.000010, mv_avg_loss: 12.549941, loss: 11.716780
00:52:02.162132 Step[1100/6995], lr: 0.000010, mv_avg_loss: 11.604014, loss: 12.661196
00:52:06.916685 Step[1200/6995], lr: 0.000010, mv_avg_loss: 12.798477, loss: 9.253906
00:52:11.652281 Step[1300/6995], lr: 0.000010, mv_avg_loss: 12.729676, loss: 7.873260
00:52:16.402476 Step[1400/6995], lr: 0.000010, mv_avg_loss: 12.566976, loss: 13.886423
00:52:21.140947 Step[1500/6995], lr: 0.000010, mv_avg_loss: 12.049352, loss: 10.590527
00:52:25.875647 Step[1600/6995], lr: 0.000010, mv_avg_loss: 12.516644, loss: 9.701117
00:52:30.613686 Step[1700/6995], lr: 0.000010, mv_avg_loss: 13.024761, loss: 11.869614
00:52:35.343804 Step[1800/6995], lr: 0.000010, mv_avg_loss: 12.779936, loss: 6.520887
00:52:40.079323 Step[1900/6995], lr: 0.000010, mv_avg_loss: 11.774246, loss: 7.692471
00:52:44.818277 Step[2000/6995], lr: 0.000010, mv_avg_loss: 13.825146, loss: 14.013125
00:52:49.560543 Step[2100/6995], lr: 0.000010, mv_avg_loss: 13.180299, loss: 10.800961
00:52:54.312542 Step[2200/6995], lr: 0.000010, mv_avg_loss: 12.234334, loss: 8.072292
00:52:59.066559 Step[2300/6995], lr: 0.000010, mv_avg_loss: 11.829227, loss: 15.594162
00:53:03.820520 Step[2400/6995], lr: 0.000010, mv_avg_loss: 13.256894, loss: 10.103815
00:53:08.557658 Step[2500/6995], lr: 0.000010, mv_avg_loss: 12.601125, loss: 11.180385
00:53:13.307668 Step[2600/6995], lr: 0.000010, mv_avg_loss: 11.899493, loss: 9.905960
00:53:18.068201 Step[2700/6995], lr: 0.000010, mv_avg_loss: 12.427516, loss: 6.004444
00:53:22.818818 Step[2800/6995], lr: 0.000010, mv_avg_loss: 12.242946, loss: 10.954894
00:53:27.566354 Step[2900/6995], lr: 0.000010, mv_avg_loss: 11.896901, loss: 7.692038
00:53:32.319837 Step[3000/6995], lr: 0.000010, mv_avg_loss: 13.193155, loss: 14.859573
00:53:37.078250 Step[3100/6995], lr: 0.000010, mv_avg_loss: 13.371861, loss: 29.287251
00:53:41.819174 Step[3200/6995], lr: 0.000010, mv_avg_loss: 12.687393, loss: 12.844406
00:53:46.564456 Step[3300/6995], lr: 0.000010, mv_avg_loss: 12.555051, loss: 8.125641
00:53:51.308977 Step[3400/6995], lr: 0.000010, mv_avg_loss: 12.873197, loss: 10.693306
00:53:56.080191 Step[3500/6995], lr: 0.000010, mv_avg_loss: 12.927560, loss: 9.299068
00:54:00.828741 Step[3600/6995], lr: 0.000010, mv_avg_loss: 12.334950, loss: 10.076118
00:54:05.558789 Step[3700/6995], lr: 0.000010, mv_avg_loss: 12.530103, loss: 9.853487
00:54:10.310098 Step[3800/6995], lr: 0.000010, mv_avg_loss: 12.362685, loss: 8.578041
00:54:15.048378 Step[3900/6995], lr: 0.000010, mv_avg_loss: 11.966964, loss: 10.158108
00:54:19.801545 Step[4000/6995], lr: 0.000010, mv_avg_loss: 13.398570, loss: 15.218740
00:54:24.545924 Step[4100/6995], lr: 0.000010, mv_avg_loss: 12.027788, loss: 13.058537
00:54:29.303690 Step[4200/6995], lr: 0.000010, mv_avg_loss: 13.225108, loss: 10.449943
00:54:34.060322 Step[4300/6995], lr: 0.000010, mv_avg_loss: 12.089853, loss: 7.660241
00:54:38.808606 Step[4400/6995], lr: 0.000010, mv_avg_loss: 11.769179, loss: 6.886863
00:54:43.568340 Step[4500/6995], lr: 0.000010, mv_avg_loss: 12.274484, loss: 16.090672
00:54:48.324519 Step[4600/6995], lr: 0.000010, mv_avg_loss: 12.240510, loss: 12.132628
00:54:53.075831 Step[4700/6995], lr: 0.000010, mv_avg_loss: 11.549091, loss: 16.323023
00:54:57.809203 Step[4800/6995], lr: 0.000010, mv_avg_loss: 12.224042, loss: 15.224841
00:55:02.553533 Step[4900/6995], lr: 0.000010, mv_avg_loss: 12.693256, loss: 10.582559
00:55:07.301803 Step[5000/6995], lr: 0.000010, mv_avg_loss: 12.901936, loss: 18.041922
00:55:12.046776 Step[5100/6995], lr: 0.000010, mv_avg_loss: 11.290719, loss: 10.397327
00:55:16.803776 Step[5200/6995], lr: 0.000010, mv_avg_loss: 12.287425, loss: 18.490091
00:55:21.553928 Step[5300/6995], lr: 0.000010, mv_avg_loss: 12.397908, loss: 14.261100
00:55:26.307004 Step[5400/6995], lr: 0.000010, mv_avg_loss: 12.821520, loss: 8.995013
00:55:31.052802 Step[5500/6995], lr: 0.000010, mv_avg_loss: 12.712355, loss: 19.352736
00:55:35.794177 Step[5600/6995], lr: 0.000010, mv_avg_loss: 12.842121, loss: 15.981903
00:55:40.534479 Step[5700/6995], lr: 0.000010, mv_avg_loss: 12.370279, loss: 8.889303
00:55:45.276412 Step[5800/6995], lr: 0.000010, mv_avg_loss: 12.400749, loss: 14.773280
00:55:50.011757 Step[5900/6995], lr: 0.000010, mv_avg_loss: 11.662402, loss: 13.193222
00:55:54.765089 Step[6000/6995], lr: 0.000010, mv_avg_loss: 11.095986, loss: 8.785429
00:55:59.514704 Step[6100/6995], lr: 0.000010, mv_avg_loss: 12.252782, loss: 8.365765
00:56:04.277595 Step[6200/6995], lr: 0.000010, mv_avg_loss: 12.506611, loss: 10.100321
00:56:09.015258 Step[6300/6995], lr: 0.000010, mv_avg_loss: 12.367837, loss: 8.665627
00:56:13.754819 Step[6400/6995], lr: 0.000010, mv_avg_loss: 11.021119, loss: 8.042243
00:56:18.501664 Step[6500/6995], lr: 0.000010, mv_avg_loss: 12.182718, loss: 11.038387
00:56:23.242814 Step[6600/6995], lr: 0.000010, mv_avg_loss: 11.800715, loss: 6.494310
00:56:27.973342 Step[6700/6995], lr: 0.000010, mv_avg_loss: 12.577933, loss: 7.302346
00:56:32.714095 Step[6800/6995], lr: 0.000010, mv_avg_loss: 12.239865, loss: 12.731849
00:56:37.446410 Step[6900/6995], lr: 0.000010, mv_avg_loss: 12.913301, loss: 23.975317
Testing for epoch: 81
Average test PNSR is 25.317327 for 500 images
Start to train epoch 82
00:57:03.998456 Step[0/6995], lr: 0.000010, mv_avg_loss: 12.442885, loss: 9.327647
00:57:08.723560 Step[100/6995], lr: 0.000010, mv_avg_loss: 12.338354, loss: 11.452673
00:57:13.466072 Step[200/6995], lr: 0.000010, mv_avg_loss: 14.092473, loss: 12.308434
00:57:18.199281 Step[300/6995], lr: 0.000010, mv_avg_loss: 11.606473, loss: 6.045341
00:57:22.936642 Step[400/6995], lr: 0.000010, mv_avg_loss: 12.866273, loss: 9.328060
00:57:27.673101 Step[500/6995], lr: 0.000010, mv_avg_loss: 12.228067, loss: 11.834944
00:57:32.399915 Step[600/6995], lr: 0.000010, mv_avg_loss: 12.050586, loss: 12.173668
00:57:37.137083 Step[700/6995], lr: 0.000010, mv_avg_loss: 12.472500, loss: 14.245723
00:57:41.870292 Step[800/6995], lr: 0.000010, mv_avg_loss: 13.116285, loss: 19.658651
00:57:46.598684 Step[900/6995], lr: 0.000010, mv_avg_loss: 12.230438, loss: 14.607802
00:57:51.331638 Step[1000/6995], lr: 0.000010, mv_avg_loss: 12.258203, loss: 9.243616
00:57:56.062918 Step[1100/6995], lr: 0.000010, mv_avg_loss: 12.500196, loss: 14.839442
00:58:00.805688 Step[1200/6995], lr: 0.000010, mv_avg_loss: 12.665115, loss: 9.252712
00:58:05.548446 Step[1300/6995], lr: 0.000010, mv_avg_loss: 12.498181, loss: 8.277995
00:58:10.295144 Step[1400/6995], lr: 0.000010, mv_avg_loss: 13.172738, loss: 10.248893
00:58:15.038626 Step[1500/6995], lr: 0.000010, mv_avg_loss: 13.227800, loss: 12.701270
00:58:19.782905 Step[1600/6995], lr: 0.000010, mv_avg_loss: 11.536453, loss: 9.504965
00:58:24.522076 Step[1700/6995], lr: 0.000010, mv_avg_loss: 12.206858, loss: 9.723858
00:58:29.252726 Step[1800/6995], lr: 0.000010, mv_avg_loss: 12.659508, loss: 18.786434
00:58:33.999938 Step[1900/6995], lr: 0.000010, mv_avg_loss: 12.367863, loss: 8.725204
00:58:38.726697 Step[2000/6995], lr: 0.000010, mv_avg_loss: 12.447056, loss: 11.330459
00:58:43.456770 Step[2100/6995], lr: 0.000010, mv_avg_loss: 12.052060, loss: 14.004784
00:58:48.181243 Step[2200/6995], lr: 0.000010, mv_avg_loss: 12.577148, loss: 6.948715
00:58:52.914977 Step[2300/6995], lr: 0.000010, mv_avg_loss: 12.445521, loss: 13.062731
00:58:57.634661 Step[2400/6995], lr: 0.000010, mv_avg_loss: 12.469192, loss: 15.961321
00:59:02.372958 Step[2500/6995], lr: 0.000010, mv_avg_loss: 11.191646, loss: 15.294968
00:59:07.099752 Step[2600/6995], lr: 0.000010, mv_avg_loss: 12.365327, loss: 16.118134
00:59:11.832276 Step[2700/6995], lr: 0.000010, mv_avg_loss: 13.751401, loss: 13.233673
00:59:16.559705 Step[2800/6995], lr: 0.000010, mv_avg_loss: 11.805875, loss: 11.159663
00:59:21.280860 Step[2900/6995], lr: 0.000010, mv_avg_loss: 12.413989, loss: 7.795820
00:59:25.994511 Step[3000/6995], lr: 0.000010, mv_avg_loss: 12.674532, loss: 11.910644
00:59:30.725277 Step[3100/6995], lr: 0.000010, mv_avg_loss: 11.213036, loss: 9.964469
00:59:35.456508 Step[3200/6995], lr: 0.000010, mv_avg_loss: 12.544688, loss: 9.186575
00:59:40.163626 Step[3300/6995], lr: 0.000010, mv_avg_loss: 12.900791, loss: 7.553741
00:59:44.875610 Step[3400/6995], lr: 0.000010, mv_avg_loss: 12.364957, loss: 9.767683
00:59:49.598599 Step[3500/6995], lr: 0.000010, mv_avg_loss: 11.298130, loss: 8.321016
00:59:54.335870 Step[3600/6995], lr: 0.000010, mv_avg_loss: 12.515591, loss: 12.580517
00:59:59.052031 Step[3700/6995], lr: 0.000010, mv_avg_loss: 11.944802, loss: 8.833916
01:00:03.768096 Step[3800/6995], lr: 0.000010, mv_avg_loss: 12.801392, loss: 11.443217
01:00:08.505263 Step[3900/6995], lr: 0.000010, mv_avg_loss: 12.584586, loss: 10.411561
01:00:13.222681 Step[4000/6995], lr: 0.000010, mv_avg_loss: 12.674584, loss: 8.657948
01:00:17.953988 Step[4100/6995], lr: 0.000010, mv_avg_loss: 11.673878, loss: 11.258413
01:00:22.684361 Step[4200/6995], lr: 0.000010, mv_avg_loss: 12.517338, loss: 6.998736
01:00:27.416480 Step[4300/6995], lr: 0.000010, mv_avg_loss: 11.503247, loss: 8.849923
01:00:32.145861 Step[4400/6995], lr: 0.000010, mv_avg_loss: 11.877347, loss: 12.692837
01:00:36.874204 Step[4500/6995], lr: 0.000010, mv_avg_loss: 11.769477, loss: 12.353962
01:00:41.600387 Step[4600/6995], lr: 0.000010, mv_avg_loss: 12.028826, loss: 7.743547
01:00:46.329413 Step[4700/6995], lr: 0.000010, mv_avg_loss: 12.608106, loss: 9.531857
01:00:51.064112 Step[4800/6995], lr: 0.000010, mv_avg_loss: 12.892898, loss: 15.878609
01:00:55.794394 Step[4900/6995], lr: 0.000010, mv_avg_loss: 11.380930, loss: 7.209459
01:01:00.528713 Step[5000/6995], lr: 0.000010, mv_avg_loss: 13.045262, loss: 8.768844
01:01:05.259543 Step[5100/6995], lr: 0.000010, mv_avg_loss: 11.357662, loss: 7.996006
01:01:09.985477 Step[5200/6995], lr: 0.000010, mv_avg_loss: 12.428015, loss: 9.876263
01:01:14.727152 Step[5300/6995], lr: 0.000010, mv_avg_loss: 11.744705, loss: 10.500132
01:01:19.452754 Step[5400/6995], lr: 0.000010, mv_avg_loss: 12.044166, loss: 11.642230
01:01:24.179873 Step[5500/6995], lr: 0.000010, mv_avg_loss: 13.147894, loss: 12.380928
01:01:28.896922 Step[5600/6995], lr: 0.000010, mv_avg_loss: 11.200588, loss: 13.140257
01:01:33.623480 Step[5700/6995], lr: 0.000010, mv_avg_loss: 13.454598, loss: 16.431652
01:01:38.346810 Step[5800/6995], lr: 0.000010, mv_avg_loss: 11.929327, loss: 11.838699
01:01:43.083947 Step[5900/6995], lr: 0.000010, mv_avg_loss: 12.168469, loss: 11.552292
01:01:47.821688 Step[6000/6995], lr: 0.000010, mv_avg_loss: 12.501161, loss: 14.963247
01:01:52.534649 Step[6100/6995], lr: 0.000010, mv_avg_loss: 12.136207, loss: 8.484035
01:01:57.262047 Step[6200/6995], lr: 0.000010, mv_avg_loss: 12.372984, loss: 21.065796
01:02:01.981434 Step[6300/6995], lr: 0.000010, mv_avg_loss: 12.534216, loss: 10.780334
01:02:06.703185 Step[6400/6995], lr: 0.000010, mv_avg_loss: 12.380224, loss: 8.382302
01:02:11.459746 Step[6500/6995], lr: 0.000010, mv_avg_loss: 12.432495, loss: 10.239280
01:02:16.184019 Step[6600/6995], lr: 0.000010, mv_avg_loss: 12.318549, loss: 12.974970
01:02:20.907493 Step[6700/6995], lr: 0.000010, mv_avg_loss: 11.515255, loss: 5.574064
01:02:25.630837 Step[6800/6995], lr: 0.000010, mv_avg_loss: 12.895289, loss: 12.505427
01:02:30.354519 Step[6900/6995], lr: 0.000010, mv_avg_loss: 12.987435, loss: 10.744503
Testing for epoch: 82
Average test PNSR is 25.319418 for 500 images
Start to train epoch 83
01:02:57.441562 Step[0/6995], lr: 0.000010, mv_avg_loss: 13.116357, loss: 21.815733
01:03:02.142866 Step[100/6995], lr: 0.000010, mv_avg_loss: 11.551894, loss: 13.272554
01:03:06.899913 Step[200/6995], lr: 0.000010, mv_avg_loss: 12.806447, loss: 16.735069
01:03:11.607061 Step[300/6995], lr: 0.000010, mv_avg_loss: 13.361926, loss: 10.614682
01:03:16.317247 Step[400/6995], lr: 0.000010, mv_avg_loss: 13.245262, loss: 10.134864
01:03:21.033778 Step[500/6995], lr: 0.000010, mv_avg_loss: 12.941373, loss: 12.488579
01:03:25.738394 Step[600/6995], lr: 0.000010, mv_avg_loss: 11.603156, loss: 8.301075
01:03:30.445841 Step[700/6995], lr: 0.000010, mv_avg_loss: 12.168286, loss: 15.276107
01:03:35.149161 Step[800/6995], lr: 0.000010, mv_avg_loss: 11.873586, loss: 12.622816
01:03:39.859314 Step[900/6995], lr: 0.000010, mv_avg_loss: 13.259140, loss: 10.878551
01:03:44.590118 Step[1000/6995], lr: 0.000010, mv_avg_loss: 12.255499, loss: 10.533182
01:03:49.315751 Step[1100/6995], lr: 0.000010, mv_avg_loss: 11.667956, loss: 13.239023
01:03:54.041946 Step[1200/6995], lr: 0.000010, mv_avg_loss: 12.144894, loss: 10.832167
01:03:58.781419 Step[1300/6995], lr: 0.000010, mv_avg_loss: 12.426104, loss: 12.199934
01:04:03.506598 Step[1400/6995], lr: 0.000010, mv_avg_loss: 11.956213, loss: 9.339273
01:04:08.232387 Step[1500/6995], lr: 0.000010, mv_avg_loss: 12.386564, loss: 14.880573
01:04:12.966154 Step[1600/6995], lr: 0.000010, mv_avg_loss: 13.011858, loss: 9.584682
01:04:17.702146 Step[1700/6995], lr: 0.000010, mv_avg_loss: 12.888062, loss: 12.221595
01:04:22.444586 Step[1800/6995], lr: 0.000010, mv_avg_loss: 12.466680, loss: 8.031576
01:04:27.180111 Step[1900/6995], lr: 0.000010, mv_avg_loss: 12.451198, loss: 8.318184
01:04:31.911822 Step[2000/6995], lr: 0.000010, mv_avg_loss: 11.524961, loss: 8.804592
01:04:36.633846 Step[2100/6995], lr: 0.000010, mv_avg_loss: 12.384247, loss: 15.044348
01:04:41.363114 Step[2200/6995], lr: 0.000010, mv_avg_loss: 12.629950, loss: 12.335170
01:04:46.087879 Step[2300/6995], lr: 0.000010, mv_avg_loss: 12.375485, loss: 14.687309
01:04:50.820618 Step[2400/6995], lr: 0.000010, mv_avg_loss: 11.084043, loss: 6.537916
01:04:55.562268 Step[2500/6995], lr: 0.000010, mv_avg_loss: 12.298527, loss: 10.404931
01:05:00.298387 Step[2600/6995], lr: 0.000010, mv_avg_loss: 12.485589, loss: 8.341669
01:05:05.039457 Step[2700/6995], lr: 0.000010, mv_avg_loss: 12.472601, loss: 24.682434
01:05:09.783714 Step[2800/6995], lr: 0.000010, mv_avg_loss: 13.566992, loss: 20.806240
01:05:14.522746 Step[2900/6995], lr: 0.000010, mv_avg_loss: 12.061541, loss: 8.749889
01:05:19.267302 Step[3000/6995], lr: 0.000010, mv_avg_loss: 12.412953, loss: 8.124903
01:05:24.016578 Step[3100/6995], lr: 0.000010, mv_avg_loss: 12.521090, loss: 13.385443
01:05:28.753161 Step[3200/6995], lr: 0.000010, mv_avg_loss: 13.020292, loss: 7.363139
01:05:33.484601 Step[3300/6995], lr: 0.000010, mv_avg_loss: 12.477365, loss: 10.079051
01:05:38.214567 Step[3400/6995], lr: 0.000010, mv_avg_loss: 13.134972, loss: 10.833114
01:05:42.951635 Step[3500/6995], lr: 0.000010, mv_avg_loss: 12.883659, loss: 10.903032
01:05:47.691874 Step[3600/6995], lr: 0.000010, mv_avg_loss: 12.337223, loss: 7.494489
01:05:52.401341 Step[3700/6995], lr: 0.000010, mv_avg_loss: 12.287214, loss: 12.306811
01:05:57.120364 Step[3800/6995], lr: 0.000010, mv_avg_loss: 11.237125, loss: 9.149142
01:06:01.864931 Step[3900/6995], lr: 0.000010, mv_avg_loss: 12.359114, loss: 9.941729
01:06:06.598088 Step[4000/6995], lr: 0.000010, mv_avg_loss: 12.084072, loss: 13.180519
01:06:11.333420 Step[4100/6995], lr: 0.000010, mv_avg_loss: 11.913167, loss: 11.041756
01:06:16.068014 Step[4200/6995], lr: 0.000010, mv_avg_loss: 12.625371, loss: 10.732067
01:06:20.803771 Step[4300/6995], lr: 0.000010, mv_avg_loss: 12.932832, loss: 11.683615
01:06:25.528435 Step[4400/6995], lr: 0.000010, mv_avg_loss: 12.002439, loss: 6.485410
01:06:30.291064 Step[4500/6995], lr: 0.000010, mv_avg_loss: 13.473952, loss: 12.143940
01:06:35.034523 Step[4600/6995], lr: 0.000010, mv_avg_loss: 12.302308, loss: 8.617826
01:06:39.776646 Step[4700/6995], lr: 0.000010, mv_avg_loss: 12.524541, loss: 12.436892
01:06:44.503978 Step[4800/6995], lr: 0.000010, mv_avg_loss: 12.329742, loss: 9.516830
01:06:49.232982 Step[4900/6995], lr: 0.000010, mv_avg_loss: 11.471234, loss: 18.343622
01:06:53.968730 Step[5000/6995], lr: 0.000010, mv_avg_loss: 12.490879, loss: 11.679630
01:06:58.700663 Step[5100/6995], lr: 0.000010, mv_avg_loss: 12.782094, loss: 5.374977
01:07:03.445835 Step[5200/6995], lr: 0.000010, mv_avg_loss: 12.739901, loss: 11.198799
01:07:08.183090 Step[5300/6995], lr: 0.000010, mv_avg_loss: 12.591266, loss: 10.201487
01:07:12.928149 Step[5400/6995], lr: 0.000010, mv_avg_loss: 12.854158, loss: 16.044731
01:07:17.663679 Step[5500/6995], lr: 0.000010, mv_avg_loss: 10.895449, loss: 17.928341
01:07:22.409194 Step[5600/6995], lr: 0.000010, mv_avg_loss: 12.817616, loss: 7.316576
01:07:27.146354 Step[5700/6995], lr: 0.000010, mv_avg_loss: 12.217280, loss: 9.693958
01:07:31.897299 Step[5800/6995], lr: 0.000010, mv_avg_loss: 12.420233, loss: 7.802484
01:07:36.656630 Step[5900/6995], lr: 0.000010, mv_avg_loss: 12.987638, loss: 12.441032
01:07:41.398808 Step[6000/6995], lr: 0.000010, mv_avg_loss: 12.686174, loss: 20.911518
01:07:46.155015 Step[6100/6995], lr: 0.000010, mv_avg_loss: 12.570418, loss: 7.005728
01:07:50.902608 Step[6200/6995], lr: 0.000010, mv_avg_loss: 11.819001, loss: 9.593332
01:07:55.657466 Step[6300/6995], lr: 0.000010, mv_avg_loss: 12.125149, loss: 18.158386
01:08:00.413447 Step[6400/6995], lr: 0.000010, mv_avg_loss: 11.490842, loss: 9.775386
01:08:05.171933 Step[6500/6995], lr: 0.000010, mv_avg_loss: 11.976825, loss: 19.311964
01:08:09.918144 Step[6600/6995], lr: 0.000010, mv_avg_loss: 11.330399, loss: 9.125392
01:08:14.676112 Step[6700/6995], lr: 0.000010, mv_avg_loss: 12.226569, loss: 11.903717
01:08:19.424003 Step[6800/6995], lr: 0.000010, mv_avg_loss: 12.477932, loss: 10.148121
01:08:24.177460 Step[6900/6995], lr: 0.000010, mv_avg_loss: 11.698478, loss: 13.363097
Testing for epoch: 83
Average test PNSR is 25.327863 for 500 images
Start to train epoch 84
01:08:50.103102 Step[0/6995], lr: 0.000010, mv_avg_loss: 11.223641, loss: 9.466656
01:08:54.845691 Step[100/6995], lr: 0.000010, mv_avg_loss: 12.318600, loss: 17.237972
01:08:59.604657 Step[200/6995], lr: 0.000010, mv_avg_loss: 12.660689, loss: 17.506308
01:09:04.359917 Step[300/6995], lr: 0.000010, mv_avg_loss: 13.054161, loss: 9.482681
01:09:09.099571 Step[400/6995], lr: 0.000010, mv_avg_loss: 11.966751, loss: 17.890671
01:09:13.843725 Step[500/6995], lr: 0.000010, mv_avg_loss: 13.204932, loss: 10.012117
01:09:18.589369 Step[600/6995], lr: 0.000010, mv_avg_loss: 11.705189, loss: 7.778360
01:09:23.332776 Step[700/6995], lr: 0.000010, mv_avg_loss: 12.960582, loss: 14.694285
01:09:28.078742 Step[800/6995], lr: 0.000010, mv_avg_loss: 12.399254, loss: 10.836627
01:09:32.828829 Step[900/6995], lr: 0.000010, mv_avg_loss: 12.082648, loss: 18.053511
01:09:37.559849 Step[1000/6995], lr: 0.000010, mv_avg_loss: 12.878638, loss: 14.866011
01:09:42.301981 Step[1100/6995], lr: 0.000010, mv_avg_loss: 13.310278, loss: 12.735060
01:09:47.043525 Step[1200/6995], lr: 0.000010, mv_avg_loss: 11.880233, loss: 8.254479
01:09:51.783816 Step[1300/6995], lr: 0.000010, mv_avg_loss: 12.599977, loss: 13.576606
01:09:56.531136 Step[1400/6995], lr: 0.000010, mv_avg_loss: 12.703964, loss: 18.999151
01:10:01.281218 Step[1500/6995], lr: 0.000010, mv_avg_loss: 10.833581, loss: 10.251285
01:10:06.032463 Step[1600/6995], lr: 0.000010, mv_avg_loss: 12.741356, loss: 21.924719
01:10:10.777911 Step[1700/6995], lr: 0.000010, mv_avg_loss: 11.806628, loss: 9.881343
01:10:15.527025 Step[1800/6995], lr: 0.000010, mv_avg_loss: 12.178947, loss: 15.188617
01:10:20.280749 Step[1900/6995], lr: 0.000010, mv_avg_loss: 12.044666, loss: 9.375903
01:10:25.030875 Step[2000/6995], lr: 0.000010, mv_avg_loss: 11.831809, loss: 9.000454
01:10:29.779577 Step[2100/6995], lr: 0.000010, mv_avg_loss: 12.799644, loss: 14.823086
01:10:34.523741 Step[2200/6995], lr: 0.000010, mv_avg_loss: 11.949707, loss: 12.752150
01:10:39.253539 Step[2300/6995], lr: 0.000010, mv_avg_loss: 13.217446, loss: 11.863406
01:10:43.956021 Step[2400/6995], lr: 0.000010, mv_avg_loss: 12.561558, loss: 11.816596
01:10:48.709464 Step[2500/6995], lr: 0.000010, mv_avg_loss: 12.860710, loss: 12.234829
01:10:53.459639 Step[2600/6995], lr: 0.000010, mv_avg_loss: 11.824299, loss: 11.133750
01:10:58.188287 Step[2700/6995], lr: 0.000010, mv_avg_loss: 13.080340, loss: 28.834473
01:11:02.930970 Step[2800/6995], lr: 0.000010, mv_avg_loss: 11.835447, loss: 16.964933
01:11:07.673925 Step[2900/6995], lr: 0.000010, mv_avg_loss: 11.643450, loss: 9.775546
01:11:12.406124 Step[3000/6995], lr: 0.000010, mv_avg_loss: 11.528617, loss: 13.158079
01:11:17.158823 Step[3100/6995], lr: 0.000010, mv_avg_loss: 13.597702, loss: 14.825454
01:11:21.904862 Step[3200/6995], lr: 0.000010, mv_avg_loss: 13.060612, loss: 14.375270
01:11:26.649364 Step[3300/6995], lr: 0.000010, mv_avg_loss: 12.568072, loss: 11.986252
01:11:31.392206 Step[3400/6995], lr: 0.000010, mv_avg_loss: 12.996753, loss: 22.401512
01:11:36.138165 Step[3500/6995], lr: 0.000010, mv_avg_loss: 12.124891, loss: 10.789427
01:11:40.877321 Step[3600/6995], lr: 0.000010, mv_avg_loss: 12.246231, loss: 14.428096
01:11:45.624128 Step[3700/6995], lr: 0.000010, mv_avg_loss: 11.558255, loss: 12.802689
01:11:50.358518 Step[3800/6995], lr: 0.000010, mv_avg_loss: 12.248315, loss: 9.532727
01:11:55.102333 Step[3900/6995], lr: 0.000010, mv_avg_loss: 12.084187, loss: 9.962000
01:11:59.833189 Step[4000/6995], lr: 0.000010, mv_avg_loss: 10.542742, loss: 6.157592
01:12:04.573136 Step[4100/6995], lr: 0.000010, mv_avg_loss: 12.518302, loss: 10.460572
01:12:09.300799 Step[4200/6995], lr: 0.000010, mv_avg_loss: 12.427911, loss: 8.041588
01:12:14.049031 Step[4300/6995], lr: 0.000010, mv_avg_loss: 11.715900, loss: 11.755114
01:12:18.807674 Step[4400/6995], lr: 0.000010, mv_avg_loss: 12.019191, loss: 8.252771
01:12:23.546810 Step[4500/6995], lr: 0.000010, mv_avg_loss: 12.872123, loss: 8.544389
01:12:28.297672 Step[4600/6995], lr: 0.000010, mv_avg_loss: 12.543876, loss: 12.329845
01:12:33.030127 Step[4700/6995], lr: 0.000010, mv_avg_loss: 11.707821, loss: 11.441473
01:12:37.763909 Step[4800/6995], lr: 0.000010, mv_avg_loss: 11.993864, loss: 11.477866
01:12:42.510739 Step[4900/6995], lr: 0.000010, mv_avg_loss: 13.649359, loss: 11.426249
01:12:47.247289 Step[5000/6995], lr: 0.000010, mv_avg_loss: 11.475492, loss: 13.272334
01:12:51.967161 Step[5100/6995], lr: 0.000010, mv_avg_loss: 11.731704, loss: 16.660557
01:12:56.697155 Step[5200/6995], lr: 0.000010, mv_avg_loss: 11.324217, loss: 15.009596
01:13:01.444009 Step[5300/6995], lr: 0.000010, mv_avg_loss: 12.185279, loss: 7.930958
01:13:06.182970 Step[5400/6995], lr: 0.000010, mv_avg_loss: 11.858125, loss: 9.005283
01:13:10.927820 Step[5500/6995], lr: 0.000010, mv_avg_loss: 11.780319, loss: 23.994041
01:13:15.664407 Step[5600/6995], lr: 0.000010, mv_avg_loss: 11.770098, loss: 10.109486
01:13:20.406885 Step[5700/6995], lr: 0.000010, mv_avg_loss: 12.925795, loss: 8.724185
01:13:25.124274 Step[5800/6995], lr: 0.000010, mv_avg_loss: 12.827408, loss: 12.740261
01:13:29.846954 Step[5900/6995], lr: 0.000010, mv_avg_loss: 11.800159, loss: 15.132373
01:13:34.574786 Step[6000/6995], lr: 0.000010, mv_avg_loss: 12.565289, loss: 13.058566
01:13:39.306506 Step[6100/6995], lr: 0.000010, mv_avg_loss: 11.504201, loss: 9.935638
01:13:44.038334 Step[6200/6995], lr: 0.000010, mv_avg_loss: 12.472957, loss: 10.112173
01:13:48.778435 Step[6300/6995], lr: 0.000010, mv_avg_loss: 12.474769, loss: 8.464104
01:13:53.522565 Step[6400/6995], lr: 0.000010, mv_avg_loss: 11.426587, loss: 9.117620
01:13:58.239967 Step[6500/6995], lr: 0.000010, mv_avg_loss: 12.615776, loss: 7.598112
01:14:02.977107 Step[6600/6995], lr: 0.000010, mv_avg_loss: 12.066404, loss: 13.182348
01:14:07.702105 Step[6700/6995], lr: 0.000010, mv_avg_loss: 12.143660, loss: 8.738858
01:14:12.432684 Step[6800/6995], lr: 0.000010, mv_avg_loss: 12.990179, loss: 10.825293
01:14:17.160019 Step[6900/6995], lr: 0.000010, mv_avg_loss: 12.103006, loss: 10.957798
saving model for epoch 84
Testing for epoch: 84
Average test PNSR is 25.325879 for 500 images
Start to train epoch 85
01:14:43.444431 Step[0/6995], lr: 0.000010, mv_avg_loss: 12.804648, loss: 5.270574
01:14:48.150982 Step[100/6995], lr: 0.000010, mv_avg_loss: 12.193176, loss: 12.816475
01:14:52.882086 Step[200/6995], lr: 0.000010, mv_avg_loss: 13.822041, loss: 18.088642
01:14:57.606744 Step[300/6995], lr: 0.000010, mv_avg_loss: 12.474686, loss: 6.255430
01:15:02.322973 Step[400/6995], lr: 0.000010, mv_avg_loss: 12.050931, loss: 13.200641
01:15:07.067730 Step[500/6995], lr: 0.000010, mv_avg_loss: 12.640657, loss: 8.424011
01:15:11.792017 Step[600/6995], lr: 0.000010, mv_avg_loss: 13.250132, loss: 10.820060
01:15:16.521829 Step[700/6995], lr: 0.000010, mv_avg_loss: 12.444299, loss: 8.945575
01:15:21.244598 Step[800/6995], lr: 0.000010, mv_avg_loss: 12.516739, loss: 9.147779
01:15:25.983584 Step[900/6995], lr: 0.000010, mv_avg_loss: 13.068831, loss: 15.400187
01:15:30.715133 Step[1000/6995], lr: 0.000010, mv_avg_loss: 12.690936, loss: 7.972808
01:15:35.452711 Step[1100/6995], lr: 0.000010, mv_avg_loss: 11.723591, loss: 14.251560
01:15:40.181171 Step[1200/6995], lr: 0.000010, mv_avg_loss: 12.614373, loss: 15.802254
01:15:44.922037 Step[1300/6995], lr: 0.000010, mv_avg_loss: 13.779780, loss: 16.319630
01:15:49.659117 Step[1400/6995], lr: 0.000010, mv_avg_loss: 12.166448, loss: 17.290333
01:15:54.382065 Step[1500/6995], lr: 0.000010, mv_avg_loss: 11.979637, loss: 13.844909
01:15:59.112254 Step[1600/6995], lr: 0.000010, mv_avg_loss: 12.944612, loss: 14.009395
01:16:03.837753 Step[1700/6995], lr: 0.000010, mv_avg_loss: 12.309331, loss: 12.191298
01:16:08.577751 Step[1800/6995], lr: 0.000010, mv_avg_loss: 12.177258, loss: 11.790251
01:16:13.308005 Step[1900/6995], lr: 0.000010, mv_avg_loss: 11.673991, loss: 7.053194
01:16:18.044587 Step[2000/6995], lr: 0.000010, mv_avg_loss: 11.325265, loss: 7.654335
01:16:22.794225 Step[2100/6995], lr: 0.000010, mv_avg_loss: 12.070358, loss: 16.503719
01:16:27.542236 Step[2200/6995], lr: 0.000010, mv_avg_loss: 12.164630, loss: 8.695880
01:16:32.268126 Step[2300/6995], lr: 0.000010, mv_avg_loss: 12.528592, loss: 21.244852
01:16:37.006901 Step[2400/6995], lr: 0.000010, mv_avg_loss: 12.685351, loss: 9.669870
01:16:41.739852 Step[2500/6995], lr: 0.000010, mv_avg_loss: 11.331589, loss: 15.952162
01:16:46.485711 Step[2600/6995], lr: 0.000010, mv_avg_loss: 12.828641, loss: 11.353937
01:16:51.219913 Step[2700/6995], lr: 0.000010, mv_avg_loss: 12.345103, loss: 20.562908
01:16:55.965252 Step[2800/6995], lr: 0.000010, mv_avg_loss: 11.808895, loss: 12.054896
01:17:00.698632 Step[2900/6995], lr: 0.000010, mv_avg_loss: 12.489804, loss: 11.736166
01:17:05.425876 Step[3000/6995], lr: 0.000010, mv_avg_loss: 13.103524, loss: 10.227608
01:17:10.162223 Step[3100/6995], lr: 0.000010, mv_avg_loss: 12.819844, loss: 8.690218
01:17:14.892191 Step[3200/6995], lr: 0.000010, mv_avg_loss: 13.251378, loss: 10.760201
01:17:19.632877 Step[3300/6995], lr: 0.000010, mv_avg_loss: 12.230915, loss: 16.648163
01:17:24.366913 Step[3400/6995], lr: 0.000010, mv_avg_loss: 12.622988, loss: 9.958595
01:17:29.114991 Step[3500/6995], lr: 0.000010, mv_avg_loss: 11.891098, loss: 7.900049
01:17:33.850961 Step[3600/6995], lr: 0.000010, mv_avg_loss: 12.506590, loss: 8.057855
01:17:38.586054 Step[3700/6995], lr: 0.000010, mv_avg_loss: 13.288775, loss: 8.446288
01:17:43.320660 Step[3800/6995], lr: 0.000010, mv_avg_loss: 12.214845, loss: 10.040215
01:17:48.047077 Step[3900/6995], lr: 0.000010, mv_avg_loss: 12.496246, loss: 21.671469
01:17:52.773948 Step[4000/6995], lr: 0.000010, mv_avg_loss: 12.535306, loss: 11.604013
01:17:57.509571 Step[4100/6995], lr: 0.000010, mv_avg_loss: 10.892965, loss: 7.379529
01:18:02.246342 Step[4200/6995], lr: 0.000010, mv_avg_loss: 12.182127, loss: 11.988723
01:18:06.972621 Step[4300/6995], lr: 0.000010, mv_avg_loss: 11.737296, loss: 10.936773
01:18:11.700530 Step[4400/6995], lr: 0.000010, mv_avg_loss: 12.001155, loss: 8.556038
01:18:16.439019 Step[4500/6995], lr: 0.000010, mv_avg_loss: 12.637658, loss: 15.045658
01:18:21.173846 Step[4600/6995], lr: 0.000010, mv_avg_loss: 12.780122, loss: 8.126617
01:18:25.907532 Step[4700/6995], lr: 0.000010, mv_avg_loss: 12.974454, loss: 13.732848
01:18:30.634829 Step[4800/6995], lr: 0.000010, mv_avg_loss: 12.810867, loss: 10.487857
01:18:35.366179 Step[4900/6995], lr: 0.000010, mv_avg_loss: 11.870975, loss: 11.398120
01:18:40.110311 Step[5000/6995], lr: 0.000010, mv_avg_loss: 11.882771, loss: 15.062065
01:18:44.846738 Step[5100/6995], lr: 0.000010, mv_avg_loss: 11.808214, loss: 14.863880
01:18:49.570701 Step[5200/6995], lr: 0.000010, mv_avg_loss: 13.162262, loss: 12.826442
01:18:54.314932 Step[5300/6995], lr: 0.000010, mv_avg_loss: 12.692334, loss: 11.132544
01:18:59.045767 Step[5400/6995], lr: 0.000010, mv_avg_loss: 13.169318, loss: 9.483891
01:19:03.809876 Step[5500/6995], lr: 0.000010, mv_avg_loss: 12.114511, loss: 7.985136
01:19:08.549921 Step[5600/6995], lr: 0.000010, mv_avg_loss: 12.640590, loss: 24.046169
01:19:13.296235 Step[5700/6995], lr: 0.000010, mv_avg_loss: 12.361648, loss: 12.050455
01:19:18.044219 Step[5800/6995], lr: 0.000010, mv_avg_loss: 12.264264, loss: 15.804243
01:19:22.793605 Step[5900/6995], lr: 0.000010, mv_avg_loss: 12.177037, loss: 11.959240
01:19:27.537173 Step[6000/6995], lr: 0.000010, mv_avg_loss: 12.131365, loss: 10.746923
01:19:32.279575 Step[6100/6995], lr: 0.000010, mv_avg_loss: 11.869948, loss: 8.362616
01:19:37.020536 Step[6200/6995], lr: 0.000010, mv_avg_loss: 11.397927, loss: 8.340701
01:19:41.765955 Step[6300/6995], lr: 0.000010, mv_avg_loss: 12.384187, loss: 13.154627
01:19:46.516502 Step[6400/6995], lr: 0.000010, mv_avg_loss: 12.666113, loss: 10.231813
01:19:51.271437 Step[6500/6995], lr: 0.000010, mv_avg_loss: 12.511599, loss: 12.424449
01:19:56.025012 Step[6600/6995], lr: 0.000010, mv_avg_loss: 12.327149, loss: 14.989296
01:20:00.774330 Step[6700/6995], lr: 0.000010, mv_avg_loss: 12.298089, loss: 9.314700
01:20:05.520159 Step[6800/6995], lr: 0.000010, mv_avg_loss: 12.627435, loss: 24.287014
01:20:10.268332 Step[6900/6995], lr: 0.000010, mv_avg_loss: 12.326473, loss: 8.763323
Testing for epoch: 85
Average test PNSR is 25.319825 for 500 images
Start to train epoch 86
01:20:36.625234 Step[0/6995], lr: 0.000010, mv_avg_loss: 12.663149, loss: 14.166151
01:20:41.363742 Step[100/6995], lr: 0.000010, mv_avg_loss: 11.870911, loss: 10.416121
01:20:46.103865 Step[200/6995], lr: 0.000010, mv_avg_loss: 12.716143, loss: 10.902950
01:20:50.837731 Step[300/6995], lr: 0.000010, mv_avg_loss: 11.792218, loss: 10.028284
01:20:55.572481 Step[400/6995], lr: 0.000010, mv_avg_loss: 12.779119, loss: 8.367390
01:21:00.318809 Step[500/6995], lr: 0.000010, mv_avg_loss: 12.408879, loss: 8.267174
01:21:05.058272 Step[600/6995], lr: 0.000010, mv_avg_loss: 11.864286, loss: 11.391582
01:21:09.803055 Step[700/6995], lr: 0.000010, mv_avg_loss: 12.708061, loss: 14.021334
01:21:14.537638 Step[800/6995], lr: 0.000010, mv_avg_loss: 11.837703, loss: 7.863569
01:21:19.279026 Step[900/6995], lr: 0.000010, mv_avg_loss: 11.783216, loss: 12.352743
01:21:24.030377 Step[1000/6995], lr: 0.000010, mv_avg_loss: 13.011847, loss: 17.654755
01:21:28.781110 Step[1100/6995], lr: 0.000010, mv_avg_loss: 12.402339, loss: 15.421336
01:21:33.533507 Step[1200/6995], lr: 0.000010, mv_avg_loss: 11.541432, loss: 9.066052
01:21:38.272256 Step[1300/6995], lr: 0.000010, mv_avg_loss: 11.862400, loss: 13.344265
01:21:43.022492 Step[1400/6995], lr: 0.000010, mv_avg_loss: 12.472022, loss: 12.045866
01:21:47.766327 Step[1500/6995], lr: 0.000010, mv_avg_loss: 12.084643, loss: 8.223536
01:21:52.512692 Step[1600/6995], lr: 0.000010, mv_avg_loss: 12.617342, loss: 10.481280
01:21:57.256110 Step[1700/6995], lr: 0.000010, mv_avg_loss: 12.180949, loss: 10.239590
01:22:02.000179 Step[1800/6995], lr: 0.000010, mv_avg_loss: 12.253316, loss: 13.531055
01:22:06.737972 Step[1900/6995], lr: 0.000010, mv_avg_loss: 11.913157, loss: 12.752066
01:22:11.489088 Step[2000/6995], lr: 0.000010, mv_avg_loss: 13.050335, loss: 6.958793
01:22:16.226594 Step[2100/6995], lr: 0.000010, mv_avg_loss: 11.926418, loss: 11.392088
01:22:20.965016 Step[2200/6995], lr: 0.000010, mv_avg_loss: 11.173048, loss: 11.125384
01:22:25.703453 Step[2300/6995], lr: 0.000010, mv_avg_loss: 12.717067, loss: 16.630367
01:22:30.450006 Step[2400/6995], lr: 0.000010, mv_avg_loss: 12.023159, loss: 10.181683
01:22:35.181021 Step[2500/6995], lr: 0.000010, mv_avg_loss: 11.548293, loss: 10.724577
01:22:39.928437 Step[2600/6995], lr: 0.000010, mv_avg_loss: 12.872944, loss: 23.231956
01:22:44.663535 Step[2700/6995], lr: 0.000010, mv_avg_loss: 12.260395, loss: 11.506763
01:22:49.401985 Step[2800/6995], lr: 0.000010, mv_avg_loss: 11.165739, loss: 6.397414
01:22:54.137916 Step[2900/6995], lr: 0.000010, mv_avg_loss: 12.830867, loss: 18.287794
01:22:58.876440 Step[3000/6995], lr: 0.000010, mv_avg_loss: 12.198668, loss: 12.865283
01:23:03.615777 Step[3100/6995], lr: 0.000010, mv_avg_loss: 11.755863, loss: 11.416096
01:23:08.338824 Step[3200/6995], lr: 0.000010, mv_avg_loss: 12.022423, loss: 16.693380
01:23:13.065973 Step[3300/6995], lr: 0.000010, mv_avg_loss: 12.203441, loss: 9.215899
01:23:17.796113 Step[3400/6995], lr: 0.000010, mv_avg_loss: 11.772443, loss: 8.123377
01:23:22.547965 Step[3500/6995], lr: 0.000010, mv_avg_loss: 12.108505, loss: 6.570893
01:23:27.290482 Step[3600/6995], lr: 0.000010, mv_avg_loss: 11.620939, loss: 10.482675
01:23:32.022155 Step[3700/6995], lr: 0.000010, mv_avg_loss: 11.995956, loss: 10.277252
01:23:36.770775 Step[3800/6995], lr: 0.000010, mv_avg_loss: 12.817181, loss: 14.144389
01:23:41.506767 Step[3900/6995], lr: 0.000010, mv_avg_loss: 12.302017, loss: 10.174623
01:23:46.260845 Step[4000/6995], lr: 0.000010, mv_avg_loss: 11.713596, loss: 11.013911
01:23:51.000973 Step[4100/6995], lr: 0.000010, mv_avg_loss: 12.379884, loss: 11.815933
01:23:55.750003 Step[4200/6995], lr: 0.000010, mv_avg_loss: 11.876050, loss: 9.848425
01:24:00.493775 Step[4300/6995], lr: 0.000010, mv_avg_loss: 12.468373, loss: 11.414040
01:24:05.234569 Step[4400/6995], lr: 0.000010, mv_avg_loss: 12.217185, loss: 12.216881
01:24:09.972369 Step[4500/6995], lr: 0.000010, mv_avg_loss: 12.490308, loss: 17.342680
01:24:14.706719 Step[4600/6995], lr: 0.000010, mv_avg_loss: 12.337049, loss: 17.574131
01:24:19.444360 Step[4700/6995], lr: 0.000010, mv_avg_loss: 12.510410, loss: 16.901945
01:24:24.172836 Step[4800/6995], lr: 0.000010, mv_avg_loss: 11.223706, loss: 8.110409
01:24:28.916538 Step[4900/6995], lr: 0.000010, mv_avg_loss: 11.872585, loss: 7.638319
01:24:33.651198 Step[5000/6995], lr: 0.000010, mv_avg_loss: 11.591184, loss: 10.660574
01:24:38.387229 Step[5100/6995], lr: 0.000010, mv_avg_loss: 13.020252, loss: 11.696112
01:24:43.117330 Step[5200/6995], lr: 0.000010, mv_avg_loss: 13.387385, loss: 8.491846
01:24:47.852944 Step[5300/6995], lr: 0.000010, mv_avg_loss: 13.148603, loss: 15.483451
01:24:52.587844 Step[5400/6995], lr: 0.000010, mv_avg_loss: 13.582608, loss: 14.130844
01:24:57.322735 Step[5500/6995], lr: 0.000010, mv_avg_loss: 12.992620, loss: 10.331429
01:25:02.062009 Step[5600/6995], lr: 0.000010, mv_avg_loss: 11.698279, loss: 9.625587
01:25:06.800104 Step[5700/6995], lr: 0.000010, mv_avg_loss: 11.872884, loss: 11.169966
01:25:11.536662 Step[5800/6995], lr: 0.000010, mv_avg_loss: 12.456099, loss: 7.968554
01:25:16.267576 Step[5900/6995], lr: 0.000010, mv_avg_loss: 11.493465, loss: 13.316894
01:25:20.997106 Step[6000/6995], lr: 0.000010, mv_avg_loss: 11.590371, loss: 10.272836
01:25:25.733015 Step[6100/6995], lr: 0.000010, mv_avg_loss: 12.022837, loss: 12.867682
01:25:30.469496 Step[6200/6995], lr: 0.000010, mv_avg_loss: 12.432918, loss: 12.889311
01:25:35.200610 Step[6300/6995], lr: 0.000010, mv_avg_loss: 12.485389, loss: 6.964072
01:25:39.929166 Step[6400/6995], lr: 0.000010, mv_avg_loss: 12.802535, loss: 12.069656
01:25:44.650625 Step[6500/6995], lr: 0.000010, mv_avg_loss: 11.133514, loss: 10.745926
01:25:49.378683 Step[6600/6995], lr: 0.000010, mv_avg_loss: 11.755965, loss: 10.610202
01:25:54.102883 Step[6700/6995], lr: 0.000010, mv_avg_loss: 12.804962, loss: 8.448470
01:25:58.840590 Step[6800/6995], lr: 0.000010, mv_avg_loss: 12.105505, loss: 13.576017
01:26:03.583446 Step[6900/6995], lr: 0.000010, mv_avg_loss: 12.461578, loss: 21.619087
Testing for epoch: 86
Average test PNSR is 25.332818 for 500 images
Start to train epoch 87
01:26:29.535185 Step[0/6995], lr: 0.000010, mv_avg_loss: 12.410588, loss: 10.300414
01:26:34.250406 Step[100/6995], lr: 0.000010, mv_avg_loss: 12.535548, loss: 12.095238
01:26:38.944890 Step[200/6995], lr: 0.000010, mv_avg_loss: 12.596624, loss: 10.439156
01:26:43.655940 Step[300/6995], lr: 0.000010, mv_avg_loss: 12.135304, loss: 11.738737
01:26:48.387411 Step[400/6995], lr: 0.000010, mv_avg_loss: 11.899406, loss: 13.911493
01:26:53.113784 Step[500/6995], lr: 0.000010, mv_avg_loss: 12.287181, loss: 8.220085
01:26:57.824712 Step[600/6995], lr: 0.000010, mv_avg_loss: 12.004621, loss: 15.068518
01:27:02.552393 Step[700/6995], lr: 0.000010, mv_avg_loss: 12.439813, loss: 16.878929
01:27:07.273079 Step[800/6995], lr: 0.000010, mv_avg_loss: 11.856173, loss: 12.527748
01:27:11.980835 Step[900/6995], lr: 0.000010, mv_avg_loss: 11.255747, loss: 8.575683
01:27:16.689872 Step[1000/6995], lr: 0.000010, mv_avg_loss: 12.347241, loss: 26.379452
01:27:21.413869 Step[1100/6995], lr: 0.000010, mv_avg_loss: 12.437623, loss: 12.211851
01:27:26.157121 Step[1200/6995], lr: 0.000010, mv_avg_loss: 12.142277, loss: 16.801514
01:27:30.872464 Step[1300/6995], lr: 0.000010, mv_avg_loss: 12.110456, loss: 9.577534
01:27:35.590230 Step[1400/6995], lr: 0.000010, mv_avg_loss: 12.255067, loss: 12.245003
01:27:40.353534 Step[1500/6995], lr: 0.000010, mv_avg_loss: 11.897001, loss: 9.247546
01:27:45.065977 Step[1600/6995], lr: 0.000010, mv_avg_loss: 12.881676, loss: 19.783749
01:27:49.779582 Step[1700/6995], lr: 0.000010, mv_avg_loss: 11.977400, loss: 17.294876
01:27:54.486018 Step[1800/6995], lr: 0.000010, mv_avg_loss: 12.285583, loss: 14.954352
01:27:59.211303 Step[1900/6995], lr: 0.000010, mv_avg_loss: 12.808289, loss: 18.319601
01:28:03.930315 Step[2000/6995], lr: 0.000010, mv_avg_loss: 12.632455, loss: 19.431423
01:28:08.637764 Step[2100/6995], lr: 0.000010, mv_avg_loss: 11.872829, loss: 10.573984
01:28:13.351912 Step[2200/6995], lr: 0.000010, mv_avg_loss: 12.789468, loss: 7.046007
01:28:18.070473 Step[2300/6995], lr: 0.000010, mv_avg_loss: 11.524475, loss: 11.621674
01:28:22.780308 Step[2400/6995], lr: 0.000010, mv_avg_loss: 12.242956, loss: 9.513844
01:28:27.516608 Step[2500/6995], lr: 0.000010, mv_avg_loss: 12.161622, loss: 11.407730
01:28:32.247812 Step[2600/6995], lr: 0.000010, mv_avg_loss: 11.917768, loss: 9.731193
01:28:36.981454 Step[2700/6995], lr: 0.000010, mv_avg_loss: 12.742163, loss: 7.133635
01:28:41.699879 Step[2800/6995], lr: 0.000010, mv_avg_loss: 11.778992, loss: 14.908779
01:28:46.432854 Step[2900/6995], lr: 0.000010, mv_avg_loss: 12.575540, loss: 13.910720
01:28:51.158894 Step[3000/6995], lr: 0.000010, mv_avg_loss: 12.174320, loss: 10.668903
01:28:55.876048 Step[3100/6995], lr: 0.000010, mv_avg_loss: 11.954686, loss: 19.737408
01:29:00.598975 Step[3200/6995], lr: 0.000010, mv_avg_loss: 11.967065, loss: 9.976546
01:29:05.329203 Step[3300/6995], lr: 0.000010, mv_avg_loss: 11.723495, loss: 6.508953
01:29:10.050750 Step[3400/6995], lr: 0.000010, mv_avg_loss: 12.517208, loss: 12.912890
01:29:14.782364 Step[3500/6995], lr: 0.000010, mv_avg_loss: 12.118473, loss: 10.412667
01:29:19.504359 Step[3600/6995], lr: 0.000010, mv_avg_loss: 12.050239, loss: 19.739906
01:29:24.223617 Step[3700/6995], lr: 0.000010, mv_avg_loss: 13.161146, loss: 16.223125
01:29:28.950662 Step[3800/6995], lr: 0.000010, mv_avg_loss: 11.951633, loss: 9.820354
01:29:33.674178 Step[3900/6995], lr: 0.000010, mv_avg_loss: 12.499490, loss: 16.073143
01:29:38.405020 Step[4000/6995], lr: 0.000010, mv_avg_loss: 11.713590, loss: 13.189154
01:29:43.132866 Step[4100/6995], lr: 0.000010, mv_avg_loss: 12.324604, loss: 9.477558
01:29:47.869241 Step[4200/6995], lr: 0.000010, mv_avg_loss: 11.225671, loss: 10.337420
01:29:52.584023 Step[4300/6995], lr: 0.000010, mv_avg_loss: 12.957046, loss: 11.701982
01:29:57.306320 Step[4400/6995], lr: 0.000010, mv_avg_loss: 12.980331, loss: 17.803303
01:30:02.029644 Step[4500/6995], lr: 0.000010, mv_avg_loss: 11.900489, loss: 11.809330
01:30:06.747651 Step[4600/6995], lr: 0.000010, mv_avg_loss: 11.038805, loss: 8.801668
01:30:11.467319 Step[4700/6995], lr: 0.000010, mv_avg_loss: 11.884806, loss: 10.569787
01:30:16.180261 Step[4800/6995], lr: 0.000010, mv_avg_loss: 12.019149, loss: 11.250036
01:30:20.913654 Step[4900/6995], lr: 0.000010, mv_avg_loss: 12.419712, loss: 17.897858
01:30:25.641491 Step[5000/6995], lr: 0.000010, mv_avg_loss: 12.735658, loss: 16.435339
01:30:30.371062 Step[5100/6995], lr: 0.000010, mv_avg_loss: 11.834222, loss: 16.149609
01:30:35.108932 Step[5200/6995], lr: 0.000010, mv_avg_loss: 13.208740, loss: 9.914770
01:30:39.832325 Step[5300/6995], lr: 0.000010, mv_avg_loss: 11.892983, loss: 14.537895
01:30:44.568595 Step[5400/6995], lr: 0.000010, mv_avg_loss: 12.419212, loss: 12.764846
01:30:49.276923 Step[5500/6995], lr: 0.000010, mv_avg_loss: 11.841871, loss: 8.711519
01:30:54.002076 Step[5600/6995], lr: 0.000010, mv_avg_loss: 11.944386, loss: 8.398571
01:30:58.726985 Step[5700/6995], lr: 0.000010, mv_avg_loss: 12.877622, loss: 11.687864
01:31:03.466278 Step[5800/6995], lr: 0.000010, mv_avg_loss: 12.276568, loss: 11.831759
01:31:08.199076 Step[5900/6995], lr: 0.000010, mv_avg_loss: 11.658264, loss: 7.794757
01:31:12.913960 Step[6000/6995], lr: 0.000010, mv_avg_loss: 11.596419, loss: 9.066882
01:31:17.650617 Step[6100/6995], lr: 0.000010, mv_avg_loss: 11.879064, loss: 7.552753
01:31:22.370120 Step[6200/6995], lr: 0.000010, mv_avg_loss: 11.702702, loss: 10.435875
01:31:27.089572 Step[6300/6995], lr: 0.000010, mv_avg_loss: 13.173817, loss: 16.137047
01:31:31.817017 Step[6400/6995], lr: 0.000010, mv_avg_loss: 11.736753, loss: 9.297079
01:31:36.560708 Step[6500/6995], lr: 0.000010, mv_avg_loss: 13.841837, loss: 11.644537
01:31:41.290636 Step[6600/6995], lr: 0.000010, mv_avg_loss: 12.171972, loss: 10.971605
01:31:46.018231 Step[6700/6995], lr: 0.000010, mv_avg_loss: 11.696922, loss: 10.509916
01:31:50.765899 Step[6800/6995], lr: 0.000010, mv_avg_loss: 11.755885, loss: 15.317397
01:31:55.493833 Step[6900/6995], lr: 0.000010, mv_avg_loss: 12.159468, loss: 9.732502
Testing for epoch: 87
Average test PNSR is 25.330879 for 500 images
Start to train epoch 88
01:32:22.253075 Step[0/6995], lr: 0.000010, mv_avg_loss: 12.314376, loss: 12.557246
01:32:26.983621 Step[100/6995], lr: 0.000010, mv_avg_loss: 11.652153, loss: 9.448101
01:32:31.719504 Step[200/6995], lr: 0.000010, mv_avg_loss: 12.573631, loss: 7.683404
01:32:36.447197 Step[300/6995], lr: 0.000010, mv_avg_loss: 12.089187, loss: 10.784439
01:32:41.190433 Step[400/6995], lr: 0.000010, mv_avg_loss: 12.238111, loss: 11.761169
01:32:45.923849 Step[500/6995], lr: 0.000010, mv_avg_loss: 11.574804, loss: 15.682041
01:32:50.666670 Step[600/6995], lr: 0.000010, mv_avg_loss: 12.659271, loss: 15.353966
01:32:55.376906 Step[700/6995], lr: 0.000010, mv_avg_loss: 12.213114, loss: 9.707403
01:33:00.106882 Step[800/6995], lr: 0.000010, mv_avg_loss: 12.312483, loss: 12.170268
01:33:04.833102 Step[900/6995], lr: 0.000010, mv_avg_loss: 11.473957, loss: 10.215315
01:33:09.581191 Step[1000/6995], lr: 0.000010, mv_avg_loss: 11.958751, loss: 9.634706
01:33:14.310001 Step[1100/6995], lr: 0.000010, mv_avg_loss: 11.947664, loss: 15.827800
01:33:19.044927 Step[1200/6995], lr: 0.000010, mv_avg_loss: 12.199730, loss: 13.439422
01:33:23.760055 Step[1300/6995], lr: 0.000010, mv_avg_loss: 13.025344, loss: 25.214520
01:33:28.497832 Step[1400/6995], lr: 0.000010, mv_avg_loss: 12.675817, loss: 9.575895
01:33:33.230445 Step[1500/6995], lr: 0.000010, mv_avg_loss: 12.765675, loss: 13.988052
01:33:37.951031 Step[1600/6995], lr: 0.000010, mv_avg_loss: 12.721030, loss: 12.939960
01:33:42.680583 Step[1700/6995], lr: 0.000010, mv_avg_loss: 12.333010, loss: 13.609289
01:33:47.423289 Step[1800/6995], lr: 0.000010, mv_avg_loss: 12.472551, loss: 16.256643
01:33:52.168415 Step[1900/6995], lr: 0.000010, mv_avg_loss: 12.942028, loss: 10.924629
01:33:56.908844 Step[2000/6995], lr: 0.000010, mv_avg_loss: 12.165926, loss: 8.679964
01:34:01.638856 Step[2100/6995], lr: 0.000010, mv_avg_loss: 12.887753, loss: 19.082100
01:34:06.367999 Step[2200/6995], lr: 0.000010, mv_avg_loss: 11.764022, loss: 12.318339
01:34:11.109178 Step[2300/6995], lr: 0.000010, mv_avg_loss: 12.020564, loss: 10.209911
01:34:15.846514 Step[2400/6995], lr: 0.000010, mv_avg_loss: 11.937968, loss: 7.319889
01:34:20.577819 Step[2500/6995], lr: 0.000010, mv_avg_loss: 13.046378, loss: 13.100080
01:34:25.312423 Step[2600/6995], lr: 0.000010, mv_avg_loss: 12.804709, loss: 12.715064
01:34:30.044362 Step[2700/6995], lr: 0.000010, mv_avg_loss: 12.097811, loss: 13.084021
01:34:34.792593 Step[2800/6995], lr: 0.000010, mv_avg_loss: 12.382499, loss: 15.275124
01:34:39.546925 Step[2900/6995], lr: 0.000010, mv_avg_loss: 11.583974, loss: 8.747896
01:34:44.294092 Step[3000/6995], lr: 0.000010, mv_avg_loss: 12.469991, loss: 13.509600
01:34:49.034988 Step[3100/6995], lr: 0.000010, mv_avg_loss: 12.195378, loss: 18.334475
01:34:53.777341 Step[3200/6995], lr: 0.000010, mv_avg_loss: 11.392529, loss: 9.350605
01:34:58.529622 Step[3300/6995], lr: 0.000010, mv_avg_loss: 12.169334, loss: 31.097649
01:35:03.268630 Step[3400/6995], lr: 0.000010, mv_avg_loss: 11.825387, loss: 19.212065
01:35:08.010824 Step[3500/6995], lr: 0.000010, mv_avg_loss: 12.141019, loss: 8.778762
01:35:12.758638 Step[3600/6995], lr: 0.000010, mv_avg_loss: 11.972058, loss: 14.259148
01:35:17.507319 Step[3700/6995], lr: 0.000010, mv_avg_loss: 12.854122, loss: 11.762444
01:35:22.248798 Step[3800/6995], lr: 0.000010, mv_avg_loss: 11.942566, loss: 9.566898
01:35:26.996307 Step[3900/6995], lr: 0.000010, mv_avg_loss: 12.144506, loss: 13.530500
01:35:31.729869 Step[4000/6995], lr: 0.000010, mv_avg_loss: 11.335403, loss: 8.431780
01:35:36.453628 Step[4100/6995], lr: 0.000010, mv_avg_loss: 11.877543, loss: 8.092811
01:35:41.213856 Step[4200/6995], lr: 0.000010, mv_avg_loss: 12.456004, loss: 8.811872
01:35:45.980124 Step[4300/6995], lr: 0.000010, mv_avg_loss: 12.207386, loss: 9.025066
01:35:50.736198 Step[4400/6995], lr: 0.000010, mv_avg_loss: 12.870682, loss: 11.228703
01:35:55.517233 Step[4500/6995], lr: 0.000010, mv_avg_loss: 12.309506, loss: 7.295238
01:36:00.270822 Step[4600/6995], lr: 0.000010, mv_avg_loss: 12.251113, loss: 9.496088
01:36:05.014614 Step[4700/6995], lr: 0.000010, mv_avg_loss: 12.205231, loss: 7.292768
01:36:09.760608 Step[4800/6995], lr: 0.000010, mv_avg_loss: 11.766084, loss: 10.721533
01:36:14.503636 Step[4900/6995], lr: 0.000010, mv_avg_loss: 11.779920, loss: 9.041264
01:36:19.246091 Step[5000/6995], lr: 0.000010, mv_avg_loss: 11.983797, loss: 12.058235
01:36:24.006145 Step[5100/6995], lr: 0.000010, mv_avg_loss: 10.645723, loss: 6.277948
01:36:28.754045 Step[5200/6995], lr: 0.000010, mv_avg_loss: 12.905140, loss: 11.004040
01:36:33.523446 Step[5300/6995], lr: 0.000010, mv_avg_loss: 11.948739, loss: 9.846380
01:36:38.277514 Step[5400/6995], lr: 0.000010, mv_avg_loss: 11.628878, loss: 14.368117
01:36:43.011417 Step[5500/6995], lr: 0.000010, mv_avg_loss: 12.114476, loss: 8.277750
01:36:47.764720 Step[5600/6995], lr: 0.000010, mv_avg_loss: 12.273810, loss: 13.301643
01:36:52.531685 Step[5700/6995], lr: 0.000010, mv_avg_loss: 12.626070, loss: 13.778522
01:36:57.283369 Step[5800/6995], lr: 0.000010, mv_avg_loss: 12.979210, loss: 9.310581
01:37:02.031320 Step[5900/6995], lr: 0.000010, mv_avg_loss: 13.638181, loss: 16.466423
01:37:06.789235 Step[6000/6995], lr: 0.000010, mv_avg_loss: 11.955828, loss: 10.083126
01:37:11.539987 Step[6100/6995], lr: 0.000010, mv_avg_loss: 12.481661, loss: 9.850827
01:37:16.296294 Step[6200/6995], lr: 0.000010, mv_avg_loss: 11.877309, loss: 10.378765
01:37:21.135506 Step[6300/6995], lr: 0.000010, mv_avg_loss: 12.388873, loss: 21.192646
01:37:25.896420 Step[6400/6995], lr: 0.000010, mv_avg_loss: 12.393760, loss: 10.057911
01:37:30.656457 Step[6500/6995], lr: 0.000010, mv_avg_loss: 12.525871, loss: 9.759832
01:37:35.400242 Step[6600/6995], lr: 0.000010, mv_avg_loss: 12.668324, loss: 14.695750
01:37:40.140114 Step[6700/6995], lr: 0.000010, mv_avg_loss: 11.196919, loss: 9.701800
01:37:44.887985 Step[6800/6995], lr: 0.000010, mv_avg_loss: 12.164640, loss: 16.104746
01:37:49.627840 Step[6900/6995], lr: 0.000010, mv_avg_loss: 11.151910, loss: 7.494245
Testing for epoch: 88
Average test PNSR is 25.328007 for 500 images
Start to train epoch 89
01:38:15.828606 Step[0/6995], lr: 0.000010, mv_avg_loss: 12.600873, loss: 9.122890
01:38:20.562064 Step[100/6995], lr: 0.000010, mv_avg_loss: 12.123039, loss: 11.929771
01:38:25.294034 Step[200/6995], lr: 0.000010, mv_avg_loss: 12.343579, loss: 9.395102
01:38:30.019459 Step[300/6995], lr: 0.000010, mv_avg_loss: 12.520719, loss: 12.202189
01:38:34.755828 Step[400/6995], lr: 0.000010, mv_avg_loss: 12.838698, loss: 17.827656
01:38:39.504863 Step[500/6995], lr: 0.000010, mv_avg_loss: 13.493736, loss: 14.829867
01:38:44.233505 Step[600/6995], lr: 0.000010, mv_avg_loss: 12.282990, loss: 12.414495
01:38:48.972379 Step[700/6995], lr: 0.000010, mv_avg_loss: 12.641310, loss: 9.322302
01:38:53.711540 Step[800/6995], lr: 0.000010, mv_avg_loss: 11.218663, loss: 14.960878
01:38:58.443093 Step[900/6995], lr: 0.000010, mv_avg_loss: 12.457101, loss: 8.607342
01:39:03.173599 Step[1000/6995], lr: 0.000010, mv_avg_loss: 11.761681, loss: 13.192671
01:39:07.899181 Step[1100/6995], lr: 0.000010, mv_avg_loss: 12.974415, loss: 8.298061
01:39:12.638799 Step[1200/6995], lr: 0.000010, mv_avg_loss: 12.703873, loss: 16.878902
01:39:17.374922 Step[1300/6995], lr: 0.000010, mv_avg_loss: 12.531446, loss: 10.947443
01:39:22.111036 Step[1400/6995], lr: 0.000010, mv_avg_loss: 11.967128, loss: 13.633228
01:39:26.826731 Step[1500/6995], lr: 0.000010, mv_avg_loss: 11.541843, loss: 8.875563
01:39:31.523749 Step[1600/6995], lr: 0.000010, mv_avg_loss: 12.757442, loss: 11.322926
01:39:36.258891 Step[1700/6995], lr: 0.000010, mv_avg_loss: 12.237562, loss: 8.499130
01:39:40.993427 Step[1800/6995], lr: 0.000010, mv_avg_loss: 11.969223, loss: 11.221750
01:39:45.715884 Step[1900/6995], lr: 0.000010, mv_avg_loss: 11.611171, loss: 7.298684
01:39:50.448563 Step[2000/6995], lr: 0.000010, mv_avg_loss: 11.530430, loss: 8.117920
01:39:55.179868 Step[2100/6995], lr: 0.000010, mv_avg_loss: 11.980528, loss: 9.723368
01:39:59.905879 Step[2200/6995], lr: 0.000010, mv_avg_loss: 11.822020, loss: 16.045994
01:40:04.624212 Step[2300/6995], lr: 0.000010, mv_avg_loss: 13.126410, loss: 12.293422
01:40:09.340014 Step[2400/6995], lr: 0.000010, mv_avg_loss: 12.201404, loss: 12.373816
01:40:14.086439 Step[2500/6995], lr: 0.000010, mv_avg_loss: 11.694585, loss: 8.387266
01:40:18.803470 Step[2600/6995], lr: 0.000010, mv_avg_loss: 12.766286, loss: 9.249999
01:40:23.534530 Step[2700/6995], lr: 0.000010, mv_avg_loss: 12.503625, loss: 15.497783
01:40:28.254026 Step[2800/6995], lr: 0.000010, mv_avg_loss: 12.694691, loss: 5.617293
01:40:32.987451 Step[2900/6995], lr: 0.000010, mv_avg_loss: 11.965934, loss: 10.423602
01:40:37.713285 Step[3000/6995], lr: 0.000010, mv_avg_loss: 12.261915, loss: 14.176029
01:40:42.446368 Step[3100/6995], lr: 0.000010, mv_avg_loss: 11.355115, loss: 11.587667
01:40:47.176470 Step[3200/6995], lr: 0.000010, mv_avg_loss: 11.700592, loss: 13.373308
01:40:51.909211 Step[3300/6995], lr: 0.000010, mv_avg_loss: 12.193954, loss: 7.016739
01:40:56.640340 Step[3400/6995], lr: 0.000010, mv_avg_loss: 12.596185, loss: 10.999331
01:41:01.374732 Step[3500/6995], lr: 0.000010, mv_avg_loss: 12.050512, loss: 17.887602
01:41:06.100747 Step[3600/6995], lr: 0.000010, mv_avg_loss: 11.598870, loss: 8.076345
01:41:10.838532 Step[3700/6995], lr: 0.000010, mv_avg_loss: 12.019023, loss: 9.133591
01:41:15.564805 Step[3800/6995], lr: 0.000010, mv_avg_loss: 12.052728, loss: 15.594665
01:41:20.294686 Step[3900/6995], lr: 0.000010, mv_avg_loss: 11.525240, loss: 18.552383
01:41:25.018772 Step[4000/6995], lr: 0.000010, mv_avg_loss: 11.939228, loss: 7.024006
01:41:29.744070 Step[4100/6995], lr: 0.000010, mv_avg_loss: 12.277642, loss: 17.197025
01:41:34.461818 Step[4200/6995], lr: 0.000010, mv_avg_loss: 12.663771, loss: 10.244726
01:41:39.205937 Step[4300/6995], lr: 0.000010, mv_avg_loss: 12.469404, loss: 8.905865
01:41:43.934299 Step[4400/6995], lr: 0.000010, mv_avg_loss: 12.992630, loss: 12.971563
01:41:48.657106 Step[4500/6995], lr: 0.000010, mv_avg_loss: 12.448011, loss: 14.754040
01:41:53.371293 Step[4600/6995], lr: 0.000010, mv_avg_loss: 10.813647, loss: 10.080598
01:41:58.099752 Step[4700/6995], lr: 0.000010, mv_avg_loss: 12.281769, loss: 11.140425
01:42:02.818853 Step[4800/6995], lr: 0.000010, mv_avg_loss: 11.658347, loss: 9.398944
01:42:07.538540 Step[4900/6995], lr: 0.000010, mv_avg_loss: 12.956684, loss: 13.006343
01:42:12.262444 Step[5000/6995], lr: 0.000010, mv_avg_loss: 12.920930, loss: 10.584215
01:42:16.981637 Step[5100/6995], lr: 0.000010, mv_avg_loss: 12.797648, loss: 7.638183
01:42:21.710736 Step[5200/6995], lr: 0.000010, mv_avg_loss: 12.597575, loss: 9.284481
01:42:26.441118 Step[5300/6995], lr: 0.000010, mv_avg_loss: 11.757189, loss: 8.998032
01:42:31.168530 Step[5400/6995], lr: 0.000010, mv_avg_loss: 12.801254, loss: 14.142136
01:42:35.898866 Step[5500/6995], lr: 0.000010, mv_avg_loss: 13.391020, loss: 10.065535
01:42:40.611700 Step[5600/6995], lr: 0.000010, mv_avg_loss: 11.571386, loss: 8.211936
01:42:45.332696 Step[5700/6995], lr: 0.000010, mv_avg_loss: 12.200750, loss: 12.678185
01:42:50.057532 Step[5800/6995], lr: 0.000010, mv_avg_loss: 11.556702, loss: 13.083200
01:42:54.780089 Step[5900/6995], lr: 0.000010, mv_avg_loss: 11.373657, loss: 16.897892
01:42:59.499257 Step[6000/6995], lr: 0.000010, mv_avg_loss: 11.830399, loss: 13.046928
01:43:04.237061 Step[6100/6995], lr: 0.000010, mv_avg_loss: 12.709076, loss: 13.433852
01:43:08.960156 Step[6200/6995], lr: 0.000010, mv_avg_loss: 11.670624, loss: 8.352549
01:43:13.697281 Step[6300/6995], lr: 0.000010, mv_avg_loss: 11.918651, loss: 11.412153
01:43:18.411121 Step[6400/6995], lr: 0.000010, mv_avg_loss: 13.211501, loss: 14.855724
01:43:23.115664 Step[6500/6995], lr: 0.000010, mv_avg_loss: 11.641079, loss: 11.715843
01:43:27.842713 Step[6600/6995], lr: 0.000010, mv_avg_loss: 12.088778, loss: 10.177589
01:43:32.561406 Step[6700/6995], lr: 0.000010, mv_avg_loss: 12.072451, loss: 10.021200
01:43:37.272980 Step[6800/6995], lr: 0.000010, mv_avg_loss: 12.644082, loss: 8.228466
01:43:41.967992 Step[6900/6995], lr: 0.000010, mv_avg_loss: 11.531987, loss: 11.467193
saving model for epoch 89
Testing for epoch: 89
Average test PNSR is 25.315696 for 500 images
Start to train epoch 90
01:44:08.085419 Step[0/6995], lr: 0.000010, mv_avg_loss: 12.968362, loss: 8.491961
01:44:12.800339 Step[100/6995], lr: 0.000010, mv_avg_loss: 12.219706, loss: 10.727230
01:44:17.518215 Step[200/6995], lr: 0.000010, mv_avg_loss: 11.957577, loss: 9.255579
01:44:22.228670 Step[300/6995], lr: 0.000010, mv_avg_loss: 12.558484, loss: 9.745399
01:44:26.952113 Step[400/6995], lr: 0.000010, mv_avg_loss: 12.263249, loss: 12.153851
01:44:31.695364 Step[500/6995], lr: 0.000010, mv_avg_loss: 12.451567, loss: 22.010489
01:44:36.419163 Step[600/6995], lr: 0.000010, mv_avg_loss: 11.392741, loss: 11.274496
01:44:41.142828 Step[700/6995], lr: 0.000010, mv_avg_loss: 12.477223, loss: 10.872831
01:44:45.864030 Step[800/6995], lr: 0.000010, mv_avg_loss: 11.556010, loss: 16.908989
01:44:50.584726 Step[900/6995], lr: 0.000010, mv_avg_loss: 12.470810, loss: 13.998470
01:44:55.303177 Step[1000/6995], lr: 0.000010, mv_avg_loss: 12.675367, loss: 6.306347
01:45:00.022562 Step[1100/6995], lr: 0.000010, mv_avg_loss: 11.645771, loss: 13.472952
01:45:04.751103 Step[1200/6995], lr: 0.000010, mv_avg_loss: 11.858771, loss: 9.561271
01:45:09.465389 Step[1300/6995], lr: 0.000010, mv_avg_loss: 12.605406, loss: 10.241757
01:45:14.198291 Step[1400/6995], lr: 0.000010, mv_avg_loss: 12.159664, loss: 15.880221
01:45:18.923262 Step[1500/6995], lr: 0.000010, mv_avg_loss: 13.901598, loss: 16.987511
01:45:23.643851 Step[1600/6995], lr: 0.000010, mv_avg_loss: 11.440100, loss: 9.250693
01:45:28.364127 Step[1700/6995], lr: 0.000010, mv_avg_loss: 11.901771, loss: 13.959890
01:45:33.090345 Step[1800/6995], lr: 0.000010, mv_avg_loss: 11.832137, loss: 8.279734
01:45:37.816075 Step[1900/6995], lr: 0.000010, mv_avg_loss: 10.948147, loss: 13.532415
01:45:42.540383 Step[2000/6995], lr: 0.000010, mv_avg_loss: 12.157690, loss: 24.817696
01:45:47.259370 Step[2100/6995], lr: 0.000010, mv_avg_loss: 12.604973, loss: 8.534327
01:45:51.985187 Step[2200/6995], lr: 0.000010, mv_avg_loss: 12.227007, loss: 9.673917
01:45:56.706252 Step[2300/6995], lr: 0.000010, mv_avg_loss: 12.298865, loss: 7.691572
01:46:01.429993 Step[2400/6995], lr: 0.000010, mv_avg_loss: 11.980963, loss: 13.368467
01:46:06.162790 Step[2500/6995], lr: 0.000010, mv_avg_loss: 12.467256, loss: 13.924009
01:46:10.881799 Step[2600/6995], lr: 0.000010, mv_avg_loss: 12.731922, loss: 15.971642
01:46:15.616293 Step[2700/6995], lr: 0.000010, mv_avg_loss: 11.830597, loss: 7.615777
01:46:20.343641 Step[2800/6995], lr: 0.000010, mv_avg_loss: 12.350674, loss: 10.611099
01:46:25.072781 Step[2900/6995], lr: 0.000010, mv_avg_loss: 12.353447, loss: 13.550323
01:46:29.820856 Step[3000/6995], lr: 0.000010, mv_avg_loss: 12.501735, loss: 14.152767
01:46:34.546832 Step[3100/6995], lr: 0.000010, mv_avg_loss: 12.563956, loss: 16.693624
01:46:39.286382 Step[3200/6995], lr: 0.000010, mv_avg_loss: 11.433201, loss: 9.637571
01:46:44.026601 Step[3300/6995], lr: 0.000010, mv_avg_loss: 11.104349, loss: 8.997844
01:46:48.746293 Step[3400/6995], lr: 0.000010, mv_avg_loss: 12.343878, loss: 12.972911
01:46:53.491206 Step[3500/6995], lr: 0.000010, mv_avg_loss: 11.996289, loss: 12.790076
01:46:58.215621 Step[3600/6995], lr: 0.000010, mv_avg_loss: 12.471585, loss: 14.433894
01:47:02.938835 Step[3700/6995], lr: 0.000010, mv_avg_loss: 12.752084, loss: 9.248311
01:47:07.667279 Step[3800/6995], lr: 0.000010, mv_avg_loss: 12.199023, loss: 15.931358
01:47:12.383130 Step[3900/6995], lr: 0.000010, mv_avg_loss: 11.996325, loss: 8.914822
01:47:17.098077 Step[4000/6995], lr: 0.000010, mv_avg_loss: 12.248054, loss: 11.228666
01:47:21.846285 Step[4100/6995], lr: 0.000010, mv_avg_loss: 11.040462, loss: 10.788437
01:47:26.584372 Step[4200/6995], lr: 0.000010, mv_avg_loss: 11.828947, loss: 11.264923
01:47:31.316566 Step[4300/6995], lr: 0.000010, mv_avg_loss: 13.497874, loss: 18.077360
01:47:36.062719 Step[4400/6995], lr: 0.000010, mv_avg_loss: 12.074411, loss: 13.819452
01:47:40.800181 Step[4500/6995], lr: 0.000010, mv_avg_loss: 12.106175, loss: 15.367236
01:47:45.532097 Step[4600/6995], lr: 0.000010, mv_avg_loss: 13.091007, loss: 14.398928
01:47:50.269711 Step[4700/6995], lr: 0.000010, mv_avg_loss: 12.732763, loss: 6.866832
01:47:55.004870 Step[4800/6995], lr: 0.000010, mv_avg_loss: 12.885455, loss: 15.074856
01:47:59.750562 Step[4900/6995], lr: 0.000010, mv_avg_loss: 12.790110, loss: 9.477465
01:48:04.500767 Step[5000/6995], lr: 0.000010, mv_avg_loss: 13.368936, loss: 8.586092
01:48:09.245404 Step[5100/6995], lr: 0.000010, mv_avg_loss: 11.860881, loss: 12.967337
01:48:14.011735 Step[5200/6995], lr: 0.000010, mv_avg_loss: 11.862322, loss: 8.331768
01:48:18.756588 Step[5300/6995], lr: 0.000010, mv_avg_loss: 12.842854, loss: 11.108229
01:48:23.486838 Step[5400/6995], lr: 0.000010, mv_avg_loss: 11.875900, loss: 7.207116
01:48:28.242355 Step[5500/6995], lr: 0.000010, mv_avg_loss: 11.686547, loss: 13.424346
01:48:32.968520 Step[5600/6995], lr: 0.000010, mv_avg_loss: 13.761344, loss: 13.902264
01:48:37.707533 Step[5700/6995], lr: 0.000010, mv_avg_loss: 13.518389, loss: 8.041255
01:48:42.447465 Step[5800/6995], lr: 0.000010, mv_avg_loss: 12.659766, loss: 8.321637
01:48:47.191256 Step[5900/6995], lr: 0.000010, mv_avg_loss: 11.489090, loss: 6.925426
01:48:51.947813 Step[6000/6995], lr: 0.000010, mv_avg_loss: 12.457844, loss: 14.369205
01:48:56.685322 Step[6100/6995], lr: 0.000010, mv_avg_loss: 11.849033, loss: 8.116198
01:49:01.415940 Step[6200/6995], lr: 0.000010, mv_avg_loss: 13.365870, loss: 15.628349
01:49:06.143900 Step[6300/6995], lr: 0.000010, mv_avg_loss: 11.767593, loss: 10.812785
01:49:10.876575 Step[6400/6995], lr: 0.000010, mv_avg_loss: 12.653539, loss: 13.729819
01:49:15.606441 Step[6500/6995], lr: 0.000010, mv_avg_loss: 11.324699, loss: 14.837976
01:49:20.343461 Step[6600/6995], lr: 0.000010, mv_avg_loss: 12.977015, loss: 15.944038
01:49:25.076165 Step[6700/6995], lr: 0.000010, mv_avg_loss: 11.192012, loss: 8.460342
01:49:29.828252 Step[6800/6995], lr: 0.000010, mv_avg_loss: 12.522022, loss: 11.449072
01:49:34.554272 Step[6900/6995], lr: 0.000010, mv_avg_loss: 12.313020, loss: 14.853226
Testing for epoch: 90
Average test PNSR is 25.324828 for 500 images
Start to train epoch 91
01:50:00.657429 Step[0/6995], lr: 0.000010, mv_avg_loss: 13.320858, loss: 8.153271
01:50:05.401223 Step[100/6995], lr: 0.000010, mv_avg_loss: 12.668275, loss: 12.291573
01:50:10.144259 Step[200/6995], lr: 0.000010, mv_avg_loss: 12.220359, loss: 8.217028
01:50:14.886309 Step[300/6995], lr: 0.000010, mv_avg_loss: 12.039222, loss: 12.973190
01:50:19.629294 Step[400/6995], lr: 0.000010, mv_avg_loss: 10.980892, loss: 11.938389
01:50:24.365222 Step[500/6995], lr: 0.000010, mv_avg_loss: 11.471838, loss: 13.604060
01:50:29.096633 Step[600/6995], lr: 0.000010, mv_avg_loss: 12.226809, loss: 9.684127
01:50:33.834343 Step[700/6995], lr: 0.000010, mv_avg_loss: 12.427998, loss: 12.218364
01:50:38.582472 Step[800/6995], lr: 0.000010, mv_avg_loss: 11.736282, loss: 13.422388
01:50:43.347243 Step[900/6995], lr: 0.000010, mv_avg_loss: 12.752201, loss: 9.502662
01:50:48.085291 Step[1000/6995], lr: 0.000010, mv_avg_loss: 11.672810, loss: 6.621412
01:50:52.822305 Step[1100/6995], lr: 0.000010, mv_avg_loss: 13.168601, loss: 11.110312
01:50:57.570535 Step[1200/6995], lr: 0.000010, mv_avg_loss: 12.515605, loss: 9.196159
01:51:02.325680 Step[1300/6995], lr: 0.000010, mv_avg_loss: 12.579970, loss: 10.782996
01:51:07.075645 Step[1400/6995], lr: 0.000010, mv_avg_loss: 12.449633, loss: 15.004509
01:51:11.827677 Step[1500/6995], lr: 0.000010, mv_avg_loss: 12.242128, loss: 9.358246
01:51:16.572500 Step[1600/6995], lr: 0.000010, mv_avg_loss: 12.465127, loss: 9.361540
01:51:21.321466 Step[1700/6995], lr: 0.000010, mv_avg_loss: 12.668160, loss: 6.517718
01:51:26.067381 Step[1800/6995], lr: 0.000010, mv_avg_loss: 12.806072, loss: 10.110332
01:51:30.813051 Step[1900/6995], lr: 0.000010, mv_avg_loss: 12.825210, loss: 23.069916
01:51:35.562046 Step[2000/6995], lr: 0.000010, mv_avg_loss: 12.906446, loss: 18.914499
01:51:40.310204 Step[2100/6995], lr: 0.000010, mv_avg_loss: 12.856934, loss: 8.464743
01:51:45.051050 Step[2200/6995], lr: 0.000010, mv_avg_loss: 12.239717, loss: 15.844131
01:51:49.795901 Step[2300/6995], lr: 0.000010, mv_avg_loss: 12.348768, loss: 17.281506
01:51:54.539660 Step[2400/6995], lr: 0.000010, mv_avg_loss: 12.479871, loss: 15.883319
01:51:59.283023 Step[2500/6995], lr: 0.000010, mv_avg_loss: 12.663714, loss: 11.565351
01:52:04.029880 Step[2600/6995], lr: 0.000010, mv_avg_loss: 12.202874, loss: 10.353022
01:52:08.774853 Step[2700/6995], lr: 0.000010, mv_avg_loss: 12.162691, loss: 9.276449
01:52:13.529460 Step[2800/6995], lr: 0.000010, mv_avg_loss: 12.263573, loss: 6.623094
01:52:18.266330 Step[2900/6995], lr: 0.000010, mv_avg_loss: 11.992949, loss: 11.572582
01:52:23.017033 Step[3000/6995], lr: 0.000010, mv_avg_loss: 12.793298, loss: 8.862900
01:52:27.761020 Step[3100/6995], lr: 0.000010, mv_avg_loss: 11.966910, loss: 12.497253
01:52:32.511404 Step[3200/6995], lr: 0.000010, mv_avg_loss: 13.073754, loss: 11.423586
01:52:37.247304 Step[3300/6995], lr: 0.000010, mv_avg_loss: 11.929942, loss: 10.011257
01:52:41.987849 Step[3400/6995], lr: 0.000010, mv_avg_loss: 11.748409, loss: 9.060266
01:52:46.747421 Step[3500/6995], lr: 0.000010, mv_avg_loss: 11.741656, loss: 12.587706
01:52:51.481786 Step[3600/6995], lr: 0.000010, mv_avg_loss: 11.743467, loss: 14.473166
01:52:56.233162 Step[3700/6995], lr: 0.000010, mv_avg_loss: 11.616017, loss: 10.937026
01:53:00.980450 Step[3800/6995], lr: 0.000010, mv_avg_loss: 12.550687, loss: 11.666527
01:53:05.717998 Step[3900/6995], lr: 0.000010, mv_avg_loss: 12.183576, loss: 12.457808
01:53:10.457189 Step[4000/6995], lr: 0.000010, mv_avg_loss: 12.544056, loss: 18.488106
01:53:15.207027 Step[4100/6995], lr: 0.000010, mv_avg_loss: 11.331077, loss: 15.097106
01:53:19.945407 Step[4200/6995], lr: 0.000010, mv_avg_loss: 11.565824, loss: 13.057653
01:53:24.690320 Step[4300/6995], lr: 0.000010, mv_avg_loss: 10.957225, loss: 9.392611
01:53:29.437261 Step[4400/6995], lr: 0.000010, mv_avg_loss: 12.517801, loss: 17.035259
01:53:34.167727 Step[4500/6995], lr: 0.000010, mv_avg_loss: 12.251784, loss: 9.894398
01:53:38.907516 Step[4600/6995], lr: 0.000010, mv_avg_loss: 11.368758, loss: 9.162014
01:53:43.666694 Step[4700/6995], lr: 0.000010, mv_avg_loss: 12.467446, loss: 10.863265
01:53:48.406571 Step[4800/6995], lr: 0.000010, mv_avg_loss: 12.096760, loss: 14.561682
01:53:53.143809 Step[4900/6995], lr: 0.000010, mv_avg_loss: 12.174304, loss: 8.819870
01:53:57.879957 Step[5000/6995], lr: 0.000010, mv_avg_loss: 11.618649, loss: 9.237300
01:54:02.626204 Step[5100/6995], lr: 0.000010, mv_avg_loss: 11.375139, loss: 11.301739
01:54:07.361546 Step[5200/6995], lr: 0.000010, mv_avg_loss: 12.697053, loss: 13.643288
01:54:12.098316 Step[5300/6995], lr: 0.000010, mv_avg_loss: 11.689911, loss: 10.037529
01:54:16.845261 Step[5400/6995], lr: 0.000010, mv_avg_loss: 12.760976, loss: 13.810252
01:54:21.586116 Step[5500/6995], lr: 0.000010, mv_avg_loss: 12.546592, loss: 14.819480
01:54:26.312946 Step[5600/6995], lr: 0.000010, mv_avg_loss: 11.640683, loss: 11.616576
01:54:31.045040 Step[5700/6995], lr: 0.000010, mv_avg_loss: 10.869215, loss: 9.212929
01:54:35.773754 Step[5800/6995], lr: 0.000010, mv_avg_loss: 12.246204, loss: 17.096680
01:54:40.499434 Step[5900/6995], lr: 0.000010, mv_avg_loss: 11.637560, loss: 12.333559
01:54:45.226332 Step[6000/6995], lr: 0.000010, mv_avg_loss: 11.147917, loss: 7.435756
01:54:49.967332 Step[6100/6995], lr: 0.000010, mv_avg_loss: 13.289636, loss: 11.497900
01:54:54.689372 Step[6200/6995], lr: 0.000010, mv_avg_loss: 11.065192, loss: 8.876829
01:54:59.424113 Step[6300/6995], lr: 0.000010, mv_avg_loss: 11.878263, loss: 6.889281
01:55:04.149816 Step[6400/6995], lr: 0.000010, mv_avg_loss: 12.531173, loss: 21.493277
01:55:08.878476 Step[6500/6995], lr: 0.000010, mv_avg_loss: 11.805174, loss: 8.139770
01:55:13.623661 Step[6600/6995], lr: 0.000010, mv_avg_loss: 12.554766, loss: 8.589392
01:55:18.358928 Step[6700/6995], lr: 0.000010, mv_avg_loss: 12.342139, loss: 13.368106
01:55:23.084646 Step[6800/6995], lr: 0.000010, mv_avg_loss: 12.798706, loss: 14.474821
01:55:27.820980 Step[6900/6995], lr: 0.000010, mv_avg_loss: 13.239046, loss: 10.886459
Testing for epoch: 91
Average test PNSR is 25.324660 for 500 images
Start to train epoch 92
01:55:54.001321 Step[0/6995], lr: 0.000010, mv_avg_loss: 11.072642, loss: 8.057772
01:55:58.714304 Step[100/6995], lr: 0.000010, mv_avg_loss: 12.210445, loss: 10.940097
01:56:03.434466 Step[200/6995], lr: 0.000010, mv_avg_loss: 12.717111, loss: 9.836188
01:56:08.165187 Step[300/6995], lr: 0.000010, mv_avg_loss: 11.798742, loss: 11.992747
01:56:12.893537 Step[400/6995], lr: 0.000010, mv_avg_loss: 11.851529, loss: 12.885959
01:56:17.606837 Step[500/6995], lr: 0.000010, mv_avg_loss: 12.459257, loss: 10.197387
01:56:22.323357 Step[600/6995], lr: 0.000010, mv_avg_loss: 12.578070, loss: 12.411261
01:56:27.026083 Step[700/6995], lr: 0.000010, mv_avg_loss: 12.916292, loss: 11.704524
01:56:31.742570 Step[800/6995], lr: 0.000010, mv_avg_loss: 12.053670, loss: 10.434694
01:56:36.468694 Step[900/6995], lr: 0.000010, mv_avg_loss: 12.798550, loss: 7.529974
01:56:41.184718 Step[1000/6995], lr: 0.000010, mv_avg_loss: 12.156193, loss: 12.075076
01:56:45.900831 Step[1100/6995], lr: 0.000010, mv_avg_loss: 13.094454, loss: 11.310527
01:56:50.611396 Step[1200/6995], lr: 0.000010, mv_avg_loss: 11.767079, loss: 7.626386
01:56:55.321416 Step[1300/6995], lr: 0.000010, mv_avg_loss: 12.105238, loss: 9.369921
01:57:00.032578 Step[1400/6995], lr: 0.000010, mv_avg_loss: 11.952506, loss: 7.054806
01:57:04.779177 Step[1500/6995], lr: 0.000010, mv_avg_loss: 12.715480, loss: 10.771009
01:57:09.504803 Step[1600/6995], lr: 0.000010, mv_avg_loss: 12.537119, loss: 11.648775
01:57:14.221497 Step[1700/6995], lr: 0.000010, mv_avg_loss: 11.934524, loss: 10.133701
01:57:18.953395 Step[1800/6995], lr: 0.000010, mv_avg_loss: 12.099991, loss: 10.071724
01:57:23.678663 Step[1900/6995], lr: 0.000010, mv_avg_loss: 12.296116, loss: 14.021957
01:57:28.399041 Step[2000/6995], lr: 0.000010, mv_avg_loss: 12.411893, loss: 7.610630
01:57:33.129081 Step[2100/6995], lr: 0.000010, mv_avg_loss: 11.612141, loss: 8.396769
01:57:37.850903 Step[2200/6995], lr: 0.000010, mv_avg_loss: 11.367106, loss: 14.862677
01:57:42.572009 Step[2300/6995], lr: 0.000010, mv_avg_loss: 13.447164, loss: 8.663204
01:57:47.289860 Step[2400/6995], lr: 0.000010, mv_avg_loss: 11.665929, loss: 9.469150
01:57:52.008008 Step[2500/6995], lr: 0.000010, mv_avg_loss: 11.702712, loss: 10.925047
01:57:56.723219 Step[2600/6995], lr: 0.000010, mv_avg_loss: 11.544927, loss: 10.704353
01:58:01.438343 Step[2700/6995], lr: 0.000010, mv_avg_loss: 12.399316, loss: 9.939553
01:58:06.165083 Step[2800/6995], lr: 0.000010, mv_avg_loss: 12.529477, loss: 15.434042
01:58:10.881272 Step[2900/6995], lr: 0.000010, mv_avg_loss: 12.112009, loss: 13.260847
01:58:15.598993 Step[3000/6995], lr: 0.000010, mv_avg_loss: 12.091105, loss: 13.902945
01:58:20.324826 Step[3100/6995], lr: 0.000010, mv_avg_loss: 11.963449, loss: 10.983928
01:58:25.052308 Step[3200/6995], lr: 0.000010, mv_avg_loss: 11.784517, loss: 14.691318
01:58:29.779666 Step[3300/6995], lr: 0.000010, mv_avg_loss: 12.578864, loss: 13.373724
01:58:34.504064 Step[3400/6995], lr: 0.000010, mv_avg_loss: 11.985941, loss: 9.817520
01:58:39.221989 Step[3500/6995], lr: 0.000010, mv_avg_loss: 12.146564, loss: 14.861346
01:58:43.940231 Step[3600/6995], lr: 0.000010, mv_avg_loss: 12.154442, loss: 10.914515
01:58:48.656434 Step[3700/6995], lr: 0.000010, mv_avg_loss: 13.569516, loss: 21.893408
01:58:53.365588 Step[3800/6995], lr: 0.000010, mv_avg_loss: 11.429296, loss: 12.201590
01:58:58.085869 Step[3900/6995], lr: 0.000010, mv_avg_loss: 12.530347, loss: 18.430105
01:59:02.787387 Step[4000/6995], lr: 0.000010, mv_avg_loss: 11.724762, loss: 6.444950
01:59:07.510660 Step[4100/6995], lr: 0.000010, mv_avg_loss: 11.734332, loss: 10.542660
01:59:12.220133 Step[4200/6995], lr: 0.000010, mv_avg_loss: 11.706950, loss: 11.289871
01:59:16.936571 Step[4300/6995], lr: 0.000010, mv_avg_loss: 11.854698, loss: 11.988459
01:59:21.659984 Step[4400/6995], lr: 0.000010, mv_avg_loss: 12.181952, loss: 8.340307
01:59:26.399575 Step[4500/6995], lr: 0.000010, mv_avg_loss: 11.947274, loss: 11.203242
01:59:31.135874 Step[4600/6995], lr: 0.000010, mv_avg_loss: 12.056966, loss: 16.596144
01:59:35.864455 Step[4700/6995], lr: 0.000010, mv_avg_loss: 13.210181, loss: 16.077078
01:59:40.585562 Step[4800/6995], lr: 0.000010, mv_avg_loss: 12.716496, loss: 13.546185
01:59:45.314078 Step[4900/6995], lr: 0.000010, mv_avg_loss: 11.596162, loss: 13.629512
01:59:50.031194 Step[5000/6995], lr: 0.000010, mv_avg_loss: 11.177012, loss: 6.337505
01:59:54.751755 Step[5100/6995], lr: 0.000010, mv_avg_loss: 13.195173, loss: 11.703498
01:59:59.495430 Step[5200/6995], lr: 0.000010, mv_avg_loss: 12.332845, loss: 9.337187
02:00:04.232992 Step[5300/6995], lr: 0.000010, mv_avg_loss: 12.317036, loss: 9.476239
02:00:08.967876 Step[5400/6995], lr: 0.000010, mv_avg_loss: 11.918201, loss: 15.090025
02:00:13.700810 Step[5500/6995], lr: 0.000010, mv_avg_loss: 11.863313, loss: 13.106387
02:00:18.431589 Step[5600/6995], lr: 0.000010, mv_avg_loss: 12.449443, loss: 10.642075
02:00:23.168945 Step[5700/6995], lr: 0.000010, mv_avg_loss: 12.236174, loss: 12.084259
02:00:27.902006 Step[5800/6995], lr: 0.000010, mv_avg_loss: 11.771167, loss: 12.410263
02:00:32.635453 Step[5900/6995], lr: 0.000010, mv_avg_loss: 12.751767, loss: 10.318949
02:00:37.362667 Step[6000/6995], lr: 0.000010, mv_avg_loss: 13.427400, loss: 10.297813
02:00:42.097470 Step[6100/6995], lr: 0.000010, mv_avg_loss: 13.292076, loss: 13.400844
02:00:46.823189 Step[6200/6995], lr: 0.000010, mv_avg_loss: 12.399799, loss: 14.557722
02:00:51.547849 Step[6300/6995], lr: 0.000010, mv_avg_loss: 12.202558, loss: 11.618594
02:00:56.283732 Step[6400/6995], lr: 0.000010, mv_avg_loss: 12.283781, loss: 18.045240
02:01:01.034106 Step[6500/6995], lr: 0.000010, mv_avg_loss: 11.472958, loss: 10.219646
02:01:05.757092 Step[6600/6995], lr: 0.000010, mv_avg_loss: 12.764427, loss: 8.108075
02:01:10.490381 Step[6700/6995], lr: 0.000010, mv_avg_loss: 11.800869, loss: 11.548651
02:01:15.227712 Step[6800/6995], lr: 0.000010, mv_avg_loss: 12.230103, loss: 7.910476
02:01:19.962009 Step[6900/6995], lr: 0.000010, mv_avg_loss: 12.706160, loss: 9.520757
Testing for epoch: 92
Average test PNSR is 25.314360 for 500 images
Start to train epoch 93
02:01:46.379167 Step[0/6995], lr: 0.000010, mv_avg_loss: 11.069109, loss: 6.689438
02:01:51.117743 Step[100/6995], lr: 0.000010, mv_avg_loss: 11.765598, loss: 14.696440
02:01:55.850533 Step[200/6995], lr: 0.000010, mv_avg_loss: 11.846833, loss: 12.174578
02:02:00.593922 Step[300/6995], lr: 0.000010, mv_avg_loss: 11.154129, loss: 8.570445
02:02:05.322581 Step[400/6995], lr: 0.000010, mv_avg_loss: 12.555560, loss: 10.324121
02:02:10.060749 Step[500/6995], lr: 0.000010, mv_avg_loss: 12.567544, loss: 14.462141
02:02:14.802417 Step[600/6995], lr: 0.000010, mv_avg_loss: 12.310147, loss: 12.455437
02:02:19.535991 Step[700/6995], lr: 0.000010, mv_avg_loss: 12.818932, loss: 10.402006
02:02:24.275627 Step[800/6995], lr: 0.000010, mv_avg_loss: 12.324234, loss: 18.336061
02:02:29.007040 Step[900/6995], lr: 0.000010, mv_avg_loss: 11.815963, loss: 11.290400
02:02:33.743623 Step[1000/6995], lr: 0.000010, mv_avg_loss: 12.446464, loss: 20.883873
02:02:38.475486 Step[1100/6995], lr: 0.000010, mv_avg_loss: 13.143077, loss: 18.898716
02:02:43.220265 Step[1200/6995], lr: 0.000010, mv_avg_loss: 12.589653, loss: 15.597456
02:02:47.954244 Step[1300/6995], lr: 0.000010, mv_avg_loss: 11.846716, loss: 7.871246
02:02:52.685300 Step[1400/6995], lr: 0.000010, mv_avg_loss: 12.112933, loss: 9.127615
02:02:57.429779 Step[1500/6995], lr: 0.000010, mv_avg_loss: 12.232064, loss: 6.740960
02:03:02.161175 Step[1600/6995], lr: 0.000010, mv_avg_loss: 12.629713, loss: 18.680767
02:03:06.899428 Step[1700/6995], lr: 0.000010, mv_avg_loss: 12.781775, loss: 10.298781
02:03:11.641053 Step[1800/6995], lr: 0.000010, mv_avg_loss: 12.323229, loss: 11.268577
02:03:16.388248 Step[1900/6995], lr: 0.000010, mv_avg_loss: 12.327040, loss: 9.472436
02:03:21.103369 Step[2000/6995], lr: 0.000010, mv_avg_loss: 12.665732, loss: 9.804790
02:03:25.840537 Step[2100/6995], lr: 0.000010, mv_avg_loss: 12.589384, loss: 13.247965
02:03:30.588148 Step[2200/6995], lr: 0.000010, mv_avg_loss: 12.155013, loss: 8.364061
02:03:35.319887 Step[2300/6995], lr: 0.000010, mv_avg_loss: 12.472400, loss: 8.101182
02:03:40.064279 Step[2400/6995], lr: 0.000010, mv_avg_loss: 12.891817, loss: 12.637023
02:03:44.801641 Step[2500/6995], lr: 0.000010, mv_avg_loss: 12.120150, loss: 9.004653
02:03:49.548323 Step[2600/6995], lr: 0.000010, mv_avg_loss: 12.375090, loss: 10.522844
02:03:54.303147 Step[2700/6995], lr: 0.000010, mv_avg_loss: 12.107330, loss: 10.643921
02:03:59.042934 Step[2800/6995], lr: 0.000010, mv_avg_loss: 11.991475, loss: 13.133670
02:04:03.791898 Step[2900/6995], lr: 0.000010, mv_avg_loss: 12.412633, loss: 10.296564
02:04:08.517551 Step[3000/6995], lr: 0.000010, mv_avg_loss: 12.471292, loss: 13.719109
02:04:13.236615 Step[3100/6995], lr: 0.000010, mv_avg_loss: 12.187029, loss: 14.242880
02:04:17.971650 Step[3200/6995], lr: 0.000010, mv_avg_loss: 11.975412, loss: 13.017387
02:04:22.711444 Step[3300/6995], lr: 0.000010, mv_avg_loss: 11.706016, loss: 8.642434
02:04:27.457382 Step[3400/6995], lr: 0.000010, mv_avg_loss: 12.554761, loss: 11.150885
02:04:32.205507 Step[3500/6995], lr: 0.000010, mv_avg_loss: 11.715458, loss: 11.296116
02:04:36.952053 Step[3600/6995], lr: 0.000010, mv_avg_loss: 12.355223, loss: 19.912254
02:04:41.700547 Step[3700/6995], lr: 0.000010, mv_avg_loss: 11.570295, loss: 12.407058
02:04:46.431538 Step[3800/6995], lr: 0.000010, mv_avg_loss: 12.306314, loss: 13.759787
02:04:51.165133 Step[3900/6995], lr: 0.000010, mv_avg_loss: 11.240664, loss: 9.980816
02:04:55.918695 Step[4000/6995], lr: 0.000010, mv_avg_loss: 11.840062, loss: 7.867206
02:05:00.678335 Step[4100/6995], lr: 0.000010, mv_avg_loss: 12.863395, loss: 13.348862
02:05:05.413413 Step[4200/6995], lr: 0.000010, mv_avg_loss: 12.212996, loss: 15.160559
02:05:10.167164 Step[4300/6995], lr: 0.000010, mv_avg_loss: 11.382203, loss: 7.942432
02:05:14.925749 Step[4400/6995], lr: 0.000010, mv_avg_loss: 12.856991, loss: 10.616127
02:05:19.689761 Step[4500/6995], lr: 0.000010, mv_avg_loss: 12.227563, loss: 10.230433
02:05:24.428916 Step[4600/6995], lr: 0.000010, mv_avg_loss: 12.162320, loss: 12.102858
02:05:29.170869 Step[4700/6995], lr: 0.000010, mv_avg_loss: 11.713717, loss: 9.528524
02:05:33.933488 Step[4800/6995], lr: 0.000010, mv_avg_loss: 13.351372, loss: 14.939129
02:05:38.684415 Step[4900/6995], lr: 0.000010, mv_avg_loss: 12.341290, loss: 12.214844
02:05:43.427875 Step[5000/6995], lr: 0.000010, mv_avg_loss: 12.553813, loss: 9.900698
02:05:48.173928 Step[5100/6995], lr: 0.000010, mv_avg_loss: 11.710264, loss: 9.048782
02:05:52.910930 Step[5200/6995], lr: 0.000010, mv_avg_loss: 12.771460, loss: 9.041798
02:05:57.650922 Step[5300/6995], lr: 0.000010, mv_avg_loss: 12.479840, loss: 9.893923
02:06:02.380938 Step[5400/6995], lr: 0.000010, mv_avg_loss: 11.223795, loss: 24.922277
02:06:07.117123 Step[5500/6995], lr: 0.000010, mv_avg_loss: 12.798036, loss: 9.984579
02:06:11.856783 Step[5600/6995], lr: 0.000010, mv_avg_loss: 12.056135, loss: 8.872059
02:06:16.613578 Step[5700/6995], lr: 0.000010, mv_avg_loss: 13.032384, loss: 16.196856
02:06:21.351771 Step[5800/6995], lr: 0.000010, mv_avg_loss: 12.275252, loss: 9.565021
02:06:26.106555 Step[5900/6995], lr: 0.000010, mv_avg_loss: 13.175198, loss: 12.038627
02:06:30.865393 Step[6000/6995], lr: 0.000010, mv_avg_loss: 12.821548, loss: 12.421985
02:06:35.614683 Step[6100/6995], lr: 0.000010, mv_avg_loss: 12.206017, loss: 13.047451
02:06:40.367275 Step[6200/6995], lr: 0.000010, mv_avg_loss: 13.321690, loss: 11.517092
02:06:45.124312 Step[6300/6995], lr: 0.000010, mv_avg_loss: 12.876007, loss: 10.026697
02:06:49.862804 Step[6400/6995], lr: 0.000010, mv_avg_loss: 13.083903, loss: 12.001890
02:06:54.607670 Step[6500/6995], lr: 0.000010, mv_avg_loss: 12.618343, loss: 7.541857
02:06:59.348342 Step[6600/6995], lr: 0.000010, mv_avg_loss: 12.350290, loss: 13.981627
02:07:04.089980 Step[6700/6995], lr: 0.000010, mv_avg_loss: 11.393827, loss: 9.128881
02:07:08.837071 Step[6800/6995], lr: 0.000010, mv_avg_loss: 12.595320, loss: 9.680714
02:07:13.583654 Step[6900/6995], lr: 0.000010, mv_avg_loss: 12.348468, loss: 14.033870
Testing for epoch: 93
Average test PNSR is 25.311458 for 500 images
Start to train epoch 94
02:07:40.246117 Step[0/6995], lr: 0.000010, mv_avg_loss: 11.366712, loss: 11.960516
02:07:44.981846 Step[100/6995], lr: 0.000010, mv_avg_loss: 13.053137, loss: 11.133133
02:07:49.710590 Step[200/6995], lr: 0.000010, mv_avg_loss: 11.793978, loss: 19.812746
02:07:54.432329 Step[300/6995], lr: 0.000010, mv_avg_loss: 12.015904, loss: 13.954748
02:07:59.159720 Step[400/6995], lr: 0.000010, mv_avg_loss: 12.635328, loss: 11.537958
02:08:03.888322 Step[500/6995], lr: 0.000010, mv_avg_loss: 12.661696, loss: 9.562892
02:08:08.624436 Step[600/6995], lr: 0.000010, mv_avg_loss: 12.680786, loss: 10.407773
02:08:13.368989 Step[700/6995], lr: 0.000010, mv_avg_loss: 12.384518, loss: 15.951471
02:08:18.111219 Step[800/6995], lr: 0.000010, mv_avg_loss: 12.452556, loss: 7.972360
02:08:22.853371 Step[900/6995], lr: 0.000010, mv_avg_loss: 12.135194, loss: 16.085747
02:08:27.583830 Step[1000/6995], lr: 0.000010, mv_avg_loss: 11.328315, loss: 9.200046
02:08:32.316333 Step[1100/6995], lr: 0.000010, mv_avg_loss: 12.302124, loss: 8.966971
02:08:37.052772 Step[1200/6995], lr: 0.000010, mv_avg_loss: 11.927453, loss: 8.316851
02:08:41.773837 Step[1300/6995], lr: 0.000010, mv_avg_loss: 12.227770, loss: 8.680560
02:08:46.505400 Step[1400/6995], lr: 0.000010, mv_avg_loss: 12.511598, loss: 17.394709
02:08:51.239191 Step[1500/6995], lr: 0.000010, mv_avg_loss: 12.916879, loss: 10.029741
02:08:55.968646 Step[1600/6995], lr: 0.000010, mv_avg_loss: 12.522017, loss: 9.423083
02:09:00.704879 Step[1700/6995], lr: 0.000010, mv_avg_loss: 11.802255, loss: 17.142605
02:09:05.437702 Step[1800/6995], lr: 0.000010, mv_avg_loss: 12.523531, loss: 12.383688
02:09:10.180284 Step[1900/6995], lr: 0.000010, mv_avg_loss: 12.024371, loss: 10.421774
02:09:14.913167 Step[2000/6995], lr: 0.000010, mv_avg_loss: 12.189947, loss: 10.959915
02:09:19.645755 Step[2100/6995], lr: 0.000010, mv_avg_loss: 12.498261, loss: 8.022365
02:09:24.390496 Step[2200/6995], lr: 0.000010, mv_avg_loss: 12.480988, loss: 15.322180
02:09:29.124336 Step[2300/6995], lr: 0.000010, mv_avg_loss: 11.837097, loss: 12.437102
02:09:33.858228 Step[2400/6995], lr: 0.000010, mv_avg_loss: 12.421762, loss: 9.407704
02:09:38.629323 Step[2500/6995], lr: 0.000010, mv_avg_loss: 11.998857, loss: 11.153984
02:09:43.381141 Step[2600/6995], lr: 0.000010, mv_avg_loss: 11.728005, loss: 12.586696
02:09:48.115947 Step[2700/6995], lr: 0.000010, mv_avg_loss: 12.470295, loss: 14.881369
02:09:52.845007 Step[2800/6995], lr: 0.000010, mv_avg_loss: 11.822248, loss: 10.428516
02:09:57.583105 Step[2900/6995], lr: 0.000010, mv_avg_loss: 12.193379, loss: 10.735225
02:10:02.314798 Step[3000/6995], lr: 0.000010, mv_avg_loss: 11.880189, loss: 11.586627
02:10:07.049028 Step[3100/6995], lr: 0.000010, mv_avg_loss: 12.381757, loss: 10.972965
02:10:11.779171 Step[3200/6995], lr: 0.000010, mv_avg_loss: 12.196964, loss: 15.772716
02:10:16.509911 Step[3300/6995], lr: 0.000010, mv_avg_loss: 11.706635, loss: 13.372421
02:10:21.256859 Step[3400/6995], lr: 0.000010, mv_avg_loss: 11.859970, loss: 8.398742
02:10:26.066283 Step[3500/6995], lr: 0.000010, mv_avg_loss: 12.089722, loss: 10.398405
02:10:30.804219 Step[3600/6995], lr: 0.000010, mv_avg_loss: 12.032228, loss: 10.433632
02:10:35.542204 Step[3700/6995], lr: 0.000010, mv_avg_loss: 11.895286, loss: 9.887355
02:10:40.277197 Step[3800/6995], lr: 0.000010, mv_avg_loss: 12.696167, loss: 9.319797
02:10:45.020098 Step[3900/6995], lr: 0.000010, mv_avg_loss: 12.322268, loss: 11.380188
02:10:49.765714 Step[4000/6995], lr: 0.000010, mv_avg_loss: 12.150438, loss: 18.891741
02:10:54.516286 Step[4100/6995], lr: 0.000010, mv_avg_loss: 12.930225, loss: 12.625269
02:10:59.246803 Step[4200/6995], lr: 0.000010, mv_avg_loss: 11.448756, loss: 15.293906
02:11:03.978736 Step[4300/6995], lr: 0.000010, mv_avg_loss: 10.910082, loss: 9.385786
02:11:08.715598 Step[4400/6995], lr: 0.000010, mv_avg_loss: 12.838554, loss: 14.925966
02:11:13.453044 Step[4500/6995], lr: 0.000010, mv_avg_loss: 12.230872, loss: 18.156727
02:11:18.190988 Step[4600/6995], lr: 0.000010, mv_avg_loss: 12.417561, loss: 10.776983
02:11:22.928854 Step[4700/6995], lr: 0.000010, mv_avg_loss: 12.785314, loss: 10.856029
02:11:27.673133 Step[4800/6995], lr: 0.000010, mv_avg_loss: 12.401364, loss: 6.806024
02:11:32.391096 Step[4900/6995], lr: 0.000010, mv_avg_loss: 11.882960, loss: 10.181179
02:11:37.121337 Step[5000/6995], lr: 0.000010, mv_avg_loss: 11.497975, loss: 10.150446
02:11:41.855776 Step[5100/6995], lr: 0.000010, mv_avg_loss: 11.282601, loss: 14.074642
02:11:46.586895 Step[5200/6995], lr: 0.000010, mv_avg_loss: 12.187313, loss: 8.194012
02:11:51.314616 Step[5300/6995], lr: 0.000010, mv_avg_loss: 12.175775, loss: 9.278004
02:11:56.059307 Step[5400/6995], lr: 0.000010, mv_avg_loss: 11.731568, loss: 18.578562
02:12:00.790495 Step[5500/6995], lr: 0.000010, mv_avg_loss: 12.214761, loss: 10.933525
02:12:05.494617 Step[5600/6995], lr: 0.000010, mv_avg_loss: 12.907646, loss: 9.701462
02:12:10.238994 Step[5700/6995], lr: 0.000010, mv_avg_loss: 11.780427, loss: 26.155645
02:12:14.970302 Step[5800/6995], lr: 0.000010, mv_avg_loss: 12.327602, loss: 12.868323
02:12:19.698288 Step[5900/6995], lr: 0.000010, mv_avg_loss: 12.095565, loss: 8.509693
02:12:24.429886 Step[6000/6995], lr: 0.000010, mv_avg_loss: 12.868396, loss: 7.280313
02:12:29.161164 Step[6100/6995], lr: 0.000010, mv_avg_loss: 12.632534, loss: 6.888651
02:12:33.882442 Step[6200/6995], lr: 0.000010, mv_avg_loss: 12.572039, loss: 15.829700
02:12:38.608744 Step[6300/6995], lr: 0.000010, mv_avg_loss: 12.179962, loss: 14.236606
02:12:43.341852 Step[6400/6995], lr: 0.000010, mv_avg_loss: 11.976751, loss: 6.849246
02:12:48.076845 Step[6500/6995], lr: 0.000010, mv_avg_loss: 11.907111, loss: 12.325409
02:12:52.807664 Step[6600/6995], lr: 0.000010, mv_avg_loss: 12.297414, loss: 11.915747
02:12:57.537589 Step[6700/6995], lr: 0.000010, mv_avg_loss: 12.611005, loss: 16.720985
02:13:02.264875 Step[6800/6995], lr: 0.000010, mv_avg_loss: 11.732664, loss: 8.168024
02:13:06.992493 Step[6900/6995], lr: 0.000010, mv_avg_loss: 11.657896, loss: 9.714262
saving model for epoch 94
Testing for epoch: 94
Average test PNSR is 25.320723 for 500 images
Start to train epoch 95
02:13:33.046911 Step[0/6995], lr: 0.000010, mv_avg_loss: 12.237352, loss: 14.293739
02:13:37.782036 Step[100/6995], lr: 0.000010, mv_avg_loss: 12.129083, loss: 7.738936
02:13:42.511882 Step[200/6995], lr: 0.000010, mv_avg_loss: 11.451215, loss: 11.076925
02:13:47.236573 Step[300/6995], lr: 0.000010, mv_avg_loss: 10.659249, loss: 7.353771
02:13:51.966832 Step[400/6995], lr: 0.000010, mv_avg_loss: 11.733064, loss: 11.718744
02:13:56.718513 Step[500/6995], lr: 0.000010, mv_avg_loss: 12.689065, loss: 14.661499
02:14:01.444373 Step[600/6995], lr: 0.000010, mv_avg_loss: 11.780296, loss: 8.319606
02:14:06.174152 Step[700/6995], lr: 0.000010, mv_avg_loss: 12.198663, loss: 10.868450
02:14:10.913797 Step[800/6995], lr: 0.000010, mv_avg_loss: 12.763892, loss: 12.009978
02:14:15.645995 Step[900/6995], lr: 0.000010, mv_avg_loss: 13.203890, loss: 10.933280
02:14:20.375444 Step[1000/6995], lr: 0.000010, mv_avg_loss: 11.770842, loss: 9.801701
02:14:25.107733 Step[1100/6995], lr: 0.000010, mv_avg_loss: 11.616735, loss: 10.598381
02:14:29.841616 Step[1200/6995], lr: 0.000010, mv_avg_loss: 11.910768, loss: 27.667997
02:14:34.576283 Step[1300/6995], lr: 0.000010, mv_avg_loss: 12.695798, loss: 9.424703
02:14:39.315816 Step[1400/6995], lr: 0.000010, mv_avg_loss: 11.888270, loss: 9.492107
02:14:44.052554 Step[1500/6995], lr: 0.000010, mv_avg_loss: 12.081357, loss: 14.303966
02:14:48.784700 Step[1600/6995], lr: 0.000010, mv_avg_loss: 12.364146, loss: 7.306659
02:14:53.534295 Step[1700/6995], lr: 0.000010, mv_avg_loss: 11.916801, loss: 9.536676
02:14:58.267762 Step[1800/6995], lr: 0.000010, mv_avg_loss: 11.967656, loss: 12.239029
02:15:03.010094 Step[1900/6995], lr: 0.000010, mv_avg_loss: 12.589589, loss: 20.198471
02:15:07.757000 Step[2000/6995], lr: 0.000010, mv_avg_loss: 12.198077, loss: 10.798525
02:15:12.502665 Step[2100/6995], lr: 0.000010, mv_avg_loss: 12.800426, loss: 13.251018
02:15:17.233754 Step[2200/6995], lr: 0.000010, mv_avg_loss: 12.269601, loss: 17.528061
02:15:21.981654 Step[2300/6995], lr: 0.000010, mv_avg_loss: 11.976152, loss: 8.442966
02:15:26.718197 Step[2400/6995], lr: 0.000010, mv_avg_loss: 11.965220, loss: 14.085594
02:15:31.462448 Step[2500/6995], lr: 0.000010, mv_avg_loss: 11.743919, loss: 10.013011
02:15:36.203498 Step[2600/6995], lr: 0.000010, mv_avg_loss: 12.619413, loss: 10.815928
02:15:40.942598 Step[2700/6995], lr: 0.000010, mv_avg_loss: 12.344173, loss: 23.742352
02:15:45.685507 Step[2800/6995], lr: 0.000010, mv_avg_loss: 12.045913, loss: 10.942062
02:15:50.424627 Step[2900/6995], lr: 0.000010, mv_avg_loss: 12.189778, loss: 7.605493
02:15:55.166564 Step[3000/6995], lr: 0.000010, mv_avg_loss: 11.930062, loss: 8.468066
02:15:59.899073 Step[3100/6995], lr: 0.000010, mv_avg_loss: 12.508604, loss: 17.068563
02:16:04.653090 Step[3200/6995], lr: 0.000010, mv_avg_loss: 12.492973, loss: 13.768977
02:16:09.403253 Step[3300/6995], lr: 0.000010, mv_avg_loss: 12.291750, loss: 6.489806
02:16:14.160658 Step[3400/6995], lr: 0.000010, mv_avg_loss: 12.009718, loss: 10.825354
02:16:18.917125 Step[3500/6995], lr: 0.000010, mv_avg_loss: 12.578059, loss: 16.514858
02:16:23.667821 Step[3600/6995], lr: 0.000010, mv_avg_loss: 12.316764, loss: 10.789738
02:16:28.410263 Step[3700/6995], lr: 0.000010, mv_avg_loss: 11.848228, loss: 7.432249
02:16:33.155873 Step[3800/6995], lr: 0.000010, mv_avg_loss: 12.705423, loss: 15.207210
02:16:37.897534 Step[3900/6995], lr: 0.000010, mv_avg_loss: 13.186357, loss: 8.649126
02:16:42.633315 Step[4000/6995], lr: 0.000010, mv_avg_loss: 12.805133, loss: 11.465091
02:16:47.381051 Step[4100/6995], lr: 0.000010, mv_avg_loss: 11.761366, loss: 13.748854
02:16:52.131279 Step[4200/6995], lr: 0.000010, mv_avg_loss: 11.704555, loss: 17.065786
02:16:56.858673 Step[4300/6995], lr: 0.000010, mv_avg_loss: 11.843596, loss: 22.683727
02:17:01.608556 Step[4400/6995], lr: 0.000010, mv_avg_loss: 12.479926, loss: 12.803386
02:17:06.354605 Step[4500/6995], lr: 0.000010, mv_avg_loss: 11.958542, loss: 11.540546
02:17:11.112326 Step[4600/6995], lr: 0.000010, mv_avg_loss: 12.490632, loss: 7.626866
02:17:15.862358 Step[4700/6995], lr: 0.000010, mv_avg_loss: 11.722013, loss: 12.649837
02:17:20.608295 Step[4800/6995], lr: 0.000010, mv_avg_loss: 11.876012, loss: 6.611113
02:17:25.357130 Step[4900/6995], lr: 0.000010, mv_avg_loss: 12.722880, loss: 13.108358
02:17:30.115092 Step[5000/6995], lr: 0.000010, mv_avg_loss: 12.806184, loss: 9.605053
02:17:34.865429 Step[5100/6995], lr: 0.000010, mv_avg_loss: 12.452943, loss: 15.783557
02:17:39.629128 Step[5200/6995], lr: 0.000010, mv_avg_loss: 12.240690, loss: 10.309990
02:17:44.377652 Step[5300/6995], lr: 0.000010, mv_avg_loss: 11.288503, loss: 17.577034
02:17:49.124446 Step[5400/6995], lr: 0.000010, mv_avg_loss: 12.713396, loss: 7.058173
02:17:53.890347 Step[5500/6995], lr: 0.000010, mv_avg_loss: 12.013862, loss: 7.347307
02:17:58.636987 Step[5600/6995], lr: 0.000010, mv_avg_loss: 11.903920, loss: 6.815084
02:18:03.379088 Step[5700/6995], lr: 0.000010, mv_avg_loss: 11.528834, loss: 9.952898
02:18:08.134744 Step[5800/6995], lr: 0.000010, mv_avg_loss: 10.987474, loss: 11.541212
02:18:12.886467 Step[5900/6995], lr: 0.000010, mv_avg_loss: 12.881375, loss: 15.138939
02:18:17.637162 Step[6000/6995], lr: 0.000010, mv_avg_loss: 12.183765, loss: 13.262007
02:18:22.389525 Step[6100/6995], lr: 0.000010, mv_avg_loss: 12.584208, loss: 8.041200
02:18:27.127545 Step[6200/6995], lr: 0.000010, mv_avg_loss: 11.844409, loss: 9.685238
02:18:31.861337 Step[6300/6995], lr: 0.000010, mv_avg_loss: 11.301526, loss: 11.257201
02:18:36.586995 Step[6400/6995], lr: 0.000010, mv_avg_loss: 12.717674, loss: 14.842070
02:18:41.337467 Step[6500/6995], lr: 0.000010, mv_avg_loss: 12.414330, loss: 23.609779
02:18:46.100909 Step[6600/6995], lr: 0.000010, mv_avg_loss: 12.620372, loss: 12.516775
02:18:50.854258 Step[6700/6995], lr: 0.000010, mv_avg_loss: 13.041431, loss: 11.976315
02:18:55.606115 Step[6800/6995], lr: 0.000010, mv_avg_loss: 12.136457, loss: 13.528132
02:19:00.356542 Step[6900/6995], lr: 0.000010, mv_avg_loss: 12.419435, loss: 12.900023
Testing for epoch: 95
Average test PNSR is 25.323986 for 500 images
Start to train epoch 96
02:19:27.004826 Step[0/6995], lr: 0.000010, mv_avg_loss: 11.932909, loss: 14.063148
02:19:31.751692 Step[100/6995], lr: 0.000010, mv_avg_loss: 12.708605, loss: 14.120949
02:19:36.484716 Step[200/6995], lr: 0.000010, mv_avg_loss: 12.168946, loss: 8.353237
02:19:41.221236 Step[300/6995], lr: 0.000010, mv_avg_loss: 11.860139, loss: 9.869556
02:19:45.958435 Step[400/6995], lr: 0.000010, mv_avg_loss: 11.826811, loss: 26.416534
02:19:50.697317 Step[500/6995], lr: 0.000010, mv_avg_loss: 12.306129, loss: 24.147223
02:19:55.421814 Step[600/6995], lr: 0.000010, mv_avg_loss: 12.549842, loss: 9.853608
02:20:00.161448 Step[700/6995], lr: 0.000010, mv_avg_loss: 11.984826, loss: 10.206106
02:20:04.895003 Step[800/6995], lr: 0.000010, mv_avg_loss: 12.427075, loss: 11.562553
02:20:09.613095 Step[900/6995], lr: 0.000010, mv_avg_loss: 11.914337, loss: 12.380920
02:20:14.353452 Step[1000/6995], lr: 0.000010, mv_avg_loss: 12.577419, loss: 9.541163
02:20:19.101736 Step[1100/6995], lr: 0.000010, mv_avg_loss: 12.445111, loss: 21.734875
02:20:23.844158 Step[1200/6995], lr: 0.000010, mv_avg_loss: 11.928904, loss: 16.197363
02:20:28.590192 Step[1300/6995], lr: 0.000010, mv_avg_loss: 13.146194, loss: 10.522146
02:20:33.312486 Step[1400/6995], lr: 0.000010, mv_avg_loss: 11.569931, loss: 10.023020
02:20:38.039457 Step[1500/6995], lr: 0.000010, mv_avg_loss: 12.265089, loss: 19.406851
02:20:42.775748 Step[1600/6995], lr: 0.000010, mv_avg_loss: 11.948357, loss: 16.476254
02:20:47.507055 Step[1700/6995], lr: 0.000010, mv_avg_loss: 12.275825, loss: 15.620949
02:20:52.247812 Step[1800/6995], lr: 0.000010, mv_avg_loss: 11.536972, loss: 9.228487
02:20:56.984709 Step[1900/6995], lr: 0.000010, mv_avg_loss: 11.910075, loss: 6.290997
02:21:01.722678 Step[2000/6995], lr: 0.000010, mv_avg_loss: 11.552191, loss: 10.517965
02:21:06.465698 Step[2100/6995], lr: 0.000010, mv_avg_loss: 11.764544, loss: 23.408054
02:21:11.192787 Step[2200/6995], lr: 0.000010, mv_avg_loss: 12.997415, loss: 11.325049
02:21:15.930848 Step[2300/6995], lr: 0.000010, mv_avg_loss: 11.521210, loss: 11.978189
02:21:20.664897 Step[2400/6995], lr: 0.000010, mv_avg_loss: 12.190235, loss: 21.366171
02:21:25.409114 Step[2500/6995], lr: 0.000010, mv_avg_loss: 12.014688, loss: 13.940987
02:21:30.136190 Step[2600/6995], lr: 0.000010, mv_avg_loss: 12.387461, loss: 16.965332
02:21:34.873687 Step[2700/6995], lr: 0.000010, mv_avg_loss: 11.643890, loss: 7.060810
02:21:39.623092 Step[2800/6995], lr: 0.000010, mv_avg_loss: 12.822430, loss: 18.454214
02:21:44.352272 Step[2900/6995], lr: 0.000010, mv_avg_loss: 13.098256, loss: 10.702202
02:21:49.089800 Step[3000/6995], lr: 0.000010, mv_avg_loss: 11.473141, loss: 8.877973
02:21:53.812278 Step[3100/6995], lr: 0.000010, mv_avg_loss: 11.853086, loss: 15.731296
02:21:58.544526 Step[3200/6995], lr: 0.000010, mv_avg_loss: 12.006404, loss: 13.722876
02:22:03.284042 Step[3300/6995], lr: 0.000010, mv_avg_loss: 12.977877, loss: 6.907575
02:22:08.032066 Step[3400/6995], lr: 0.000010, mv_avg_loss: 11.980910, loss: 11.653860
02:22:12.789824 Step[3500/6995], lr: 0.000010, mv_avg_loss: 11.845090, loss: 13.581902
02:22:17.515306 Step[3600/6995], lr: 0.000010, mv_avg_loss: 12.422603, loss: 16.291542
02:22:22.251484 Step[3700/6995], lr: 0.000010, mv_avg_loss: 12.186687, loss: 9.726907
02:22:26.988786 Step[3800/6995], lr: 0.000010, mv_avg_loss: 11.667943, loss: 6.532934
02:22:31.721053 Step[3900/6995], lr: 0.000010, mv_avg_loss: 13.265552, loss: 8.501360
02:22:36.460623 Step[4000/6995], lr: 0.000010, mv_avg_loss: 11.616254, loss: 13.511915
02:22:41.204018 Step[4100/6995], lr: 0.000010, mv_avg_loss: 12.131941, loss: 8.731669
02:22:45.935669 Step[4200/6995], lr: 0.000010, mv_avg_loss: 12.137514, loss: 10.097893
02:22:50.658364 Step[4300/6995], lr: 0.000010, mv_avg_loss: 12.272115, loss: 9.797948
02:22:55.398672 Step[4400/6995], lr: 0.000010, mv_avg_loss: 12.020531, loss: 11.164448
02:23:00.123692 Step[4500/6995], lr: 0.000010, mv_avg_loss: 12.909997, loss: 7.906595
02:23:04.852874 Step[4600/6995], lr: 0.000010, mv_avg_loss: 12.390313, loss: 10.539405
02:23:09.578243 Step[4700/6995], lr: 0.000010, mv_avg_loss: 12.255960, loss: 10.816649
02:23:14.308488 Step[4800/6995], lr: 0.000010, mv_avg_loss: 12.413519, loss: 13.509700
02:23:19.021286 Step[4900/6995], lr: 0.000010, mv_avg_loss: 13.207873, loss: 14.485637
02:23:23.765237 Step[5000/6995], lr: 0.000010, mv_avg_loss: 12.043769, loss: 12.509028
02:23:28.494519 Step[5100/6995], lr: 0.000010, mv_avg_loss: 11.507430, loss: 11.988303
02:23:33.225497 Step[5200/6995], lr: 0.000010, mv_avg_loss: 11.683409, loss: 9.108386
02:23:37.943282 Step[5300/6995], lr: 0.000010, mv_avg_loss: 11.827755, loss: 12.148926
02:23:42.671210 Step[5400/6995], lr: 0.000010, mv_avg_loss: 11.779904, loss: 9.040751
02:23:47.384157 Step[5500/6995], lr: 0.000010, mv_avg_loss: 12.276682, loss: 10.918831
02:23:52.107183 Step[5600/6995], lr: 0.000010, mv_avg_loss: 11.929579, loss: 11.509476
02:23:56.831467 Step[5700/6995], lr: 0.000010, mv_avg_loss: 12.112327, loss: 16.088266
02:24:01.514113 Step[5800/6995], lr: 0.000010, mv_avg_loss: 12.689036, loss: 11.285731
02:24:06.238628 Step[5900/6995], lr: 0.000010, mv_avg_loss: 12.758209, loss: 10.905342
02:24:10.964183 Step[6000/6995], lr: 0.000010, mv_avg_loss: 11.905884, loss: 11.563086
02:24:15.690630 Step[6100/6995], lr: 0.000010, mv_avg_loss: 11.538569, loss: 6.960696
02:24:20.409255 Step[6200/6995], lr: 0.000010, mv_avg_loss: 12.499696, loss: 16.087589
02:24:25.160013 Step[6300/6995], lr: 0.000010, mv_avg_loss: 11.418139, loss: 18.948927
02:24:29.880844 Step[6400/6995], lr: 0.000010, mv_avg_loss: 12.294039, loss: 14.809172
02:24:34.615373 Step[6500/6995], lr: 0.000010, mv_avg_loss: 12.233709, loss: 11.843729
02:24:39.341003 Step[6600/6995], lr: 0.000010, mv_avg_loss: 12.373619, loss: 11.955030
02:24:44.065484 Step[6700/6995], lr: 0.000010, mv_avg_loss: 12.019592, loss: 18.830170
02:24:48.792811 Step[6800/6995], lr: 0.000010, mv_avg_loss: 11.722999, loss: 13.935734
02:24:53.527355 Step[6900/6995], lr: 0.000010, mv_avg_loss: 13.018295, loss: 11.292007
Testing for epoch: 96
Average test PNSR is 25.311197 for 500 images
Start to train epoch 97
02:25:19.520593 Step[0/6995], lr: 0.000010, mv_avg_loss: 11.692275, loss: 7.279648
02:25:24.219633 Step[100/6995], lr: 0.000010, mv_avg_loss: 11.986992, loss: 10.810718
02:25:28.922683 Step[200/6995], lr: 0.000010, mv_avg_loss: 11.484515, loss: 9.193362
02:25:33.634554 Step[300/6995], lr: 0.000010, mv_avg_loss: 11.594591, loss: 9.887621
02:25:38.336351 Step[400/6995], lr: 0.000010, mv_avg_loss: 12.996053, loss: 8.589834
02:25:43.043252 Step[500/6995], lr: 0.000010, mv_avg_loss: 12.635447, loss: 12.161471
02:25:47.742459 Step[600/6995], lr: 0.000010, mv_avg_loss: 12.903713, loss: 15.016830
02:25:52.443094 Step[700/6995], lr: 0.000010, mv_avg_loss: 11.464696, loss: 12.392428
02:25:57.142094 Step[800/6995], lr: 0.000010, mv_avg_loss: 11.692405, loss: 14.507785
02:26:01.851594 Step[900/6995], lr: 0.000010, mv_avg_loss: 11.680159, loss: 8.716669
02:26:06.562829 Step[1000/6995], lr: 0.000010, mv_avg_loss: 11.975372, loss: 9.603321
02:26:11.280991 Step[1100/6995], lr: 0.000010, mv_avg_loss: 11.739761, loss: 9.005295
02:26:15.967745 Step[1200/6995], lr: 0.000010, mv_avg_loss: 12.115979, loss: 10.299317
02:26:20.687542 Step[1300/6995], lr: 0.000010, mv_avg_loss: 12.365147, loss: 10.200534
02:26:25.413266 Step[1400/6995], lr: 0.000010, mv_avg_loss: 12.315157, loss: 19.623653
02:26:30.145356 Step[1500/6995], lr: 0.000010, mv_avg_loss: 12.165590, loss: 10.859702
02:26:34.863979 Step[1600/6995], lr: 0.000010, mv_avg_loss: 12.077842, loss: 9.431405
02:26:39.587080 Step[1700/6995], lr: 0.000010, mv_avg_loss: 12.671810, loss: 12.477351
02:26:44.328597 Step[1800/6995], lr: 0.000010, mv_avg_loss: 11.407026, loss: 17.725014
02:26:49.066689 Step[1900/6995], lr: 0.000010, mv_avg_loss: 11.624318, loss: 12.264326
02:26:53.791200 Step[2000/6995], lr: 0.000010, mv_avg_loss: 13.186364, loss: 15.101543
02:26:58.511912 Step[2100/6995], lr: 0.000010, mv_avg_loss: 11.739331, loss: 8.741379
02:27:03.245290 Step[2200/6995], lr: 0.000010, mv_avg_loss: 11.242777, loss: 10.705809
02:27:07.967120 Step[2300/6995], lr: 0.000010, mv_avg_loss: 12.002082, loss: 10.350780
02:27:12.687406 Step[2400/6995], lr: 0.000010, mv_avg_loss: 12.312247, loss: 14.801186
02:27:17.420419 Step[2500/6995], lr: 0.000010, mv_avg_loss: 12.339159, loss: 10.265267
02:27:22.145758 Step[2600/6995], lr: 0.000010, mv_avg_loss: 12.056154, loss: 7.916748
02:27:26.870566 Step[2700/6995], lr: 0.000010, mv_avg_loss: 11.491006, loss: 9.496123
02:27:31.601239 Step[2800/6995], lr: 0.000010, mv_avg_loss: 11.123199, loss: 13.647242
02:27:36.328200 Step[2900/6995], lr: 0.000010, mv_avg_loss: 12.079768, loss: 17.514576
02:27:41.051584 Step[3000/6995], lr: 0.000010, mv_avg_loss: 12.286948, loss: 18.082088
02:27:45.775888 Step[3100/6995], lr: 0.000010, mv_avg_loss: 12.851871, loss: 8.094273
02:27:50.509050 Step[3200/6995], lr: 0.000010, mv_avg_loss: 12.511444, loss: 12.754169
02:27:55.251983 Step[3300/6995], lr: 0.000010, mv_avg_loss: 12.409487, loss: 9.177845
02:27:59.982273 Step[3400/6995], lr: 0.000010, mv_avg_loss: 11.453781, loss: 14.192958
02:28:04.712022 Step[3500/6995], lr: 0.000010, mv_avg_loss: 12.341212, loss: 13.126478
02:28:09.442204 Step[3600/6995], lr: 0.000010, mv_avg_loss: 12.246405, loss: 16.245327
02:28:14.172700 Step[3700/6995], lr: 0.000010, mv_avg_loss: 12.114708, loss: 9.817339
02:28:18.904833 Step[3800/6995], lr: 0.000010, mv_avg_loss: 12.702443, loss: 20.715651
02:28:23.631735 Step[3900/6995], lr: 0.000010, mv_avg_loss: 12.192501, loss: 10.164589
02:28:28.362172 Step[4000/6995], lr: 0.000010, mv_avg_loss: 12.725476, loss: 23.348503
02:28:33.091382 Step[4100/6995], lr: 0.000010, mv_avg_loss: 12.233765, loss: 8.120674
02:28:37.823480 Step[4200/6995], lr: 0.000010, mv_avg_loss: 13.876438, loss: 12.771441
02:28:42.568587 Step[4300/6995], lr: 0.000010, mv_avg_loss: 11.547411, loss: 9.937586
02:28:47.305052 Step[4400/6995], lr: 0.000010, mv_avg_loss: 13.091888, loss: 7.837093
02:28:52.044598 Step[4500/6995], lr: 0.000010, mv_avg_loss: 11.941422, loss: 12.365623
02:28:56.782299 Step[4600/6995], lr: 0.000010, mv_avg_loss: 11.932706, loss: 24.810278
02:29:01.504620 Step[4700/6995], lr: 0.000010, mv_avg_loss: 11.759147, loss: 24.486425
02:29:06.236095 Step[4800/6995], lr: 0.000010, mv_avg_loss: 12.725801, loss: 15.507358
02:29:10.977985 Step[4900/6995], lr: 0.000010, mv_avg_loss: 12.271371, loss: 7.704216
02:29:15.733663 Step[5000/6995], lr: 0.000010, mv_avg_loss: 12.666212, loss: 14.026724
02:29:20.471132 Step[5100/6995], lr: 0.000010, mv_avg_loss: 12.052752, loss: 13.546394
02:29:25.224717 Step[5200/6995], lr: 0.000010, mv_avg_loss: 11.636793, loss: 10.660267
02:29:29.958867 Step[5300/6995], lr: 0.000010, mv_avg_loss: 12.291135, loss: 18.986801
02:29:34.702945 Step[5400/6995], lr: 0.000010, mv_avg_loss: 12.392617, loss: 6.635639
02:29:39.445230 Step[5500/6995], lr: 0.000010, mv_avg_loss: 12.723212, loss: 13.686646
02:29:44.181969 Step[5600/6995], lr: 0.000010, mv_avg_loss: 11.738068, loss: 12.456110
02:29:48.925288 Step[5700/6995], lr: 0.000010, mv_avg_loss: 12.644450, loss: 8.684814
02:29:53.663143 Step[5800/6995], lr: 0.000010, mv_avg_loss: 12.054072, loss: 13.895034
02:29:58.403110 Step[5900/6995], lr: 0.000010, mv_avg_loss: 10.716749, loss: 11.852054
02:30:03.138937 Step[6000/6995], lr: 0.000010, mv_avg_loss: 12.508739, loss: 10.727626
02:30:07.891143 Step[6100/6995], lr: 0.000010, mv_avg_loss: 11.738369, loss: 8.077156
02:30:12.635932 Step[6200/6995], lr: 0.000010, mv_avg_loss: 12.962231, loss: 10.036482
02:30:17.377933 Step[6300/6995], lr: 0.000010, mv_avg_loss: 12.524579, loss: 12.348227
02:30:22.118045 Step[6400/6995], lr: 0.000010, mv_avg_loss: 12.047734, loss: 8.689816
02:30:26.885911 Step[6500/6995], lr: 0.000010, mv_avg_loss: 13.295063, loss: 8.508059
02:30:31.648690 Step[6600/6995], lr: 0.000010, mv_avg_loss: 12.485377, loss: 8.860488
02:30:36.388173 Step[6700/6995], lr: 0.000010, mv_avg_loss: 12.134462, loss: 6.713651
02:30:41.124788 Step[6800/6995], lr: 0.000010, mv_avg_loss: 12.268142, loss: 5.701190
02:30:45.873638 Step[6900/6995], lr: 0.000010, mv_avg_loss: 12.353778, loss: 10.777197
Testing for epoch: 97
Average test PNSR is 25.320907 for 500 images
Start to train epoch 98
02:31:12.562995 Step[0/6995], lr: 0.000010, mv_avg_loss: 12.604781, loss: 13.055077
02:31:17.301304 Step[100/6995], lr: 0.000010, mv_avg_loss: 11.679550, loss: 10.177319
02:31:22.028996 Step[200/6995], lr: 0.000010, mv_avg_loss: 12.651176, loss: 18.698629
02:31:26.753625 Step[300/6995], lr: 0.000010, mv_avg_loss: 11.367236, loss: 11.540962
02:31:31.488930 Step[400/6995], lr: 0.000010, mv_avg_loss: 11.995067, loss: 8.676227
02:31:36.241473 Step[500/6995], lr: 0.000010, mv_avg_loss: 12.575318, loss: 7.899409
02:31:40.972768 Step[600/6995], lr: 0.000010, mv_avg_loss: 12.187396, loss: 13.497826
02:31:45.714550 Step[700/6995], lr: 0.000010, mv_avg_loss: 11.451571, loss: 21.597942
02:31:50.451285 Step[800/6995], lr: 0.000010, mv_avg_loss: 12.379069, loss: 11.972991
02:31:55.204572 Step[900/6995], lr: 0.000010, mv_avg_loss: 12.174304, loss: 9.077343
02:31:59.948463 Step[1000/6995], lr: 0.000010, mv_avg_loss: 12.959665, loss: 15.895477
02:32:04.695961 Step[1100/6995], lr: 0.000010, mv_avg_loss: 11.802418, loss: 15.564344
02:32:09.441413 Step[1200/6995], lr: 0.000010, mv_avg_loss: 11.842048, loss: 14.448133
02:32:14.189642 Step[1300/6995], lr: 0.000010, mv_avg_loss: 12.187571, loss: 13.860580
02:32:18.939434 Step[1400/6995], lr: 0.000010, mv_avg_loss: 11.265133, loss: 8.811636
02:32:23.678947 Step[1500/6995], lr: 0.000010, mv_avg_loss: 11.695370, loss: 16.682701
02:32:28.417900 Step[1600/6995], lr: 0.000010, mv_avg_loss: 12.692568, loss: 9.103003
02:32:33.169774 Step[1700/6995], lr: 0.000010, mv_avg_loss: 11.647861, loss: 11.744417
02:32:37.922913 Step[1800/6995], lr: 0.000010, mv_avg_loss: 12.343942, loss: 16.788797
02:32:42.675088 Step[1900/6995], lr: 0.000010, mv_avg_loss: 12.524062, loss: 14.095978
02:32:47.434931 Step[2000/6995], lr: 0.000010, mv_avg_loss: 12.575816, loss: 21.805487
02:32:52.191911 Step[2100/6995], lr: 0.000010, mv_avg_loss: 12.485518, loss: 14.463031
02:32:56.946980 Step[2200/6995], lr: 0.000010, mv_avg_loss: 13.163550, loss: 12.797847
02:33:01.707692 Step[2300/6995], lr: 0.000010, mv_avg_loss: 11.783575, loss: 9.159155
02:33:06.461962 Step[2400/6995], lr: 0.000010, mv_avg_loss: 11.757051, loss: 9.780900
02:33:11.213054 Step[2500/6995], lr: 0.000010, mv_avg_loss: 11.907639, loss: 7.891396
02:33:15.967529 Step[2600/6995], lr: 0.000010, mv_avg_loss: 12.154996, loss: 7.509722
02:33:20.711710 Step[2700/6995], lr: 0.000010, mv_avg_loss: 12.482201, loss: 6.674537
02:33:25.453563 Step[2800/6995], lr: 0.000010, mv_avg_loss: 11.907459, loss: 10.310575
02:33:30.199674 Step[2900/6995], lr: 0.000010, mv_avg_loss: 12.113708, loss: 10.249176
02:33:34.943440 Step[3000/6995], lr: 0.000010, mv_avg_loss: 12.030502, loss: 11.632013
02:33:39.692036 Step[3100/6995], lr: 0.000010, mv_avg_loss: 12.662459, loss: 9.716302
02:33:44.436726 Step[3200/6995], lr: 0.000010, mv_avg_loss: 12.111883, loss: 10.184556
02:33:49.174385 Step[3300/6995], lr: 0.000010, mv_avg_loss: 12.214715, loss: 12.273520
02:33:53.922354 Step[3400/6995], lr: 0.000010, mv_avg_loss: 11.604688, loss: 10.895175
02:33:58.658513 Step[3500/6995], lr: 0.000010, mv_avg_loss: 12.648705, loss: 11.697905
02:34:03.392417 Step[3600/6995], lr: 0.000010, mv_avg_loss: 12.739382, loss: 8.658352
02:34:08.145110 Step[3700/6995], lr: 0.000010, mv_avg_loss: 11.081242, loss: 9.310867
02:34:12.897223 Step[3800/6995], lr: 0.000010, mv_avg_loss: 12.154187, loss: 7.742352
02:34:17.653074 Step[3900/6995], lr: 0.000010, mv_avg_loss: 11.114365, loss: 9.189178
02:34:22.394058 Step[4000/6995], lr: 0.000010, mv_avg_loss: 13.190059, loss: 7.358774
02:34:27.139086 Step[4100/6995], lr: 0.000010, mv_avg_loss: 12.191613, loss: 12.764491
02:34:31.891856 Step[4200/6995], lr: 0.000010, mv_avg_loss: 12.655448, loss: 21.121197
02:34:36.625191 Step[4300/6995], lr: 0.000010, mv_avg_loss: 12.628239, loss: 9.678522
02:34:41.368027 Step[4400/6995], lr: 0.000010, mv_avg_loss: 11.990319, loss: 11.423396
02:34:46.127292 Step[4500/6995], lr: 0.000010, mv_avg_loss: 11.586192, loss: 11.519755
02:34:50.874308 Step[4600/6995], lr: 0.000010, mv_avg_loss: 12.507143, loss: 15.643817
02:34:55.618662 Step[4700/6995], lr: 0.000010, mv_avg_loss: 11.445901, loss: 13.310410
02:35:00.360611 Step[4800/6995], lr: 0.000010, mv_avg_loss: 11.164481, loss: 11.719844
02:35:05.070478 Step[4900/6995], lr: 0.000010, mv_avg_loss: 12.037564, loss: 12.393005
02:35:09.816307 Step[5000/6995], lr: 0.000010, mv_avg_loss: 11.732040, loss: 16.759666
02:35:14.566114 Step[5100/6995], lr: 0.000010, mv_avg_loss: 12.499011, loss: 11.735718
02:35:19.309545 Step[5200/6995], lr: 0.000010, mv_avg_loss: 13.180797, loss: 8.712927
02:35:24.047841 Step[5300/6995], lr: 0.000010, mv_avg_loss: 12.563331, loss: 9.951298
02:35:28.786605 Step[5400/6995], lr: 0.000010, mv_avg_loss: 12.053608, loss: 10.680128
02:35:33.530284 Step[5500/6995], lr: 0.000010, mv_avg_loss: 12.573702, loss: 8.951723
02:35:38.269056 Step[5600/6995], lr: 0.000010, mv_avg_loss: 11.488129, loss: 13.151524
02:35:43.009390 Step[5700/6995], lr: 0.000010, mv_avg_loss: 11.420872, loss: 11.488276
02:35:47.748047 Step[5800/6995], lr: 0.000010, mv_avg_loss: 12.563587, loss: 8.305745
02:35:52.476114 Step[5900/6995], lr: 0.000010, mv_avg_loss: 12.749992, loss: 22.101940
02:35:57.215439 Step[6000/6995], lr: 0.000010, mv_avg_loss: 12.119778, loss: 20.834745
02:36:01.945698 Step[6100/6995], lr: 0.000010, mv_avg_loss: 11.505617, loss: 12.013037
02:36:06.675580 Step[6200/6995], lr: 0.000010, mv_avg_loss: 12.206482, loss: 11.986803
02:36:11.408837 Step[6300/6995], lr: 0.000010, mv_avg_loss: 12.722753, loss: 6.042343
02:36:16.137032 Step[6400/6995], lr: 0.000010, mv_avg_loss: 10.817357, loss: 7.456965
02:36:20.877663 Step[6500/6995], lr: 0.000010, mv_avg_loss: 12.233270, loss: 10.473063
02:36:25.612783 Step[6600/6995], lr: 0.000010, mv_avg_loss: 11.770321, loss: 10.430641
02:36:30.330015 Step[6700/6995], lr: 0.000010, mv_avg_loss: 12.685531, loss: 12.170378
02:36:35.060839 Step[6800/6995], lr: 0.000010, mv_avg_loss: 11.263770, loss: 10.983274
02:36:39.781236 Step[6900/6995], lr: 0.000010, mv_avg_loss: 13.414189, loss: 7.638380
Testing for epoch: 98
Average test PNSR is 25.314774 for 500 images
Start to train epoch 99
02:37:06.069306 Step[0/6995], lr: 0.000010, mv_avg_loss: 11.980169, loss: 7.848562
02:37:10.794553 Step[100/6995], lr: 0.000010, mv_avg_loss: 12.651660, loss: 8.188299
02:37:15.515015 Step[200/6995], lr: 0.000010, mv_avg_loss: 13.006975, loss: 13.653717
02:37:20.238278 Step[300/6995], lr: 0.000010, mv_avg_loss: 11.935433, loss: 10.579039
02:37:24.963868 Step[400/6995], lr: 0.000010, mv_avg_loss: 12.277681, loss: 10.732506
02:37:29.673303 Step[500/6995], lr: 0.000010, mv_avg_loss: 12.109281, loss: 15.168506
02:37:34.395085 Step[600/6995], lr: 0.000010, mv_avg_loss: 11.676741, loss: 8.913699
02:37:39.118575 Step[700/6995], lr: 0.000010, mv_avg_loss: 11.568624, loss: 11.615946
02:37:43.849889 Step[800/6995], lr: 0.000010, mv_avg_loss: 11.705699, loss: 15.742838
02:37:48.571314 Step[900/6995], lr: 0.000010, mv_avg_loss: 11.991172, loss: 9.315979
02:37:53.284301 Step[1000/6995], lr: 0.000010, mv_avg_loss: 12.509240, loss: 25.295380
02:37:57.995077 Step[1100/6995], lr: 0.000010, mv_avg_loss: 12.097569, loss: 11.121956
02:38:02.694508 Step[1200/6995], lr: 0.000010, mv_avg_loss: 12.810133, loss: 10.853354
02:38:07.402989 Step[1300/6995], lr: 0.000010, mv_avg_loss: 12.400866, loss: 14.236195
02:38:12.114863 Step[1400/6995], lr: 0.000010, mv_avg_loss: 12.249487, loss: 7.460660
02:38:16.825975 Step[1500/6995], lr: 0.000010, mv_avg_loss: 11.654939, loss: 11.314471
02:38:21.538388 Step[1600/6995], lr: 0.000010, mv_avg_loss: 12.246613, loss: 23.536823
02:38:26.252446 Step[1700/6995], lr: 0.000010, mv_avg_loss: 13.018487, loss: 11.926279
02:38:30.946035 Step[1800/6995], lr: 0.000010, mv_avg_loss: 12.075511, loss: 11.881990
02:38:35.647881 Step[1900/6995], lr: 0.000010, mv_avg_loss: 12.935266, loss: 17.970192
02:38:40.355212 Step[2000/6995], lr: 0.000010, mv_avg_loss: 12.371566, loss: 20.527546
02:38:45.072436 Step[2100/6995], lr: 0.000010, mv_avg_loss: 11.165084, loss: 9.084543
02:38:49.779990 Step[2200/6995], lr: 0.000010, mv_avg_loss: 12.413454, loss: 7.522644
02:38:54.489512 Step[2300/6995], lr: 0.000010, mv_avg_loss: 12.162756, loss: 21.310703
02:38:59.201009 Step[2400/6995], lr: 0.000010, mv_avg_loss: 13.212287, loss: 11.618040
02:39:03.951402 Step[2500/6995], lr: 0.000010, mv_avg_loss: 11.932020, loss: 9.736940
02:39:08.654108 Step[2600/6995], lr: 0.000010, mv_avg_loss: 13.365358, loss: 20.333700
02:39:13.384052 Step[2700/6995], lr: 0.000010, mv_avg_loss: 11.579107, loss: 10.062099
02:39:18.103442 Step[2800/6995], lr: 0.000010, mv_avg_loss: 13.394326, loss: 12.649364
02:39:22.821052 Step[2900/6995], lr: 0.000010, mv_avg_loss: 11.944578, loss: 8.689662
02:39:27.552161 Step[3000/6995], lr: 0.000010, mv_avg_loss: 12.290083, loss: 11.584575
02:39:32.281961 Step[3100/6995], lr: 0.000010, mv_avg_loss: 11.334723, loss: 7.585271
02:39:37.003190 Step[3200/6995], lr: 0.000010, mv_avg_loss: 11.728422, loss: 15.420557
02:39:41.722159 Step[3300/6995], lr: 0.000010, mv_avg_loss: 13.028469, loss: 8.762912
02:39:46.431747 Step[3400/6995], lr: 0.000010, mv_avg_loss: 12.342234, loss: 8.874785
02:39:51.139903 Step[3500/6995], lr: 0.000010, mv_avg_loss: 12.883348, loss: 10.290402
02:39:55.859919 Step[3600/6995], lr: 0.000010, mv_avg_loss: 12.074009, loss: 14.342822
02:40:00.575013 Step[3700/6995], lr: 0.000010, mv_avg_loss: 12.520748, loss: 8.848823
02:40:05.291289 Step[3800/6995], lr: 0.000010, mv_avg_loss: 11.578209, loss: 10.932155
02:40:10.015049 Step[3900/6995], lr: 0.000010, mv_avg_loss: 12.321736, loss: 8.563709
02:40:14.733196 Step[4000/6995], lr: 0.000010, mv_avg_loss: 11.056222, loss: 13.461519
02:40:19.456089 Step[4100/6995], lr: 0.000010, mv_avg_loss: 11.882853, loss: 12.569354
02:40:24.179772 Step[4200/6995], lr: 0.000010, mv_avg_loss: 12.027912, loss: 11.227791
02:40:28.878487 Step[4300/6995], lr: 0.000010, mv_avg_loss: 11.875852, loss: 9.437578
02:40:33.606115 Step[4400/6995], lr: 0.000010, mv_avg_loss: 12.749939, loss: 22.478811
02:40:38.319265 Step[4500/6995], lr: 0.000010, mv_avg_loss: 11.602437, loss: 10.005103
02:40:43.037573 Step[4600/6995], lr: 0.000010, mv_avg_loss: 12.567044, loss: 7.532670
02:40:47.754129 Step[4700/6995], lr: 0.000010, mv_avg_loss: 11.203768, loss: 13.779048
02:40:52.479644 Step[4800/6995], lr: 0.000010, mv_avg_loss: 12.907656, loss: 24.429028
02:40:57.208051 Step[4900/6995], lr: 0.000010, mv_avg_loss: 12.782637, loss: 14.232269
02:41:01.932100 Step[5000/6995], lr: 0.000010, mv_avg_loss: 11.949852, loss: 10.152278
02:41:06.668655 Step[5100/6995], lr: 0.000010, mv_avg_loss: 12.265245, loss: 9.577333
02:41:11.376159 Step[5200/6995], lr: 0.000010, mv_avg_loss: 11.437268, loss: 13.434566
02:41:16.086551 Step[5300/6995], lr: 0.000010, mv_avg_loss: 11.500690, loss: 10.927475
02:41:20.798585 Step[5400/6995], lr: 0.000010, mv_avg_loss: 12.488409, loss: 11.315551
02:41:25.510562 Step[5500/6995], lr: 0.000010, mv_avg_loss: 12.318069, loss: 7.796113
02:41:30.232619 Step[5600/6995], lr: 0.000010, mv_avg_loss: 11.787617, loss: 6.167894
02:41:34.965938 Step[5700/6995], lr: 0.000010, mv_avg_loss: 11.935140, loss: 8.902621
02:41:39.692456 Step[5800/6995], lr: 0.000010, mv_avg_loss: 11.999965, loss: 9.427862
02:41:44.412509 Step[5900/6995], lr: 0.000010, mv_avg_loss: 12.207476, loss: 12.399961
02:41:49.132274 Step[6000/6995], lr: 0.000010, mv_avg_loss: 12.028809, loss: 8.998927
02:41:53.856950 Step[6100/6995], lr: 0.000010, mv_avg_loss: 12.246255, loss: 9.571436
02:41:58.586778 Step[6200/6995], lr: 0.000010, mv_avg_loss: 12.333020, loss: 10.806107
02:42:03.317764 Step[6300/6995], lr: 0.000010, mv_avg_loss: 11.849122, loss: 13.741438
02:42:08.046618 Step[6400/6995], lr: 0.000010, mv_avg_loss: 11.922660, loss: 5.765784
02:42:12.767010 Step[6500/6995], lr: 0.000010, mv_avg_loss: 11.676938, loss: 11.865349
02:42:17.492969 Step[6600/6995], lr: 0.000010, mv_avg_loss: 11.662774, loss: 7.582352
02:42:22.218965 Step[6700/6995], lr: 0.000010, mv_avg_loss: 11.939306, loss: 17.780245
02:42:26.932370 Step[6800/6995], lr: 0.000010, mv_avg_loss: 12.739834, loss: 9.937832
02:42:31.650978 Step[6900/6995], lr: 0.000010, mv_avg_loss: 12.172020, loss: 11.712852
saving model for epoch 99
Testing for epoch: 99
Average test PNSR is 25.301502 for 500 images
================ Training Loss (Wed Nov 27 19:23:23 2024) ================
import os
import datetime
import argparse
import numpy as np

import torch
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import DataLoader

from ImagePairPrefixFolder import ImagePairPrefixFolder, var_custom_collate
from utils import MovingAvg
from tf_visualizer import TFVisualizer

parser = argparse.ArgumentParser()
parser.add_argument('--network', default='GCANet')
parser.add_argument('--name', default='default_exp')
parser.add_argument('--gpu_ids', default='5')
parser.add_argument('--epochs', type=int, default=100)
parser.add_argument('--lr', type=float, default=0.001)
parser.add_argument('--lr_step', type=int, default=40)
parser.add_argument('--lr_gamma', type=float, default=0.1)
parser.add_argument('--weight_decay', type=float, default=0.0005)
parser.add_argument('--checkpoints_dir', default='checkpoint/')
parser.add_argument('--logDir', default='tblogdir')
parser.add_argument('--resume_dir', default='')
parser.add_argument('--resume_epoch', type=int, default=0)
parser.add_argument('--save_epoch', type=int, default=5)
parser.add_argument('--save_latest_freq', type=int, default=5000)
parser.add_argument('--test_epoch', type=int, default=5)
parser.add_argument('--test_max_size', type=int, default=1080)
parser.add_argument('--size_unit', type=int,  default=8)
parser.add_argument('--print_iter', type=int,  default=100)
parser.add_argument('--input_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy')
parser.add_argument('--gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/clear')
parser.add_argument('--test_input_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy')
parser.add_argument('--test_gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear')
parser.add_argument('--num_workers', type=int, default=16)
parser.add_argument('--batch_size', type=int, default=1)
parser.add_argument('--only_residual', action='store_true', help='regress residual rather than image')
parser.add_argument('--loss_func', default='l2', help='l2|l1')
parser.add_argument('--inc', type=int, default=3)
parser.add_argument('--outc', type=int, default=3)
parser.add_argument('--force_rgb', action='store_true')
parser.add_argument('--no_edge', action='store_true')

opt = parser.parse_args()

opt.input_folder = os.path.expanduser(opt.input_folder)
opt.gt_folder = os.path.expanduser(opt.gt_folder)
opt.test_input_folder = os.path.expanduser(opt.test_input_folder)
opt.test_gt_folder = os.path.expanduser(opt.test_gt_folder)


if not os.path.exists(os.path.join(opt.checkpoints_dir, opt.name)):
    os.makedirs(os.path.join(opt.checkpoints_dir, opt.name))
opt.resume_dir = opt.resume_dir if opt.resume_dir != '' else os.path.join(opt.checkpoints_dir, opt.name)

visualizer = TFVisualizer(opt)
### Log out
with open(os.path.realpath(__file__), 'r') as fid:
    visualizer.print_logs(fid.read())

## print argument
for key, val in vars(opt).items():
    visualizer.print_logs('%s: %s' % (key, val))

opt.gpu_ids = [int(x) for x in opt.gpu_ids.split(',')]
assert all(0 <= x <= torch.cuda.device_count() for x in opt.gpu_ids), 'gpu id should ' \
                                                      'be 0~{0}'.format(torch.cuda.device_count())
torch.cuda.set_device(opt.gpu_ids[0])


train_dataset = ImagePairPrefixFolder(opt.input_folder, opt.gt_folder, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
train_dataloader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True,
                              collate_fn=var_custom_collate, pin_memory=True,
                              num_workers=opt.num_workers)

opt.do_test = opt.test_gt_folder != ''
if opt.do_test:
    test_dataset = ImagePairPrefixFolder(opt.test_input_folder, opt.test_gt_folder,
                                         max_img_size=opt.test_max_size, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,
                                 collate_fn=var_custom_collate, pin_memory=True,
                                 num_workers=1)

total_inc = opt.inc if opt.no_edge else opt.inc + 1
if opt.network == 'GCANet':
    from GCANet import GCANet
    net = GCANet()
else:
    print('network structure %s not supported' % opt.network)
    raise ValueError

if opt.loss_func == 'l2':
    loss_crit = torch.nn.MSELoss()
elif opt.loss_func == 'l1':
    loss_crit = torch.nn.SmoothL1Loss()
else:
    print('loss_func %s not supported' % opt.loss_func)
    raise ValueError
pnsr_crit = torch.nn.MSELoss()

if len(opt.gpu_ids) > 0:
    net.cuda()
    if len(opt.gpu_ids) > 1:
        net = torch.nn.DataParallel(net)
    loss_crit = loss_crit.cuda()
    pnsr_crit = pnsr_crit.cuda()

optimizer = optim.Adam(net.parameters(), lr=opt.lr)
step_optim_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.lr_step, gamma=opt.lr_gamma)
loss_avg = MovingAvg(pool_size=50)

start_epoch = 0
total_iter = 0

if os.path.exists(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth')):
    print('resuming from latest.pth')
    latest_info = torch.load(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))
    start_epoch = latest_info['epoch']
    total_iter = latest_info['total_iter']
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(latest_info['net_state'])
    else:
        net.load_state_dict(latest_info['net_state'])
    optimizer.load_state_dict(latest_info['optim_state'])

if opt.resume_epoch > 0:
    start_epoch = opt.resume_epoch
    total_iter = opt.resume_epoch * len(train_dataloader)
    resume_path = os.path.join(opt.resume_epoch, 'net_epoch_%d.pth') % opt.resume_epoch
    print('resume from : %s' % resume_path)
    assert os.path.exists(resume_path), 'cannot find the resume model: %s ' % resume_path
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(torch.load(resume_path))
    else:
        net.load_state_dict(torch.load(resume_path))

for epoch in range(start_epoch, opt.epochs):
    visualizer.print_logs("Start to train epoch %d" % epoch)
    net.train()
    for iter, data in enumerate(train_dataloader):
        total_iter += 1
        optimizer.zero_grad()
        step_optim_scheduler.step(epoch)

        batch_input_img, batch_input_edge,  batch_gt = data
        if len(opt.gpu_ids) > 0:
            batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

        if opt.no_edge:
            batch_input = batch_input_img
        else:
            batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
        batch_input_v = Variable(batch_input)
        if opt.only_residual:
            batch_gt_v = Variable(batch_gt - (batch_input_img+128))
        else:
            batch_gt_v = Variable(batch_gt)

        pred = net(batch_input_v)

        loss = loss_crit(pred, batch_gt_v)
        avg_loss = loss_avg.set_curr_val(loss.data)

        loss.backward()
        optimizer.step()

        if iter % opt.print_iter == 0:
            visualizer.plot_current_losses(total_iter, { 'loss': loss})
            visualizer.print_logs('%s Step[%d/%d], lr: %f, mv_avg_loss: %f, loss: %f' %
                                    (str(datetime.datetime.now()).split(' ')[1], iter, len(train_dataloader),
                                     step_optim_scheduler.get_lr()[0], avg_loss, loss))

        if total_iter % opt.save_latest_freq == 0:
            latest_info = {'total_iter': total_iter,
                           'epoch': epoch,
                           'optim_state': optimizer.state_dict()}
            if len(opt.gpu_ids) > 1:
                latest_info['net_state'] = net.module.state_dict()
            else:
                latest_info['net_state'] = net.state_dict()
            print('save lastest model.')
            torch.save(latest_info, os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))

    if (epoch+1) % opt.save_epoch == 0 :
        visualizer.print_logs('saving model for epoch %d' % epoch)
        if len(opt.gpu_ids) > 1:
            torch.save(net.module.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch+1)))
        else:
            torch.save(net.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch + 1)))

    if opt.do_test:        
        avg_psnr = 0
        task_cnt = 0
        net.eval()
        with torch.no_grad():
            for iter, data in enumerate(test_dataloader):
                batch_input_img, batch_input_edge,  batch_gt = data
                if len(opt.gpu_ids) > 0:
                    batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

                if opt.no_edge:
                    batch_input = batch_input_img
                else:
                    batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
                batch_input_v = Variable(batch_input)
                batch_gt_v = Variable(batch_gt)


                pred = net(batch_input_v)

                if opt.only_residual:
                    loss = pnsr_crit(pred+Variable(batch_input_img+128), batch_gt_v)
                else:
                    loss = pnsr_crit(pred, batch_gt_v)
                avg_psnr += 10 * np.log10(255 * 255 / loss.item())
                task_cnt += 1

        visualizer.print_logs('Testing for epoch: %d' % epoch)
        visualizer.print_logs('Average test PNSR is %f for %d images' % (avg_psnr/task_cnt, task_cnt))








network: GCANet
name: default_exp
gpu_ids: 5
epochs: 100
lr: 0.001
lr_step: 40
lr_gamma: 0.1
weight_decay: 0.0005
checkpoints_dir: checkpoint/
logDir: tblogdir
resume_dir: checkpoint/default_exp
resume_epoch: 0
save_epoch: 5
save_latest_freq: 5000
test_epoch: 5
test_max_size: 1080
size_unit: 8
print_iter: 100
input_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy
gt_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/clear
test_input_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy
test_gt_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear
num_workers: 16
batch_size: 1
only_residual: False
loss_func: l2
inc: 3
outc: 3
force_rgb: False
no_edge: False
================ Training Loss (Wed Nov 27 19:24:15 2024) ================
import os
import datetime
import argparse
import numpy as np

import torch
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import DataLoader

from ImagePairPrefixFolder import ImagePairPrefixFolder, var_custom_collate
from utils import MovingAvg
from tf_visualizer import TFVisualizer

parser = argparse.ArgumentParser()
parser.add_argument('--network', default='GCANet')
parser.add_argument('--name', default='default_exp')
parser.add_argument('--gpu_ids', default='5')
parser.add_argument('--epochs', type=int, default=100)
parser.add_argument('--lr', type=float, default=0.001)
parser.add_argument('--lr_step', type=int, default=40)
parser.add_argument('--lr_gamma', type=float, default=0.1)
parser.add_argument('--weight_decay', type=float, default=0.0005)
parser.add_argument('--checkpoints_dir', default='checkpoint/')
parser.add_argument('--logDir', default='tblogdir')
parser.add_argument('--resume_dir', default='')
parser.add_argument('--resume_epoch', type=int, default=0)
parser.add_argument('--save_epoch', type=int, default=5)
parser.add_argument('--save_latest_freq', type=int, default=5000)
parser.add_argument('--test_epoch', type=int, default=5)
parser.add_argument('--test_max_size', type=int, default=1080)
parser.add_argument('--size_unit', type=int,  default=8)
parser.add_argument('--print_iter', type=int,  default=100)
parser.add_argument('--input_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy')
parser.add_argument('--gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/clear')
parser.add_argument('--test_input_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy')
parser.add_argument('--test_gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear')
parser.add_argument('--num_workers', type=int, default=16)
parser.add_argument('--batch_size', type=int, default=1)
parser.add_argument('--only_residual', action='store_true', help='regress residual rather than image')
parser.add_argument('--loss_func', default='l2', help='l2|l1')
parser.add_argument('--inc', type=int, default=3)
parser.add_argument('--outc', type=int, default=3)
parser.add_argument('--force_rgb', action='store_true')
parser.add_argument('--no_edge', action='store_true')

opt = parser.parse_args()

opt.input_folder = os.path.expanduser(opt.input_folder)
opt.gt_folder = os.path.expanduser(opt.gt_folder)
opt.test_input_folder = os.path.expanduser(opt.test_input_folder)
opt.test_gt_folder = os.path.expanduser(opt.test_gt_folder)


if not os.path.exists(os.path.join(opt.checkpoints_dir, opt.name)):
    os.makedirs(os.path.join(opt.checkpoints_dir, opt.name))
opt.resume_dir = opt.resume_dir if opt.resume_dir != '' else os.path.join(opt.checkpoints_dir, opt.name)

visualizer = TFVisualizer(opt)
### Log out
with open(os.path.realpath(__file__), 'r') as fid:
    visualizer.print_logs(fid.read())

## print argument
for key, val in vars(opt).items():
    visualizer.print_logs('%s: %s' % (key, val))

opt.gpu_ids = [int(x) for x in opt.gpu_ids.split(',')]
assert all(0 <= x <= torch.cuda.device_count() for x in opt.gpu_ids), 'gpu id should ' \
                                                      'be 0~{0}'.format(torch.cuda.device_count())
torch.cuda.set_device(opt.gpu_ids[0])


train_dataset = ImagePairPrefixFolder(opt.input_folder, opt.gt_folder, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
train_dataloader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True,
                              collate_fn=var_custom_collate, pin_memory=True,
                              num_workers=opt.num_workers)

opt.do_test = opt.test_gt_folder != ''
if opt.do_test:
    test_dataset = ImagePairPrefixFolder(opt.test_input_folder, opt.test_gt_folder,
                                         max_img_size=opt.test_max_size, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,
                                 collate_fn=var_custom_collate, pin_memory=True,
                                 num_workers=1)

total_inc = opt.inc if opt.no_edge else opt.inc + 1
if opt.network == 'GCANet':
    from GCANet import GCANet
    net = GCANet()
else:
    print('network structure %s not supported' % opt.network)
    raise ValueError

if opt.loss_func == 'l2':
    loss_crit = torch.nn.MSELoss()
elif opt.loss_func == 'l1':
    loss_crit = torch.nn.SmoothL1Loss()
else:
    print('loss_func %s not supported' % opt.loss_func)
    raise ValueError
pnsr_crit = torch.nn.MSELoss()

if len(opt.gpu_ids) > 0:
    net.cuda()
    if len(opt.gpu_ids) > 1:
        net = torch.nn.DataParallel(net)
    loss_crit = loss_crit.cuda()
    pnsr_crit = pnsr_crit.cuda()

optimizer = optim.Adam(net.parameters(), lr=opt.lr)
step_optim_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.lr_step, gamma=opt.lr_gamma)
loss_avg = MovingAvg(pool_size=50)

start_epoch = 0
total_iter = 0

if os.path.exists(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth')):
    print('resuming from latest.pth')
    latest_info = torch.load(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))
    start_epoch = latest_info['epoch']
    total_iter = latest_info['total_iter']
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(latest_info['net_state'])
    else:
        net.load_state_dict(latest_info['net_state'])
    optimizer.load_state_dict(latest_info['optim_state'])

if opt.resume_epoch > 0:
    start_epoch = opt.resume_epoch
    total_iter = opt.resume_epoch * len(train_dataloader)
    resume_path = os.path.join(opt.resume_epoch, 'net_epoch_%d.pth') % opt.resume_epoch
    print('resume from : %s' % resume_path)
    assert os.path.exists(resume_path), 'cannot find the resume model: %s ' % resume_path
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(torch.load(resume_path))
    else:
        net.load_state_dict(torch.load(resume_path))

for epoch in range(start_epoch, opt.epochs):
    visualizer.print_logs("Start to train epoch %d" % epoch)
    net.train()
    for iter, data in enumerate(train_dataloader):
        total_iter += 1
        optimizer.zero_grad()
        step_optim_scheduler.step(epoch)

        batch_input_img, batch_input_edge,  batch_gt = data
        if len(opt.gpu_ids) > 0:
            batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

        if opt.no_edge:
            batch_input = batch_input_img
        else:
            batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
        batch_input_v = Variable(batch_input)
        if opt.only_residual:
            batch_gt_v = Variable(batch_gt - (batch_input_img+128))
        else:
            batch_gt_v = Variable(batch_gt)

        pred = net(batch_input_v)

        loss = loss_crit(pred, batch_gt_v)
        avg_loss = loss_avg.set_curr_val(loss.data)

        loss.backward()
        optimizer.step()

        if iter % opt.print_iter == 0:
            visualizer.plot_current_losses(total_iter, { 'loss': loss})
            visualizer.print_logs('%s Step[%d/%d], lr: %f, mv_avg_loss: %f, loss: %f' %
                                    (str(datetime.datetime.now()).split(' ')[1], iter, len(train_dataloader),
                                     step_optim_scheduler.get_lr()[0], avg_loss, loss))

        if total_iter % opt.save_latest_freq == 0:
            latest_info = {'total_iter': total_iter,
                           'epoch': epoch,
                           'optim_state': optimizer.state_dict()}
            if len(opt.gpu_ids) > 1:
                latest_info['net_state'] = net.module.state_dict()
            else:
                latest_info['net_state'] = net.state_dict()
            print('save lastest model.')
            torch.save(latest_info, os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))

    if (epoch+1) % opt.save_epoch == 0 :
        visualizer.print_logs('saving model for epoch %d' % epoch)
        if len(opt.gpu_ids) > 1:
            torch.save(net.module.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch+1)))
        else:
            torch.save(net.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch + 1)))

    if opt.do_test:        
        avg_psnr = 0
        task_cnt = 0
        net.eval()
        with torch.no_grad():
            for iter, data in enumerate(test_dataloader):
                batch_input_img, batch_input_edge,  batch_gt = data
                if len(opt.gpu_ids) > 0:
                    batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

                if opt.no_edge:
                    batch_input = batch_input_img
                else:
                    batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
                batch_input_v = Variable(batch_input)
                batch_gt_v = Variable(batch_gt)


                pred = net(batch_input_v)

                if opt.only_residual:
                    loss = pnsr_crit(pred+Variable(batch_input_img+128), batch_gt_v)
                else:
                    loss = pnsr_crit(pred, batch_gt_v)
                avg_psnr += 10 * np.log10(255 * 255 / loss.item())
                task_cnt += 1

        visualizer.print_logs('Testing for epoch: %d' % epoch)
        visualizer.print_logs('Average test PNSR is %f for %d images' % (avg_psnr/task_cnt, task_cnt))








network: GCANet
name: default_exp
gpu_ids: 5
epochs: 100
lr: 0.001
lr_step: 40
lr_gamma: 0.1
weight_decay: 0.0005
checkpoints_dir: checkpoint/
logDir: tblogdir
resume_dir: checkpoint/default_exp
resume_epoch: 0
save_epoch: 5
save_latest_freq: 5000
test_epoch: 5
test_max_size: 1080
size_unit: 8
print_iter: 100
input_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy
gt_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/clear
test_input_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy
test_gt_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear
num_workers: 16
batch_size: 1
only_residual: False
loss_func: l2
inc: 3
outc: 3
force_rgb: False
no_edge: False
================ Training Loss (Wed Nov 27 19:24:31 2024) ================
import os
import datetime
import argparse
import numpy as np

import torch
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import DataLoader

from ImagePairPrefixFolder import ImagePairPrefixFolder, var_custom_collate
from utils import MovingAvg
from tf_visualizer import TFVisualizer

parser = argparse.ArgumentParser()
parser.add_argument('--network', default='GCANet')
parser.add_argument('--name', default='default_exp')
parser.add_argument('--gpu_ids', default='5')
parser.add_argument('--epochs', type=int, default=100)
parser.add_argument('--lr', type=float, default=0.001)
parser.add_argument('--lr_step', type=int, default=40)
parser.add_argument('--lr_gamma', type=float, default=0.1)
parser.add_argument('--weight_decay', type=float, default=0.0005)
parser.add_argument('--checkpoints_dir', default='checkpoint/')
parser.add_argument('--logDir', default='tblogdir')
parser.add_argument('--resume_dir', default='')
parser.add_argument('--resume_epoch', type=int, default=0)
parser.add_argument('--save_epoch', type=int, default=5)
parser.add_argument('--save_latest_freq', type=int, default=5000)
parser.add_argument('--test_epoch', type=int, default=5)
parser.add_argument('--test_max_size', type=int, default=1080)
parser.add_argument('--size_unit', type=int,  default=8)
parser.add_argument('--print_iter', type=int,  default=100)
parser.add_argument('--input_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy')
parser.add_argument('--gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/clear')
parser.add_argument('--test_input_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy')
parser.add_argument('--test_gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear')
parser.add_argument('--num_workers', type=int, default=16)
parser.add_argument('--batch_size', type=int, default=1)
parser.add_argument('--only_residual', action='store_true', help='regress residual rather than image')
parser.add_argument('--loss_func', default='l2', help='l2|l1')
parser.add_argument('--inc', type=int, default=3)
parser.add_argument('--outc', type=int, default=3)
parser.add_argument('--force_rgb', action='store_true')
parser.add_argument('--no_edge', action='store_true')

opt = parser.parse_args()

opt.input_folder = os.path.expanduser(opt.input_folder)
opt.gt_folder = os.path.expanduser(opt.gt_folder)
opt.test_input_folder = os.path.expanduser(opt.test_input_folder)
opt.test_gt_folder = os.path.expanduser(opt.test_gt_folder)


if not os.path.exists(os.path.join(opt.checkpoints_dir, opt.name)):
    os.makedirs(os.path.join(opt.checkpoints_dir, opt.name))
opt.resume_dir = opt.resume_dir if opt.resume_dir != '' else os.path.join(opt.checkpoints_dir, opt.name)

visualizer = TFVisualizer(opt)
### Log out
with open(os.path.realpath(__file__), 'r') as fid:
    visualizer.print_logs(fid.read())

## print argument
for key, val in vars(opt).items():
    visualizer.print_logs('%s: %s' % (key, val))

opt.gpu_ids = [int(x) for x in opt.gpu_ids.split(',')]
assert all(0 <= x <= torch.cuda.device_count() for x in opt.gpu_ids), 'gpu id should ' \
                                                      'be 0~{0}'.format(torch.cuda.device_count())
torch.cuda.set_device(opt.gpu_ids[0])


train_dataset = ImagePairPrefixFolder(opt.input_folder, opt.gt_folder, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
train_dataloader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True,
                              collate_fn=var_custom_collate, pin_memory=True,
                              num_workers=opt.num_workers)

opt.do_test = opt.test_gt_folder != ''
if opt.do_test:
    test_dataset = ImagePairPrefixFolder(opt.test_input_folder, opt.test_gt_folder,
                                         max_img_size=opt.test_max_size, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,
                                 collate_fn=var_custom_collate, pin_memory=True,
                                 num_workers=1)

total_inc = opt.inc if opt.no_edge else opt.inc + 1
if opt.network == 'GCANet':
    from GCANet import GCANet
    net = GCANet()
else:
    print('network structure %s not supported' % opt.network)
    raise ValueError

if opt.loss_func == 'l2':
    loss_crit = torch.nn.MSELoss()
elif opt.loss_func == 'l1':
    loss_crit = torch.nn.SmoothL1Loss()
else:
    print('loss_func %s not supported' % opt.loss_func)
    raise ValueError
pnsr_crit = torch.nn.MSELoss()

if len(opt.gpu_ids) > 0:
    net.cuda()
    if len(opt.gpu_ids) > 1:
        net = torch.nn.DataParallel(net)
    loss_crit = loss_crit.cuda()
    pnsr_crit = pnsr_crit.cuda()

optimizer = optim.Adam(net.parameters(), lr=opt.lr)
step_optim_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.lr_step, gamma=opt.lr_gamma)
loss_avg = MovingAvg(pool_size=50)

start_epoch = 0
total_iter = 0

if os.path.exists(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth')):
    print('resuming from latest.pth')
    latest_info = torch.load(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))
    start_epoch = latest_info['epoch']
    total_iter = latest_info['total_iter']
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(latest_info['net_state'])
    else:
        net.load_state_dict(latest_info['net_state'])
    optimizer.load_state_dict(latest_info['optim_state'])

if opt.resume_epoch > 0:
    start_epoch = opt.resume_epoch
    total_iter = opt.resume_epoch * len(train_dataloader)
    resume_path = os.path.join(opt.resume_epoch, 'net_epoch_%d.pth') % opt.resume_epoch
    print('resume from : %s' % resume_path)
    assert os.path.exists(resume_path), 'cannot find the resume model: %s ' % resume_path
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(torch.load(resume_path))
    else:
        net.load_state_dict(torch.load(resume_path))

for epoch in range(start_epoch, opt.epochs):
    visualizer.print_logs("Start to train epoch %d" % epoch)
    net.train()
    for iter, data in enumerate(train_dataloader):
        total_iter += 1
        optimizer.zero_grad()
        step_optim_scheduler.step(epoch)

        batch_input_img, batch_input_edge,  batch_gt = data
        if len(opt.gpu_ids) > 0:
            batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

        if opt.no_edge:
            batch_input = batch_input_img
        else:
            batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
        batch_input_v = Variable(batch_input)
        if opt.only_residual:
            batch_gt_v = Variable(batch_gt - (batch_input_img+128))
        else:
            batch_gt_v = Variable(batch_gt)

        pred = net(batch_input_v)

        loss = loss_crit(pred, batch_gt_v)
        avg_loss = loss_avg.set_curr_val(loss.data)

        loss.backward()
        optimizer.step()

        if iter % opt.print_iter == 0:
            visualizer.plot_current_losses(total_iter, { 'loss': loss})
            visualizer.print_logs('%s Step[%d/%d], lr: %f, mv_avg_loss: %f, loss: %f' %
                                    (str(datetime.datetime.now()).split(' ')[1], iter, len(train_dataloader),
                                     step_optim_scheduler.get_lr()[0], avg_loss, loss))

        if total_iter % opt.save_latest_freq == 0:
            latest_info = {'total_iter': total_iter,
                           'epoch': epoch,
                           'optim_state': optimizer.state_dict()}
            if len(opt.gpu_ids) > 1:
                latest_info['net_state'] = net.module.state_dict()
            else:
                latest_info['net_state'] = net.state_dict()
            print('save lastest model.')
            torch.save(latest_info, os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))

    if (epoch+1) % opt.save_epoch == 0 :
        visualizer.print_logs('saving model for epoch %d' % epoch)
        if len(opt.gpu_ids) > 1:
            torch.save(net.module.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch+1)))
        else:
            torch.save(net.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch + 1)))

    if opt.do_test:        
        avg_psnr = 0
        task_cnt = 0
        net.eval()
        with torch.no_grad():
            for iter, data in enumerate(test_dataloader):
                batch_input_img, batch_input_edge,  batch_gt = data
                if len(opt.gpu_ids) > 0:
                    batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

                if opt.no_edge:
                    batch_input = batch_input_img
                else:
                    batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
                batch_input_v = Variable(batch_input)
                batch_gt_v = Variable(batch_gt)


                pred = net(batch_input_v)

                if opt.only_residual:
                    loss = pnsr_crit(pred+Variable(batch_input_img+128), batch_gt_v)
                else:
                    loss = pnsr_crit(pred, batch_gt_v)
                avg_psnr += 10 * np.log10(255 * 255 / loss.item())
                task_cnt += 1

        visualizer.print_logs('Testing for epoch: %d' % epoch)
        visualizer.print_logs('Average test PNSR is %f for %d images' % (avg_psnr/task_cnt, task_cnt))








network: GCANet
name: default_exp
gpu_ids: 5
epochs: 100
lr: 0.001
lr_step: 40
lr_gamma: 0.1
weight_decay: 0.0005
checkpoints_dir: checkpoint/
logDir: tblogdir
resume_dir: checkpoint/default_exp
resume_epoch: 0
save_epoch: 5
save_latest_freq: 5000
test_epoch: 5
test_max_size: 1080
size_unit: 8
print_iter: 100
input_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy
gt_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/clear
test_input_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy
test_gt_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear
num_workers: 16
batch_size: 1
only_residual: False
loss_func: l2
inc: 3
outc: 3
force_rgb: False
no_edge: False
Start to train epoch 99
19:24:38.418280 Step[0/13990], lr: 0.000010, mv_avg_loss: 10.390045, loss: 10.390045
19:24:41.617523 Step[100/13990], lr: 0.000010, mv_avg_loss: 11.493441, loss: 10.339088
19:24:44.761675 Step[200/13990], lr: 0.000010, mv_avg_loss: 12.094078, loss: 7.911550
19:24:47.908770 Step[300/13990], lr: 0.000010, mv_avg_loss: 12.192200, loss: 14.457767
================ Training Loss (Wed Nov 27 19:25:13 2024) ================
import os
import datetime
import argparse
import numpy as np

import torch
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import DataLoader

from ImagePairPrefixFolder import ImagePairPrefixFolder, var_custom_collate
from utils import MovingAvg
from tf_visualizer import TFVisualizer

parser = argparse.ArgumentParser()
parser.add_argument('--network', default='GCANet')
parser.add_argument('--name', default='default_exp')
parser.add_argument('--gpu_ids', default='5')
parser.add_argument('--epochs', type=int, default=100)
parser.add_argument('--lr', type=float, default=0.001)
parser.add_argument('--lr_step', type=int, default=40)
parser.add_argument('--lr_gamma', type=float, default=0.1)
parser.add_argument('--weight_decay', type=float, default=0.0005)
parser.add_argument('--checkpoints_dir', default='checkpoint/')
parser.add_argument('--logDir', default='tblogdir')
parser.add_argument('--resume_dir', default='')
parser.add_argument('--resume_epoch', type=int, default=0)
parser.add_argument('--save_epoch', type=int, default=5)
parser.add_argument('--save_latest_freq', type=int, default=5000)
parser.add_argument('--test_epoch', type=int, default=5)
parser.add_argument('--test_max_size', type=int, default=1080)
parser.add_argument('--size_unit', type=int,  default=8)
parser.add_argument('--print_iter', type=int,  default=100)
parser.add_argument('--input_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy')
parser.add_argument('--gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/clear')
parser.add_argument('--test_input_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy')
parser.add_argument('--test_gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear')
parser.add_argument('--num_workers', type=int, default=16)
parser.add_argument('--batch_size', type=int, default=4)
parser.add_argument('--only_residual', action='store_true', help='regress residual rather than image')
parser.add_argument('--loss_func', default='l2', help='l2|l1')
parser.add_argument('--inc', type=int, default=3)
parser.add_argument('--outc', type=int, default=3)
parser.add_argument('--force_rgb', action='store_true')
parser.add_argument('--no_edge', action='store_true')

opt = parser.parse_args()

opt.input_folder = os.path.expanduser(opt.input_folder)
opt.gt_folder = os.path.expanduser(opt.gt_folder)
opt.test_input_folder = os.path.expanduser(opt.test_input_folder)
opt.test_gt_folder = os.path.expanduser(opt.test_gt_folder)


if not os.path.exists(os.path.join(opt.checkpoints_dir, opt.name)):
    os.makedirs(os.path.join(opt.checkpoints_dir, opt.name))
opt.resume_dir = opt.resume_dir if opt.resume_dir != '' else os.path.join(opt.checkpoints_dir, opt.name)

visualizer = TFVisualizer(opt)
### Log out
with open(os.path.realpath(__file__), 'r') as fid:
    visualizer.print_logs(fid.read())

## print argument
for key, val in vars(opt).items():
    visualizer.print_logs('%s: %s' % (key, val))

opt.gpu_ids = [int(x) for x in opt.gpu_ids.split(',')]
assert all(0 <= x <= torch.cuda.device_count() for x in opt.gpu_ids), 'gpu id should ' \
                                                      'be 0~{0}'.format(torch.cuda.device_count())
torch.cuda.set_device(opt.gpu_ids[0])


train_dataset = ImagePairPrefixFolder(opt.input_folder, opt.gt_folder, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
train_dataloader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True,
                              collate_fn=var_custom_collate, pin_memory=True,
                              num_workers=opt.num_workers)

opt.do_test = opt.test_gt_folder != ''
if opt.do_test:
    test_dataset = ImagePairPrefixFolder(opt.test_input_folder, opt.test_gt_folder,
                                         max_img_size=opt.test_max_size, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,
                                 collate_fn=var_custom_collate, pin_memory=True,
                                 num_workers=1)

total_inc = opt.inc if opt.no_edge else opt.inc + 1
if opt.network == 'GCANet':
    from GCANet import GCANet
    net = GCANet()
else:
    print('network structure %s not supported' % opt.network)
    raise ValueError

if opt.loss_func == 'l2':
    loss_crit = torch.nn.MSELoss()
elif opt.loss_func == 'l1':
    loss_crit = torch.nn.SmoothL1Loss()
else:
    print('loss_func %s not supported' % opt.loss_func)
    raise ValueError
pnsr_crit = torch.nn.MSELoss()

if len(opt.gpu_ids) > 0:
    net.cuda()
    if len(opt.gpu_ids) > 1:
        net = torch.nn.DataParallel(net)
    loss_crit = loss_crit.cuda()
    pnsr_crit = pnsr_crit.cuda()

optimizer = optim.Adam(net.parameters(), lr=opt.lr)
step_optim_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.lr_step, gamma=opt.lr_gamma)
loss_avg = MovingAvg(pool_size=50)

start_epoch = 0
total_iter = 0

if os.path.exists(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth')):
    print('resuming from latest.pth')
    latest_info = torch.load(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))
    start_epoch = latest_info['epoch']
    total_iter = latest_info['total_iter']
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(latest_info['net_state'])
    else:
        net.load_state_dict(latest_info['net_state'])
    optimizer.load_state_dict(latest_info['optim_state'])

if opt.resume_epoch > 0:
    start_epoch = opt.resume_epoch
    total_iter = opt.resume_epoch * len(train_dataloader)
    resume_path = os.path.join(opt.resume_epoch, 'net_epoch_%d.pth') % opt.resume_epoch
    print('resume from : %s' % resume_path)
    assert os.path.exists(resume_path), 'cannot find the resume model: %s ' % resume_path
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(torch.load(resume_path))
    else:
        net.load_state_dict(torch.load(resume_path))

for epoch in range(start_epoch, opt.epochs):
    visualizer.print_logs("Start to train epoch %d" % epoch)
    net.train()
    for iter, data in enumerate(train_dataloader):
        total_iter += 1
        optimizer.zero_grad()
        step_optim_scheduler.step(epoch)

        batch_input_img, batch_input_edge,  batch_gt = data
        if len(opt.gpu_ids) > 0:
            batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

        if opt.no_edge:
            batch_input = batch_input_img
        else:
            batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
        batch_input_v = Variable(batch_input)
        if opt.only_residual:
            batch_gt_v = Variable(batch_gt - (batch_input_img+128))
        else:
            batch_gt_v = Variable(batch_gt)

        pred = net(batch_input_v)

        loss = loss_crit(pred, batch_gt_v)
        avg_loss = loss_avg.set_curr_val(loss.data)

        loss.backward()
        optimizer.step()

        if iter % opt.print_iter == 0:
            visualizer.plot_current_losses(total_iter, { 'loss': loss})
            visualizer.print_logs('%s Step[%d/%d], lr: %f, mv_avg_loss: %f, loss: %f' %
                                    (str(datetime.datetime.now()).split(' ')[1], iter, len(train_dataloader),
                                     step_optim_scheduler.get_lr()[0], avg_loss, loss))

        if total_iter % opt.save_latest_freq == 0:
            latest_info = {'total_iter': total_iter,
                           'epoch': epoch,
                           'optim_state': optimizer.state_dict()}
            if len(opt.gpu_ids) > 1:
                latest_info['net_state'] = net.module.state_dict()
            else:
                latest_info['net_state'] = net.state_dict()
            print('save lastest model.')
            torch.save(latest_info, os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))

    if (epoch+1) % opt.save_epoch == 0 :
        visualizer.print_logs('saving model for epoch %d' % epoch)
        if len(opt.gpu_ids) > 1:
            torch.save(net.module.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch+1)))
        else:
            torch.save(net.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch + 1)))

    if opt.do_test:        
        avg_psnr = 0
        task_cnt = 0
        net.eval()
        with torch.no_grad():
            for iter, data in enumerate(test_dataloader):
                batch_input_img, batch_input_edge,  batch_gt = data
                if len(opt.gpu_ids) > 0:
                    batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

                if opt.no_edge:
                    batch_input = batch_input_img
                else:
                    batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
                batch_input_v = Variable(batch_input)
                batch_gt_v = Variable(batch_gt)


                pred = net(batch_input_v)

                if opt.only_residual:
                    loss = pnsr_crit(pred+Variable(batch_input_img+128), batch_gt_v)
                else:
                    loss = pnsr_crit(pred, batch_gt_v)
                avg_psnr += 10 * np.log10(255 * 255 / loss.item())
                task_cnt += 1

        visualizer.print_logs('Testing for epoch: %d' % epoch)
        visualizer.print_logs('Average test PNSR is %f for %d images' % (avg_psnr/task_cnt, task_cnt))








network: GCANet
name: default_exp
gpu_ids: 5
epochs: 100
lr: 0.001
lr_step: 40
lr_gamma: 0.1
weight_decay: 0.0005
checkpoints_dir: checkpoint/
logDir: tblogdir
resume_dir: checkpoint/default_exp
resume_epoch: 0
save_epoch: 5
save_latest_freq: 5000
test_epoch: 5
test_max_size: 1080
size_unit: 8
print_iter: 100
input_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy
gt_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/clear
test_input_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy
test_gt_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear
num_workers: 16
batch_size: 4
only_residual: False
loss_func: l2
inc: 3
outc: 3
force_rgb: False
no_edge: False
Start to train epoch 99
19:25:19.878681 Step[0/3498], lr: 0.000010, mv_avg_loss: 14.944835, loss: 14.944835
19:25:28.664637 Step[100/3498], lr: 0.000010, mv_avg_loss: 12.444468, loss: 8.474774
================ Training Loss (Wed Nov 27 19:25:39 2024) ================
import os
import datetime
import argparse
import numpy as np

import torch
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import DataLoader

from ImagePairPrefixFolder import ImagePairPrefixFolder, var_custom_collate
from utils import MovingAvg
from tf_visualizer import TFVisualizer

parser = argparse.ArgumentParser()
parser.add_argument('--network', default='GCANet')
parser.add_argument('--name', default='default_exp')
parser.add_argument('--gpu_ids', default='5')
parser.add_argument('--epochs', type=int, default=100)
parser.add_argument('--lr', type=float, default=0.001)
parser.add_argument('--lr_step', type=int, default=40)
parser.add_argument('--lr_gamma', type=float, default=0.1)
parser.add_argument('--weight_decay', type=float, default=0.0005)
parser.add_argument('--checkpoints_dir', default='checkpoint/')
parser.add_argument('--logDir', default='tblogdir')
parser.add_argument('--resume_dir', default='')
parser.add_argument('--resume_epoch', type=int, default=0)
parser.add_argument('--save_epoch', type=int, default=5)
parser.add_argument('--save_latest_freq', type=int, default=5000)
parser.add_argument('--test_epoch', type=int, default=5)
parser.add_argument('--test_max_size', type=int, default=1080)
parser.add_argument('--size_unit', type=int,  default=8)
parser.add_argument('--print_iter', type=int,  default=100)
parser.add_argument('--input_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy')
parser.add_argument('--gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/ITS/train/clear')
parser.add_argument('--test_input_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy')
parser.add_argument('--test_gt_folder', default='/media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear')
parser.add_argument('--num_workers', type=int, default=16)
parser.add_argument('--batch_size', type=int, default=6)
parser.add_argument('--only_residual', action='store_true', help='regress residual rather than image')
parser.add_argument('--loss_func', default='l2', help='l2|l1')
parser.add_argument('--inc', type=int, default=3)
parser.add_argument('--outc', type=int, default=3)
parser.add_argument('--force_rgb', action='store_true')
parser.add_argument('--no_edge', action='store_true')

opt = parser.parse_args()

opt.input_folder = os.path.expanduser(opt.input_folder)
opt.gt_folder = os.path.expanduser(opt.gt_folder)
opt.test_input_folder = os.path.expanduser(opt.test_input_folder)
opt.test_gt_folder = os.path.expanduser(opt.test_gt_folder)


if not os.path.exists(os.path.join(opt.checkpoints_dir, opt.name)):
    os.makedirs(os.path.join(opt.checkpoints_dir, opt.name))
opt.resume_dir = opt.resume_dir if opt.resume_dir != '' else os.path.join(opt.checkpoints_dir, opt.name)

visualizer = TFVisualizer(opt)
### Log out
with open(os.path.realpath(__file__), 'r') as fid:
    visualizer.print_logs(fid.read())

## print argument
for key, val in vars(opt).items():
    visualizer.print_logs('%s: %s' % (key, val))

opt.gpu_ids = [int(x) for x in opt.gpu_ids.split(',')]
assert all(0 <= x <= torch.cuda.device_count() for x in opt.gpu_ids), 'gpu id should ' \
                                                      'be 0~{0}'.format(torch.cuda.device_count())
torch.cuda.set_device(opt.gpu_ids[0])


train_dataset = ImagePairPrefixFolder(opt.input_folder, opt.gt_folder, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
train_dataloader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True,
                              collate_fn=var_custom_collate, pin_memory=True,
                              num_workers=opt.num_workers)

opt.do_test = opt.test_gt_folder != ''
if opt.do_test:
    test_dataset = ImagePairPrefixFolder(opt.test_input_folder, opt.test_gt_folder,
                                         max_img_size=opt.test_max_size, size_unit=opt.size_unit, force_rgb=opt.force_rgb)
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,
                                 collate_fn=var_custom_collate, pin_memory=True,
                                 num_workers=1)

total_inc = opt.inc if opt.no_edge else opt.inc + 1
if opt.network == 'GCANet':
    from GCANet import GCANet
    net = GCANet()
else:
    print('network structure %s not supported' % opt.network)
    raise ValueError

if opt.loss_func == 'l2':
    loss_crit = torch.nn.MSELoss()
elif opt.loss_func == 'l1':
    loss_crit = torch.nn.SmoothL1Loss()
else:
    print('loss_func %s not supported' % opt.loss_func)
    raise ValueError
pnsr_crit = torch.nn.MSELoss()

if len(opt.gpu_ids) > 0:
    net.cuda()
    if len(opt.gpu_ids) > 1:
        net = torch.nn.DataParallel(net)
    loss_crit = loss_crit.cuda()
    pnsr_crit = pnsr_crit.cuda()

optimizer = optim.Adam(net.parameters(), lr=opt.lr)
step_optim_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.lr_step, gamma=opt.lr_gamma)
loss_avg = MovingAvg(pool_size=50)

start_epoch = 0
total_iter = 0

if os.path.exists(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth')):
    print('resuming from latest.pth')
    latest_info = torch.load(os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))
    start_epoch = latest_info['epoch']
    total_iter = latest_info['total_iter']
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(latest_info['net_state'])
    else:
        net.load_state_dict(latest_info['net_state'])
    optimizer.load_state_dict(latest_info['optim_state'])

if opt.resume_epoch > 0:
    start_epoch = opt.resume_epoch
    total_iter = opt.resume_epoch * len(train_dataloader)
    resume_path = os.path.join(opt.resume_epoch, 'net_epoch_%d.pth') % opt.resume_epoch
    print('resume from : %s' % resume_path)
    assert os.path.exists(resume_path), 'cannot find the resume model: %s ' % resume_path
    if isinstance(net, torch.nn.DataParallel):
        net.module.load_state_dict(torch.load(resume_path))
    else:
        net.load_state_dict(torch.load(resume_path))

for epoch in range(start_epoch, opt.epochs):
    visualizer.print_logs("Start to train epoch %d" % epoch)
    net.train()
    for iter, data in enumerate(train_dataloader):
        total_iter += 1
        optimizer.zero_grad()
        step_optim_scheduler.step(epoch)

        batch_input_img, batch_input_edge,  batch_gt = data
        if len(opt.gpu_ids) > 0:
            batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

        if opt.no_edge:
            batch_input = batch_input_img
        else:
            batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
        batch_input_v = Variable(batch_input)
        if opt.only_residual:
            batch_gt_v = Variable(batch_gt - (batch_input_img+128))
        else:
            batch_gt_v = Variable(batch_gt)

        pred = net(batch_input_v)

        loss = loss_crit(pred, batch_gt_v)
        avg_loss = loss_avg.set_curr_val(loss.data)

        loss.backward()
        optimizer.step()

        if iter % opt.print_iter == 0:
            visualizer.plot_current_losses(total_iter, { 'loss': loss})
            visualizer.print_logs('%s Step[%d/%d], lr: %f, mv_avg_loss: %f, loss: %f' %
                                    (str(datetime.datetime.now()).split(' ')[1], iter, len(train_dataloader),
                                     step_optim_scheduler.get_lr()[0], avg_loss, loss))

        if total_iter % opt.save_latest_freq == 0:
            latest_info = {'total_iter': total_iter,
                           'epoch': epoch,
                           'optim_state': optimizer.state_dict()}
            if len(opt.gpu_ids) > 1:
                latest_info['net_state'] = net.module.state_dict()
            else:
                latest_info['net_state'] = net.state_dict()
            print('save lastest model.')
            torch.save(latest_info, os.path.join(opt.checkpoints_dir, opt.name, 'latest.pth'))

    if (epoch+1) % opt.save_epoch == 0 :
        visualizer.print_logs('saving model for epoch %d' % epoch)
        if len(opt.gpu_ids) > 1:
            torch.save(net.module.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch+1)))
        else:
            torch.save(net.state_dict(), os.path.join(opt.checkpoints_dir, opt.name, 'net_epoch_%d.pth' % (epoch + 1)))

    if opt.do_test:        
        avg_psnr = 0
        task_cnt = 0
        net.eval()
        with torch.no_grad():
            for iter, data in enumerate(test_dataloader):
                batch_input_img, batch_input_edge,  batch_gt = data
                if len(opt.gpu_ids) > 0:
                    batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()

                if opt.no_edge:
                    batch_input = batch_input_img
                else:
                    batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)
                batch_input_v = Variable(batch_input)
                batch_gt_v = Variable(batch_gt)


                pred = net(batch_input_v)

                if opt.only_residual:
                    loss = pnsr_crit(pred+Variable(batch_input_img+128), batch_gt_v)
                else:
                    loss = pnsr_crit(pred, batch_gt_v)
                avg_psnr += 10 * np.log10(255 * 255 / loss.item())
                task_cnt += 1

        visualizer.print_logs('Testing for epoch: %d' % epoch)
        visualizer.print_logs('Average test PNSR is %f for %d images' % (avg_psnr/task_cnt, task_cnt))








network: GCANet
name: default_exp
gpu_ids: 5
epochs: 100
lr: 0.001
lr_step: 40
lr_gamma: 0.1
weight_decay: 0.0005
checkpoints_dir: checkpoint/
logDir: tblogdir
resume_dir: checkpoint/default_exp
resume_epoch: 0
save_epoch: 5
save_latest_freq: 5000
test_epoch: 5
test_max_size: 1080
size_unit: 8
print_iter: 100
input_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/hazy
gt_folder: /media/happy507/DataSpace1/zhangzijiao/ITS/train/clear
test_input_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/hazy
test_gt_folder: /media/happy507/DataSpace1/zhangzijiao/sots512/indoor/clear
num_workers: 16
batch_size: 6
only_residual: False
loss_func: l2
inc: 3
outc: 3
force_rgb: False
no_edge: False
Start to train epoch 99
19:25:46.611871 Step[0/2332], lr: 0.000010, mv_avg_loss: 12.100760, loss: 12.100760
19:25:59.631061 Step[100/2332], lr: 0.000010, mv_avg_loss: 12.342017, loss: 12.715584
19:26:12.770224 Step[200/2332], lr: 0.000010, mv_avg_loss: 12.059121, loss: 19.263004
19:26:26.014381 Step[300/2332], lr: 0.000010, mv_avg_loss: 12.422111, loss: 15.422025
19:26:39.331022 Step[400/2332], lr: 0.000010, mv_avg_loss: 12.032941, loss: 11.572751
19:26:52.744922 Step[500/2332], lr: 0.000010, mv_avg_loss: 12.158521, loss: 16.006004
19:27:06.220945 Step[600/2332], lr: 0.000010, mv_avg_loss: 12.221788, loss: 8.932940
19:27:19.737907 Step[700/2332], lr: 0.000010, mv_avg_loss: 12.233525, loss: 9.356944
19:27:33.302214 Step[800/2332], lr: 0.000010, mv_avg_loss: 12.195399, loss: 11.890299
19:27:46.894203 Step[900/2332], lr: 0.000010, mv_avg_loss: 12.141591, loss: 10.065691
19:28:00.502806 Step[1000/2332], lr: 0.000010, mv_avg_loss: 12.366146, loss: 18.136166
19:28:14.129975 Step[1100/2332], lr: 0.000010, mv_avg_loss: 12.413110, loss: 11.406995
19:28:27.781764 Step[1200/2332], lr: 0.000010, mv_avg_loss: 11.876554, loss: 7.498763
19:28:41.467407 Step[1300/2332], lr: 0.000010, mv_avg_loss: 12.370336, loss: 13.283278
19:28:55.181683 Step[1400/2332], lr: 0.000010, mv_avg_loss: 12.109102, loss: 14.820554
19:29:08.899725 Step[1500/2332], lr: 0.000010, mv_avg_loss: 12.700404, loss: 11.596995
19:29:22.643141 Step[1600/2332], lr: 0.000010, mv_avg_loss: 12.437488, loss: 11.630879
19:29:36.376470 Step[1700/2332], lr: 0.000010, mv_avg_loss: 11.779077, loss: 10.236836
19:29:50.126453 Step[1800/2332], lr: 0.000010, mv_avg_loss: 11.954599, loss: 10.144796
19:30:03.888121 Step[1900/2332], lr: 0.000010, mv_avg_loss: 11.795943, loss: 16.532280
19:30:17.661197 Step[2000/2332], lr: 0.000010, mv_avg_loss: 12.075119, loss: 11.731502
19:30:31.459531 Step[2100/2332], lr: 0.000010, mv_avg_loss: 11.854215, loss: 10.167822
19:30:45.243740 Step[2200/2332], lr: 0.000010, mv_avg_loss: 12.144492, loss: 10.710336
19:30:59.022897 Step[2300/2332], lr: 0.000010, mv_avg_loss: 11.771768, loss: 12.836264
saving model for epoch 99
Testing for epoch: 99
Average test PNSR is 25.307571 for 500 images
